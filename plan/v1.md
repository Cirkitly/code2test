### The Senior Engineer Analogy: How Humans Tackle Complexity

Think about how a top-tier human engineer approaches a million-line project they've never seen before. Do they read every single line of code? No, that's impossible.

Instead, they:

1.  **Build a High-Level Map (Indexing):** They spend time up-front to understand the architecture. They identify key modules, read `README`s, look at the directory structure, and find the main entry points. They build a *mental model* or an "index" of where to find what.
2.  **Search, Don't Read (Retrieval):** When given a task—like "fix a bug in the payment processing logic"—they don't re-read the entire codebase. They use their mental map and tools (`grep`, `git blame`, IDE search) to find the *specific, relevant files and functions* related to "payment processing."
3.  **Reason Over a Small Context (Generation):** They load only those few, relevant functions into their brain (their "working memory" or context window). They reason about the problem using this small, targeted subset of the code.

Our AI Software Foundry is designed to do exactly this, but in a structured and automated way. The "losing context" problem is solved by **not relying on the LLM's context window for long-term memory.**

---

### The Technical Solution: A Deep Dive into the RAG-Powered Workflow

Here’s precisely how the architecture avoids context loss and handles the scale.

#### 1. Phase 1: Building the "Brain" — The Knowledge Graph

This is the most critical phase. We create a rich, multi-layered representation of the codebase that is optimized for fast and accurate retrieval.

-   **Step 1: Atomic Unit Summarization.** We don't just chunk the code. We use a **`GenerateCodeSummariesNode`** in a parallel flow (`AsyncParallelBatchFlow`) to process every single function, class, and method across the entire codebase. For each "atomic unit," the LLM generates a structured "digest" containing:
    -   A natural language summary of its purpose.
    -   Its explicit dependencies (which other functions it calls).
    -   Its interface (parameters and return types).
    -   Key logic patterns (e.g., "iterates over a list," "performs a database transaction").

-   **Step 2: Multi-Modal Embedding.** The **`BuildKnowledgeGraphNode`** then creates multiple embeddings for each code unit:
    -   **Semantic Embedding:** An embedding of the natural language *summary*. This is for finding code based on *intent* (e.g., "find code that handles payment refunds").
    -   **Structural Embedding:** An embedding of the code's signature and dependencies. This is for finding code based on its *relationships* (e.g., "find functions that call the `update_user_balance` API").
    -   **(Optional) Code-As-Text Embedding:** An embedding of the raw code itself for direct similarity searches.

-   **Step 3: Storing in a Graph-Aware Vector DB.** This information is stored in a vector database that also holds the metadata (file path, function name, dependencies, call graph). We now have a **Knowledge Graph**, not just a flat vector store.

**Outcome of Phase 1:** We have a complete, searchable "brain" of the codebase. The LLM's short-term memory (context window) is no longer a constraint for knowing "what's in the project."

#### 2. Phase 2: "Thinking" with the Brain — On-Demand RAG

This is where the magic happens. When an agent (like `PlanTestsNode` or `HealNode`) needs to "think," it retrieves context instead of being given it.

Let's walk through generating a single test for a function `calculate_shipping_cost()` in `shipping.py`.

-   **Step 1: The Initial Task.** The Orchestration Service tells the `TestGenerationAgent`: "Generate tests for `calculate_shipping_cost()`."

-   **Step 2: Multi-Hop Retrieval (The Core of "Not Losing Context").** The agent doesn't get the source code. It performs a series of queries against the Knowledge Graph:
    1.  **Query for the Target:** "Retrieve the code digest for `calculate_shipping_cost` in `shipping.py`." This returns its summary, dependencies, and raw code snippet.
    2.  **Query for Dependencies:** The digest from step 1 shows that `calculate_shipping_cost` calls `get_user_address()` and `lookup_shipping_rate()`. The agent now makes *two more queries*: "Retrieve the code digests for `get_user_address` and `lookup_shipping_rate`."
    3.  **Query for Usage Examples:** The agent can also query "Retrieve digests of functions that *call* `calculate_shipping_cost`." This gives it examples of how the function is used in practice.

-   **Step 3: Synthesizing the Prompt.** The agent now has a small collection of highly relevant code digests. It assembles these into a prompt for the generation LLM. The context passed to the LLM is **tiny** compared to the full codebase:

    > **LLM Prompt (Conceptual):**
    >
    > "You are a test engineer. Your task is to write Pytest unit tests for the function `calculate_shipping_cost`.
    >
    > **Here is the relevant context, retrieved from our knowledge graph:**
    >
    > **Target Function Summary:**
    > `calculate_shipping_cost(user_id, cart_items)`: Takes a user ID and items, calculates shipping.
    >
    > **Dependency 1 Summary:**
    > `get_user_address(user_id)`: Retrieves the user's address from the database. **This must be mocked.**
    >
    > **Dependency 2 Summary:**
    > `lookup_shipping_rate(address, weight)`: Calls an external shipping API to get rates. **This must be mocked.**
    >
    > **Usage Example Summary:**
    > `process_order()` calls `calculate_shipping_cost` after totaling the cart.
    >
    > Now, write the tests."

-   **Step 4: Generation.** The LLM, with this surgical and rich context, can easily generate the required tests and identify what needs to be mocked, without ever knowing what's in the other 999,990 lines of code.

### Summary of How Context Loss is Avoided

| Problem with Naive Approach | How The Foundry Solves It |
| :--- | :--- |
| LLM context window is too small for the whole codebase. | The LLM **never sees** the whole codebase. It only sees small, relevant snippets retrieved from the Knowledge Graph. |
| LLM gets lost or confused by irrelevant details ("lost in the middle"). | **RAG acts as a filter.** Only the most relevant function summaries and dependencies are provided, eliminating noise. |
| Understanding code relationships is hard from a flat file. | The **Knowledge Graph** explicitly stores dependencies (call graphs), allowing the AI to reason about relationships without parsing the whole project. |
| Processing is slow and expensive. | Embedding and summarizing is a one-time (or incremental) cost. The online generation phase is fast and cheap because it uses small, targeted LLM calls. |

This architecture shifts the burden of "memory" from the LLM's volatile context window to a persistent, searchable, and structured Knowledge Graph. **The LLM's role changes from "knowing everything" to "expertly reasoning over provided information," which is what they are best at.**