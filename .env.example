# --- LLM Provider Configuration ---
# Choose your provider: "azure" or "ollama"
LLM_PROVIDER="azure"

# --- Azure OpenAI Credentials (if LLM_PROVIDER="azure") ---
AZURE_OPENAI_ENDPOINT="https://<your-resource-name>.openai.azure.com/"
AZURE_OPENAI_API_KEY="<your-azure-openai-api-key>"
# This is the custom name you gave the model when you deployed it in Azure.
AZURE_OPENAI_DEPLOYMENT="<your-deployment-name>" 
AZURE_OPENAI_API_VERSION="2024-05-01-preview"

# --- Ollama Configuration (if LLM_PROVIDER="ollama") ---
# The model used for generation tasks.
OLLAMA_MODEL="llama3"
# The URL for the Ollama chat API.
OLLAMA_URL="http://localhost:11434/api/chat"

# --- Embedding Model Configuration (Used for RAG) ---
# This model is always run locally via Ollama for the RAG component.
# Ensure you have run `ollama pull mxbai-embed-large`.
EMBEDDING_MODEL="mxbai-embed-large"

# --- Logging Configuration ---
# Directory to store log files from LLM calls.
LOG_DIR="logs"