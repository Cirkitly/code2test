{
  "metadata_store": {
    "0": {
      "unit_name": "setup.py",
      "file_path": "/tmp/PocketFlow/setup.py",
      "code": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"pocketflow\",\n    version=\"0.0.2\",\n    packages=find_packages(),\n    author=\"Zachary Huang\",\n    author_email=\"zh2408@columbia.edu\",\n    description=\"Pocket Flow: 100-line LLM framework. Let Agents build Agents!\",\n    url=\"https://github.com/The-Pocket/PocketFlow\",\n)",
      "summary": "**Summary of `setup.py`:**\n\n1. **Primary purpose:**  \n   This code unit serves as a setup script for installing the \"pocketflow\" Python package using setuptools. It provides the metadata and configuration needed for packaging and distribution.\n\n2. **Brief description of its parameters:**  \n   The `setup()` function is called with the following parameters:\n   - `name`: The name of the package (\"pocketflow\").\n   - `version`: The version number (\"0.0.2\").\n   - `packages`: The list of packages to include, determined by `find_packages()`.\n   - `author`: The author's name (\"Zachary Huang\").\n   - `author_email`: The author's email address.\n   - `description`: A brief description of the package.\n   - `url`: The URL for the package's homepage or repository.\n\n3. **Brief description of its return value:**  \n   This code does not return a value. Instead, it sets up package metadata for use in building, installing, and distributing the package.\n\n4. **Other functions or methods it calls internally:**  \n   - `setuptools.setup()` \u2014 The main function for configuring the package.\n   - `setuptools.find_packages()` \u2014 Used to automatically discover all packages/subpackages to include."
    },
    "1": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/pocketflow/__init__.py",
      "code": "import asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def next(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node; return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared); e=self._exec(p); return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.next(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.next(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__(); self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start=None): super().__init__(); self.start_node=start\n    def start(self,start): self.start_node=start; return start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p,last_action =copy.copy(self.start_node),(params or {**self.params}),None\n        while curr: curr.set_params(p); last_action=curr._run(shared); curr=copy.copy(self.get_next_node(curr,last_action))\n        return last_action\n    def _run(self,shared): p=self.prep(shared); o=self._orch(shared); return self.post(shared,p,o)\n    def post(self,shared,prep_res,exec_res): return exec_res\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared); e=await self._exec(p); return await self.post_async(shared,p,e)\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p,last_action =copy.copy(self.start_node),(params or {**self.params}),None\n        while curr: curr.set_params(p); last_action=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared); curr=copy.copy(self.get_next_node(curr,last_action))\n        return last_action\n    async def _run_async(self,shared): p=await self.prep_async(shared); o=await self._orch_async(shared); return await self.post_async(shared,p,o)\n    async def post_async(self,shared,prep_res,exec_res): return exec_res\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared): \n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)",
      "summary": "**Summary of `/tmp/PocketFlow/pocketflow/__init__.py`**\n\n---\n\n### 1. Primary Purpose\n\nThis code unit provides a flexible and extensible framework for defining, running, and composing computational nodes and flows (pipelines) both synchronously and asynchronously, with support for error-handling, conditional transitions, parameter injection, and batch processing. It defines a set of base classes and utilities for constructing data processing graphs where nodes perform computations and transitions can be conditioned on outcomes.\n\n---\n\n### 2. Parameters\n\n- **Node/Basenode/Flow/BatchNode/AsyncNode/AsyncFlow etc.:**\n  - Nodes generally accept parameters for controlling repeats (`max_retries`), waiting times (`wait`), and node parameters (`params`).\n  - `Flow`/`BatchFlow`/`AsyncFlow`/`AsyncBatchFlow`: Optionally take a `start` node to initialize the flow.\n\n- **Per-method parameters:**\n  - Most `run`, `run_async`, `_run`, `_run_async` methods take a `shared` object (arbitrary shared data for the flow).\n  - Many methods support passing custom per-node parameters (`params`), batch items, actions, and control over transitions.\n\n---\n\n### 3. Return Values\n\n- Nodes (`run`, `_run`) typically return the result from the node's computation via `post`.\n- `Flow` instances return the result of the last node's execution in their run (or the aggregate, in batch variants\u2014often `None` for aggregate).\n- Asynchronous variants (`run_async`, `_run_async`) return results via `await`.\n- Return type is arbitrary\u2014it depends on the node's `exec`/`post` implementation (defaults are `None` unless overridden).\n\n---\n\n### 4. Internal Functions/Methods Called\n\n- **Standard Library:**  \n  - `asyncio.sleep`, `asyncio.gather`, `warnings.warn`, `copy.copy`, `time.sleep`\n- **Internally within code:**\n  - Node methods: `prep`, `exec`, `exec_fallback`, `post`, `_run`, `_exec`\n  - Asynchronous counterparts: `prep_async`, `exec_async`, `exec_fallback_async`, `post_async`, `_run_async`, `_exec`\n  - Transition and composition: `.next`, `.set_params`, `.get_next_node`, `.start`, `.start_node`\n  - Flow orchestration: `_orch` (synchronous), `_orch_async` (async)\n  - Operator overloading for nodes/flows: `__rshift__`, `__sub__`\n  - Batch variants iterate and call node/flow methods per item.\n\n---\n\n**In short:**  \nThis file implements a composable pipeline (\"flow\") pattern for data processing and workflow orchestration, supporting both sync and async use cases with extensible hooks for preparation, execution, failure handling, conditional routing, and postprocessing, along with facilities for handling batches and retries."
    },
    "2": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-llm-streaming/main.py",
      "code": "import time\nimport threading\nfrom pocketflow import Node, Flow\nfrom utils import fake_stream_llm, stream_llm\n\nclass StreamNode(Node):\n    def prep(self, shared):\n        # Create interrupt event\n        interrupt_event = threading.Event()\n\n        # Start a thread to listen for user interrupt\n        def wait_for_interrupt():\n            input(\"Press ENTER at any time to interrupt streaming...\\n\")\n            interrupt_event.set()\n        listener_thread = threading.Thread(target=wait_for_interrupt)\n        listener_thread.start()\n        \n        # Get prompt from shared store\n        prompt = shared[\"prompt\"]\n        # Get chunks from LLM function\n        chunks = stream_llm(prompt)\n        return chunks, interrupt_event, listener_thread\n\n    def exec(self, prep_res):\n        chunks, interrupt_event, listener_thread = prep_res\n        for chunk in chunks:\n            if interrupt_event.is_set():\n                print(\"User interrupted streaming.\")\n                break\n            \n            if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:\n                chunk_content = chunk.choices[0].delta.content\n                print(chunk_content, end=\"\", flush=True)\n                time.sleep(0.1)  # simulate latency\n        return interrupt_event, listener_thread\n\n    def post(self, shared, prep_res, exec_res):\n        interrupt_event, listener_thread = exec_res\n        # Join the interrupt listener so it doesn't linger\n        interrupt_event.set()\n        listener_thread.join()\n        return \"default\"\n\n# Usage:\nnode = StreamNode()\nflow = Flow(start=node)\n\nshared = {\"prompt\": \"What's the meaning of life?\"}\nflow.run(shared)\n",
      "summary": "**Summary of /tmp/PocketFlow/cookbook/pocketflow-llm-streaming/main.py:**\n\n1. **Primary Purpose:**  \n   The code defines a streaming node for a PocketFlow-based dataflow, designed to interactively stream text chunks from a large language model (LLM) in response to a user-supplied prompt. It also enables the user to interrupt the streaming process at any time.\n\n2. **Parameters:**  \n   - The main parameters appear in the following contexts:\n     - The `prep` method receives a `shared` dictionary, expected to contain a `\"prompt\"` string.\n     - The overall workflow relies on `shared` for input.\n   - The script uses no command-line arguments; parameters are passed by dictionary.\n\n3. **Return Value:**  \n   - The `prep` method returns a tuple: (`chunks`, `interrupt_event`, `listener_thread`), where:\n     - `chunks` is an iterable of LLM output chunks (from `stream_llm`).\n     - `interrupt_event` is a threading event used to signal interruption.\n     - `listener_thread` is a background thread for interrupt listening.\n   - The `exec` method returns a tuple of (`interrupt_event`, `listener_thread`), mainly for cleanup in `post`.\n   - The `post` method returns a static string: `\"default\"`.\n   - The end result of running `flow.run(shared)` is not explicitly used.\n\n4. **Functions or Methods Called Internally:**\n   - **from PocketFlow:**  \n     - `Node`, `Flow`, and their methods.\n   - **from utils (local import):**  \n     - `stream_llm` (main LLM streaming function).\n     - `fake_stream_llm` (imported but unused).\n   - **Standard Library:**  \n     - `threading.Event`\n     - `threading.Thread`\n     - `input` (for user interrupt)\n     - `time.sleep` (to simulate latency)\n     - `print`\n     - `hasattr`, `break`, `end`, `flush` (various Python built-ins)\n\n**In summary:**  \nThis code sets up a node for streaming text chunks from an LLM with interactive, user-triggered interruption. It accomplishes this by running a listener thread for user input, using an event object for coordination, and then iterating through the streaming LLM output, printing as it goes until user stops the process."
    },
    "3": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-llm-streaming/utils.py",
      "code": "from openai import OpenAI\nimport os\n\ndef stream_llm(prompt):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n\n    # Make a streaming chat completion request\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7,\n        stream=True  # Enable streaming\n    )\n    return response\n\ndef fake_stream_llm(prompt, predefined_text=\"This is a fake response. Today is a sunny day. The sun is shining. The birds are singing. The flowers are blooming. The bees are buzzing. The wind is blowing. The clouds are drifting. The sky is blue. The grass is green. The trees are tall. The water is clear. The fish are swimming. The sun is shining. The birds are singing. The flowers are blooming. The bees are buzzing. The wind is blowing. The clouds are drifting. The sky is blue. The grass is green. The trees are tall. The water is clear. The fish are swimming.\"):\n    \"\"\"\n    Returns a list of simple objects that mimic the structure needed\n    for OpenAI streaming responses.\n    \"\"\"\n    # Split text into small chunks\n    chunk_size = 10\n    chunks = []\n    \n    # Create the chunks using a simple class outside the nested structure\n    class SimpleObject:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n    \n    # Build the chunks\n    for i in range(0, len(predefined_text), chunk_size):\n        text_chunk = predefined_text[i:i+chunk_size]\n        \n        # Create the nested structure using simple objects\n        delta = SimpleObject(content=text_chunk)\n        choice = SimpleObject(delta=delta)\n        chunk = SimpleObject(choices=[choice])\n        \n        chunks.append(chunk)\n    \n    return chunks\n\nif __name__ == \"__main__\":\n    print(\"## Testing streaming LLM\")\n    prompt = \"What's the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    # response = fake_stream_llm(prompt)\n    response = stream_llm(prompt)\n    print(f\"## Response: \")\n    for chunk in response:\n        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:\n            chunk_content = chunk.choices[0].delta.content\n            # Print the incoming text without a newline (simulate real-time streaming)\n            print(chunk_content, end=\"\", flush=True)\n\n",
      "summary": "**Summary of `utils.py`:**\n\n1. **Primary Purpose:**  \n   The code provides utility functions for generating streaming responses from a language model (LLM), either using OpenAI's GPT-4o via API (`stream_llm`), or simulating streaming output with a fake function for testing (`fake_stream_llm`). It is intended to demonstrate or test real-time LLM output streaming.\n\n2. **Parameters:**\n   - `stream_llm(prompt)`:  \n     - `prompt` (str): The user query or input to send to the language model.\n   - `fake_stream_llm(prompt, predefined_text=...)`:  \n     - `prompt` (str): Input prompt (not actually used in function logic).\n     - `predefined_text` (str, optional): The fake response text to be streamed (defaults to a long, generic cheerful description).\n\n3. **Return Value:**\n   - `stream_llm`:  \n     Returns an iterable/streaming OpenAI API response object, yielding successive chunks of the model's reply.\n   - `fake_stream_llm`:  \n     Returns a list of simple objects structured similarly to OpenAI's API streaming response chunks (each containing a small chunk of the predefined text).\n\n4. **Other Functions or Methods Called Internally:**\n   - `os.environ.get()` (standard library: reads environment variable for OpenAI API key)\n   - `OpenAI()` and `client.chat.completions.create()` (from the `openai` Python package: establishes API client and sends completion request)\n   - (Built-in class method `__init__` for internal `SimpleObject` used in `fake_stream_llm`)\n   - Standard Python features like string slicing, list operations, and printing (no additional external libraries or complex internal calls).\n\n**Usage note:**  \nThe `__main__` block provides a simple demonstration: it prints a prompt, calls either the real or fake streaming function, and then prints out the streamed result chunk by chunk, mimicking real-time generation."
    },
    "4": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-guardrail/main.py",
      "code": "from pocketflow import Node, Flow\nfrom utils import call_llm\n\nclass UserInputNode(Node):\n    def prep(self, shared):\n        # Initialize messages if this is the first run\n        if \"messages\" not in shared:\n            shared[\"messages\"] = []\n            print(\"Welcome to the Travel Advisor Chat! Type 'exit' to end the conversation.\")\n        \n        return None\n\n    def exec(self, _):\n        # Get user input\n        user_input = input(\"\\nYou: \")\n        return user_input\n\n    def post(self, shared, prep_res, exec_res):\n        user_input = exec_res\n        \n        # Check if user wants to exit\n        if user_input and user_input.lower() == 'exit':\n            print(\"\\nGoodbye! Safe travels!\")\n            return None  # End the conversation\n        \n        # Store user input in shared\n        shared[\"user_input\"] = user_input\n        \n        # Move to guardrail validation\n        return \"validate\"\n\nclass GuardrailNode(Node):\n    def prep(self, shared):\n        # Get the user input from shared data\n        user_input = shared.get(\"user_input\", \"\")\n        return user_input\n    \n    def exec(self, user_input):\n        # Basic validation checks\n        if not user_input or user_input.strip() == \"\":\n            return False, \"Your query is empty. Please provide a travel-related question.\"\n        \n        if len(user_input.strip()) < 3:\n            return False, \"Your query is too short. Please provide more details about your travel question.\"\n        \n        # LLM-based validation for travel topics\n        prompt = f\"\"\"\nEvaluate if the following user query is related to travel advice, destinations, planning, or other travel topics.\nThe chat should ONLY answer travel-related questions and reject any off-topic, harmful, or inappropriate queries.\nUser query: {user_input}\nReturn your evaluation in YAML format:\n```yaml\nvalid: true/false\nreason: [Explain why the query is valid or invalid]\n```\"\"\"\n        \n        # Call LLM with the validation prompt\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = call_llm(messages)\n        \n        # Extract YAML content\n        yaml_content = response.split(\"```yaml\")[1].split(\"```\")[0].strip() if \"```yaml\" in response else response\n        \n        import yaml\n        result = yaml.safe_load(yaml_content)\n        assert result is not None, \"Error: Invalid YAML format\"\n        assert \"valid\" in result and \"reason\" in result, \"Error: Invalid YAML format\"\n        is_valid = result.get(\"valid\", False)\n        reason = result.get(\"reason\", \"Missing reason in YAML response\")\n        \n        return is_valid, reason\n    \n    def post(self, shared, prep_res, exec_res):\n        is_valid, message = exec_res\n        \n        if not is_valid:\n            # Display error message to user\n            print(f\"\\nTravel Advisor: {message}\")\n            # Skip LLM call and go back to user input\n            return \"retry\"\n        \n        # Valid input, add to message history\n        shared[\"messages\"].append({\"role\": \"user\", \"content\": shared[\"user_input\"]})\n        # Proceed to LLM processing\n        return \"process\"\n\nclass LLMNode(Node):\n    def prep(self, shared):\n        # Add system message if not present\n        if not any(msg.get(\"role\") == \"system\" for msg in shared[\"messages\"]):\n            shared[\"messages\"].insert(0, {\n                \"role\": \"system\", \n                \"content\": \"You are a helpful travel advisor that provides information about destinations, travel planning, accommodations, transportation, activities, and other travel-related topics. Only respond to travel-related queries and keep responses informative and friendly. Your response are concise in 100 words.\"\n            })\n        \n        # Return all messages for the LLM\n        return shared[\"messages\"]\n\n    def exec(self, messages):\n        # Call LLM with the entire conversation history\n        response = call_llm(messages)\n        return response\n\n    def post(self, shared, prep_res, exec_res):\n        # Print the assistant's response\n        print(f\"\\nTravel Advisor: {exec_res}\")\n        \n        # Add assistant message to history\n        shared[\"messages\"].append({\"role\": \"assistant\", \"content\": exec_res})\n        \n        # Loop back to continue the conversation\n        return \"continue\"\n\n# Create the flow with nodes and connections\nuser_input_node = UserInputNode()\nguardrail_node = GuardrailNode()\nllm_node = LLMNode()\n\n# Create flow connections\nuser_input_node - \"validate\" >> guardrail_node\nguardrail_node - \"retry\" >> user_input_node  # Loop back if input is invalid\nguardrail_node - \"process\" >> llm_node\nllm_node - \"continue\" >> user_input_node     # Continue conversation\n\nflow = Flow(start=user_input_node)\n\n# Start the chat\nif __name__ == \"__main__\":\n    shared = {}\n    flow.run(shared)\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-chat-guardrail/main.py`:**\n\n1. **Primary Purpose:**  \n   This code implements a command-line conversational chatbot (\"Travel Advisor Chat\") using a node-based flow. The bot interacts with users, validates their questions to ensure they are travel-related, and generates responses using a language model, enforcing topic and safety guardrails.\n\n2. **Parameters:**  \n   - The main components (`prep`, `exec`, `post` methods for each node) accept and operate on a shared dictionary (`shared`) for conversation state and, in some cases, method-specific arguments like user input or LLM message history.\n   - The script itself takes no command-line parameters.\n\n3. **Return Value:**  \n   - The main script does not return a value.  \n   - Each node's `post` methods return a string (label) that determines the next step in the flow, or `None` to end the conversation.\n\n4. **Other Functions/Methods Called Internally:**  \n   - **`call_llm(messages)`**: (imported from `utils`) Used to interact with the language model (LLM) for both validation (in `GuardrailNode`) and producing answers (in `LLMNode`).\n   - **Built-ins**:  \n     - `input()`: To obtain user input.  \n     - `print()`: To display messages to the user.  \n     - `assert`: To check YAML validity.\n   - **External Libraries**:  \n     - `yaml.safe_load()`: To parse validation responses in YAML format from the LLM.\n\n**Flow Overview:**  \n1. User prompt is taken (UserInputNode).\n2. GuardrailNode validates that input is travel-related (using both basic checks and LLM-based topic checking).\n3. If valid, LLMNode uses the LLM to generate a travel advisor response.\n4. The conversation continues in a loop until the user types 'exit' or provides invalid input.\n\n**In summary:**  \nThe script defines a robust, topic-constrained chat loop for travel queries, integrating user input collection, travel-topic validation (both rule- and LLM-based), and LLM-based response generation, with state handled in a shared dictionary throughout the flow."
    },
    "5": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-guardrail/utils.py",
      "code": "from openai import OpenAI\nimport os\n\ndef call_llm(messages):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test the LLM call\n    messages = [{\"role\": \"user\", \"content\": \"In a few words, what's the meaning of life?\"}]\n    response = call_llm(messages)\n    print(f\"Prompt: {messages[0]['content']}\")\n    print(f\"Response: {response}\")\n",
      "summary": "**Summary of `utils.py`:**\n\n1. **Primary purpose:**  \n   The code defines a utility function to interact with OpenAI's GPT models (specifically GPT-4o) via the OpenAI Python SDK, sending chat-based prompts and retrieving responses. It is intended to facilitate making LLM (Large Language Model) API calls from code.\n\n2. **Parameters:**  \n   - The function `call_llm(messages)` takes one parameter:\n     - `messages`: A list of dictionaries, where each dictionary represents a chat message (with keys like `\"role\"` and `\"content\"`), formatted according to the OpenAI Chat API specification.\n\n3. **Return value:**  \n   - The function returns a string containing the content of the response generated by the LLM (the assistant's reply to the input messages).\n\n4. **Functions/methods called internally:**  \n   - `OpenAI(api_key=...)`: Initializes the OpenAI Python client with the specified API key.\n   - `os.environ.get(...)`: Accesses environment variables to retrieve the API key.\n   - `client.chat.completions.create(...)`: Sends the chat messages to the LLM and gets the response.\n   - The script also uses basic built-in Python functions: `print()` (when run as a script).\n\n**Note:** The script includes a test block that demonstrates its usage if run directly."
    },
    "6": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-communication/nodes.py",
      "code": "\"\"\"Node implementations for the communication example.\"\"\"\n\nfrom pocketflow import Node\n\nclass EndNode(Node):\n    \"\"\"Node that handles flow termination.\"\"\"\n    pass\n\nclass TextInput(Node):\n    \"\"\"Node that reads text input and initializes the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get user input and ensure shared store is initialized.\"\"\"\n        return input(\"Enter text (or 'q' to quit): \")\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store text and initialize/update statistics.\"\"\"\n        if prep_res == 'q':\n            return \"exit\"\n        \n        # Store the text\n        shared[\"text\"] = prep_res\n        \n        # Initialize statistics if they don't exist\n        if \"stats\" not in shared:\n            shared[\"stats\"] = {\n                \"total_texts\": 0,\n                \"total_words\": 0\n            }\n        shared[\"stats\"][\"total_texts\"] += 1\n        \n        return \"count\"\n\nclass WordCounter(Node):\n    \"\"\"Node that counts words in the text.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get text from shared store.\"\"\"\n        return shared[\"text\"]\n    \n    def exec(self, text):\n        \"\"\"Count words in the text.\"\"\"\n        return len(text.split())\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Update word count statistics.\"\"\"\n        shared[\"stats\"][\"total_words\"] += exec_res\n        return \"show\"\n\nclass ShowStats(Node):\n    \"\"\"Node that displays statistics from the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get statistics from shared store.\"\"\"\n        return shared[\"stats\"]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Display statistics and continue the flow.\"\"\"\n        stats = prep_res\n        print(f\"\\nStatistics:\")\n        print(f\"- Texts processed: {stats['total_texts']}\")\n        print(f\"- Total words: {stats['total_words']}\")\n        print(f\"- Average words per text: {stats['total_words'] / stats['total_texts']:.1f}\\n\")\n        return \"continue\" ",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary Purpose:**  \n   This code defines a set of node classes for a communication flow example, intended for use with the `pocketflow` framework. The nodes guide a process where the user inputs text, word counts are maintained, and aggregate statistics are displayed interactively.\n\n2. **Parameters:**  \n   - Most methods accept `shared`, a dictionary-like object that acts as a shared store for data and statistics between nodes.\n   - Some methods receive `prep_res` (the result from the `prep` phase), and/or `exec_res` (the result from the `exec` phase), depending on the method's role within the node's lifecycle.\n\n3. **Return Value:**  \n   - The `prep` methods generally return data to be processed (e.g., user input or statistics).\n   - The `post` methods typically return a string specifying the next transition or state name in the flow (e.g., \"exit\", \"count\", \"show\", \"continue\").\n   - The `exec` method in `WordCounter` returns an integer (the word count).\n\n4. **Other Functions/Methods Called Internally:**  \n   - `input()` (built-in Python function) is called in `TextInput.prep` to read text from the user.\n   - `str.split()` (built-in string method) is used in `WordCounter.exec` to split the text into words.\n   - `print()` (built-in function) is called in `ShowStats.post` to output statistics.\n   - Standard dictionary operations (access, assignment, existence checks like `\"stats\" not in shared`) are used throughout for managing the shared state."
    },
    "7": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-communication/flow.py",
      "code": "\"\"\"Flow configuration for the communication example.\"\"\"\n\nfrom pocketflow import Flow\nfrom nodes import TextInput, WordCounter, ShowStats, EndNode\n\ndef create_flow():\n    \"\"\"Create and configure the flow with all nodes.\"\"\"\n    # Create nodes\n    text_input = TextInput()\n    word_counter = WordCounter()\n    show_stats = ShowStats()\n    end_node = EndNode()\n    \n    # Configure transitions\n    text_input - \"count\" >> word_counter\n    word_counter - \"show\" >> show_stats\n    show_stats - \"continue\" >> text_input\n    text_input - \"exit\" >> end_node\n    \n    # Create and return flow\n    return Flow(start=text_input) ",
      "summary": "**Summary of flow.py:**\n\n1. **Primary Purpose:**  \n   This code unit defines a function, `create_flow()`, that sets up and configures a workflow (\"flow\") for a communication example using the PocketFlow framework. It specifies how different nodes (representing user input, word counting, statistics display, and end point) are connected and how transitions between them occur.\n\n2. **Parameters:**  \n   - `create_flow()` takes no parameters.\n\n3. **Return Value:**  \n   - `create_flow()` returns an instance of `Flow`, which represents the configured workflow. The flow starts from the `TextInput` node.\n\n4. **Other Functions or Methods Called Internally:**  \n   - `TextInput()` (constructor for the text input node)\n   - `WordCounter()` (constructor for the word counter node)\n   - `ShowStats()` (constructor for the statistics display node)\n   - `EndNode()` (constructor for the end node)\n   - Transition operators `-` and `>>` overloaded to define flow transitions between nodes\n   - `Flow()` (constructor, with the `start` argument set to the initial node)\n\n**In summary:**  \nThis module configures and returns a simple state machine representing a flow where a user inputs text, sees word counts and statistics, and can continue or exit."
    },
    "8": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-communication/main.py",
      "code": "from flow import create_flow\n\ndef main():\n    \"\"\"Run the communication example.\"\"\"\n    flow = create_flow()\n    shared = {}\n    flow.run(shared)\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-communication/main.py`:**\n\n1. **Primary purpose:**  \n   This code serves as the entry point for running a communication example by initializing and executing a predefined \"flow\" using the `create_flow` function.\n\n2. **Parameters:**  \n   - The `main` function does not take any parameters.\n\n3. **Return value:**  \n   - The `main` function does not return any value (returns `None`).\n\n4. **Functions/methods called internally:**  \n   - `create_flow()` (from the imported `flow` module)\n   - `flow.run(shared)` (calls the `run` method of the flow object created by `create_flow`)"
    },
    "9": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-parallel-batch/main.py",
      "code": "import asyncio\nimport time\nimport os\nfrom pocketflow import AsyncFlow, AsyncParallelBatchNode\nfrom utils import call_llm\n\n# --- Node Definitions ---\n\nclass TranslateTextNodeParallel(AsyncParallelBatchNode):\n    \"\"\"Translates README into multiple languages in parallel and saves files.\"\"\"\n    async def prep_async(self, shared):\n        \"\"\"Reads text and target languages from shared store.\"\"\"\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [])\n        return [(text, lang) for lang in languages]\n\n    async def exec_async(self, data_tuple):\n        \"\"\"Calls the async LLM utility for each target language.\"\"\"\n        text, language = data_tuple\n        \n        prompt = f\"\"\"\nPlease translate the following markdown file into {language}. \nBut keep the original markdown format, links and code blocks.\nDirectly return the translated text, without any other text or comments.\n\nOriginal: \n{text}\n\nTranslated:\"\"\"\n        \n        result = await call_llm(prompt)\n        print(f\"Translated {language} text\")\n        return {\"language\": language, \"translation\": result}\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        \"\"\"Stores the dictionary of {language: translation} pairs and writes to files.\"\"\"\n        output_dir = shared.get(\"output_dir\", \"translations\")\n        os.makedirs(output_dir, exist_ok=True)\n        \n        for result in exec_res_list:\n            if isinstance(result, dict):\n                language = result.get(\"language\", \"unknown\")\n                translation = result.get(\"translation\", \"\")\n                \n                filename = os.path.join(output_dir, f\"README_{language.upper()}.md\")\n                try:\n                    import aiofiles\n                    async with aiofiles.open(filename, \"w\", encoding=\"utf-8\") as f:\n                        await f.write(translation)\n                    print(f\"Saved translation to {filename}\")\n                except ImportError:\n                    with open(filename, \"w\", encoding=\"utf-8\") as f:\n                        f.write(translation)\n                    print(f\"Saved translation to {filename} (sync fallback)\")\n                except Exception as e:\n                    print(f\"Error writing file {filename}: {e}\")\n            else:\n                print(f\"Warning: Skipping invalid result item: {result}\")\n        return \"default\"\n\n# --- Flow Creation ---\n\ndef create_parallel_translation_flow():\n    \"\"\"Creates and returns the parallel translation flow.\"\"\"\n    translate_node = TranslateTextNodeParallel(max_retries=3)\n    return AsyncFlow(start=translate_node)\n\n# --- Main Execution ---\n\nasync def main():\n    source_readme_path = \"../../README.md\"\n    try:\n        with open(source_readme_path, \"r\", encoding='utf-8') as f:\n            text = f.read()\n    except FileNotFoundError:\n        print(f\"Error: Could not find the source README file at {source_readme_path}\")\n        exit(1)\n    except Exception as e:\n        print(f\"Error reading file {source_readme_path}: {e}\")\n        exit(1)\n\n    shared = {\n        \"text\": text,\n        \"languages\": [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \"Russian\", \"Portuguese\", \"French\", \"Korean\"],\n        \"output_dir\": \"translations\"\n    }\n\n    translation_flow = create_parallel_translation_flow()\n\n    print(f\"Starting parallel translation into {len(shared['languages'])} languages...\")\n    start_time = time.perf_counter()\n\n    await translation_flow.run_async(shared)\n\n    end_time = time.perf_counter()\n    duration = end_time - start_time\n    print(f\"\\nTotal parallel translation time: {duration:.4f} seconds\")\n    print(\"\\n=== Translation Complete ===\")\n    print(f\"Translations saved to: {shared['output_dir']}\")\n    print(\"============================\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main()) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-parallel-batch/main.py`:**\n\n1. **Primary Purpose:**  \n   This code automates the process of translating a source Markdown README file (\u201c../../README.md\u201d) into multiple target languages in parallel using large language model (LLM) calls, then saves each translation as a separate file. It achieves parallelism and performance using the PocketFlow asynchronous batch flow framework.\n\n2. **Parameters:**  \n   - **Main function parameters:** None; the script runs directly, but internally it reads:\n     - The README file path: hardcoded as `\"../../README.md\"`\n     - Target languages: hardcoded in a list (`[\"Chinese\", \"Spanish\", ...]`)\n     - Output directory: hardcoded as `\"translations\"`\n   - **Class parameters:** The `TranslateTextNodeParallel` node accepts an optional keyword argument `max_retries` (default here is 3).\n   - **Shared dictionary:** Passed internally to the flow, containing the source text, languages, and output directory.\n\n3. **Return Value:**  \n   - The top-level script has no return value (it just runs and prints to stdout).\n   - The translation step returns a dictionary with `{language, translation}` for each language and files for each translation, though the main value is side effects (saved files). The `post_async` method returns the string `\"default\"` but this is used internally by the flow.\n\n4. **Functions or Methods Called Internally:**  \n   - **From PocketFlow:**  \n     - `AsyncFlow`\n     - `AsyncParallelBatchNode`\n   - **Custom/utility functions:**  \n     - `call_llm(prompt)` from `utils` \u2013 the core LLM call\n   - **Python built-ins and standard library:**  \n     - `asyncio.run(main())`\n     - File I/O (`open`, `os.makedirs`, paths)\n     - Timing (`time.perf_counter`)\n   - **Async file utility (optional):**\n     - `aiofiles.open` (if `aiofiles` is available; otherwise falls back to synchronous file writing)\n   - **Flow methods:**  \n     - `prep_async`, `exec_async`, `post_async` (node lifecycle methods)\n     - `run_async(shared)` (running the async flow)\n\n**In summary:**\nThis script reads a README file, translates it in parallel to multiple languages using an LLM, and saves each translation as a Markdown file, leveraging async flow orchestration and handling file output. The main action is initiated via `asyncio.run(main())`, and most user-supplied information is hardcoded in `shared`. The script interacts with various flow methods and utility functions for translation and file management."
    },
    "10": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-parallel-batch/utils.py",
      "code": "import os\nimport asyncio\nfrom anthropic import AsyncAnthropic\n\n# Async version of the simple wrapper, using Anthropic\nasync def call_llm(prompt):\n    \"\"\"Async wrapper for Anthropic API call.\"\"\"\n    client = AsyncAnthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = await client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=20000,\n        thinking={\n            \"type\": \"enabled\",\n            \"budget_tokens\": 16000\n        },\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n    )\n    return response.content[1].text\n\nif __name__ == \"__main__\":\n    async def run_test():\n        print(\"## Testing async call_llm with Anthropic\")\n        prompt = \"In a few words, what is the meaning of life?\"\n        print(f\"## Prompt: {prompt}\")\n        response = await call_llm(prompt)\n        print(f\"## Response: {response}\")\n\n    asyncio.run(run_test()) ",
      "summary": "**Summary of Code Unit: `utils.py`**\n\n1. **Primary Purpose:**  \n   The code provides an asynchronous utility function to call the Anthropic large language model (LLM) API (using the `AsyncAnthropic` client) with a user-supplied prompt, then retrieve and return the model's textual response. It also includes a short test harness that runs this function with a sample prompt if the script is executed directly.\n\n2. **Brief Description of Parameters:**  \n   - The main function, `call_llm`, takes a single parameter:\n     - `prompt`: A string containing the user's question or input for the language model.\n\n3. **Brief Description of Return Value:**  \n   - The `call_llm` function returns a string containing the response text generated by the Anthropic LLM for the supplied prompt.\n\n4. **Other Functions or Methods Called Internally:**  \n   - `AsyncAnthropic` constructor (from the `anthropic` package)\n   - `AsyncAnthropic.messages.create` (the API call to the Anthropic model)\n   - `os.environ.get` (to retrieve the API key from environment variables)\n   - `asyncio.run` (to run the async test function if run as a script)\n   - The script also references the `content[1].text` property from the response, though (note: this may need checking based on Anthropic API specifications)."
    },
    "11": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-nested-batch/nodes.py",
      "code": "import os\nfrom pocketflow import Node\n\nclass LoadGrades(Node):\n    \"\"\"Node that loads grades from a student's file.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get file path from parameters.\"\"\"\n        class_name = self.params[\"class\"]\n        student_file = self.params[\"student\"]\n        return os.path.join(\"school\", class_name, student_file)\n    \n    def exec(self, file_path):\n        \"\"\"Load and parse grades from file.\"\"\"\n        with open(file_path, 'r') as f:\n            # Each line is a grade\n            grades = [float(line.strip()) for line in f]\n        return grades\n    \n    def post(self, shared, prep_res, grades):\n        \"\"\"Store grades in shared store.\"\"\"\n        shared[\"grades\"] = grades\n        return \"calculate\"\n\nclass CalculateAverage(Node):\n    \"\"\"Node that calculates average grade.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get grades from shared store.\"\"\"\n        return shared[\"grades\"]\n    \n    def exec(self, grades):\n        \"\"\"Calculate average.\"\"\"\n        return sum(grades) / len(grades)\n    \n    def post(self, shared, prep_res, average):\n        \"\"\"Store and print result.\"\"\"\n        # Store in results dictionary\n        if \"results\" not in shared:\n            shared[\"results\"] = {}\n        \n        class_name = self.params[\"class\"]\n        student = self.params[\"student\"]\n        \n        if class_name not in shared[\"results\"]:\n            shared[\"results\"][class_name] = {}\n            \n        shared[\"results\"][class_name][student] = average\n        \n        # Print individual result\n        print(f\"- {student}: Average = {average:.1f}\")\n        return \"default\" ",
      "summary": "**Summary of nodes.py**\n\n1. **Primary Purpose:**  \n   The code unit defines two workflow nodes used for processing student grades in a classroom context as part of a pipeline (likely using PocketFlow). The first node loads a student's grades from a file, and the second node calculates and stores the student's average grade, organizing results by class and student.\n\n2. **Parameters:**  \n   - Both **LoadGrades** and **CalculateAverage** nodes expect parameters provided in `self.params`:\n     - `\"class\"`: The class name, used to locate/store results.\n     - `\"student\"`: The student file name (for `LoadGrades`) or student name (for storing the result in `CalculateAverage`).\n\n3. **Return Value:**  \n   - **LoadGrades**:  \n     - `prep`: Returns a file path string to the student's grade file.\n     - `exec`: Returns a list of float grades loaded from the file.\n     - `post`: Stores grades in a shared dictionary and returns a string indicating the next action (\"calculate\").\n   - **CalculateAverage**:  \n     - `prep`: Returns the list of grades retrieved from the shared dictionary.\n     - `exec`: Returns a float representing the average grade.\n     - `post`: Stores the calculated average in the shared results structure, prints the result, and returns a string (\"default\").\n\n4. **Other Functions or Methods Called Internally:**  \n   - `os.path.join` (constructs file paths)\n   - Built-in functions: `open` (file reading), `float` (conversion), `sum` and `len` (average calculation), `print`\n   - List comprehensions for processing file lines\n\nNo calls to custom or third-party functions outside of the standard library and the parent Node class."
    },
    "12": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-nested-batch/flow.py",
      "code": "import os\nfrom pocketflow import Flow, BatchFlow\nfrom nodes import LoadGrades, CalculateAverage\n\ndef create_base_flow():\n    \"\"\"Create base flow for processing one student's grades.\"\"\"\n    # Create nodes\n    load = LoadGrades()\n    calc = CalculateAverage()\n    \n    # Connect nodes\n    load - \"calculate\" >> calc\n    \n    # Create and return flow\n    return Flow(start=load)\n\nclass ClassBatchFlow(BatchFlow):\n    \"\"\"BatchFlow for processing all students in a class.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Generate parameters for each student in the class.\"\"\"\n        # Get class folder from parameters\n        class_folder = self.params[\"class\"]\n        \n        # List all student files\n        class_path = os.path.join(\"school\", class_folder)\n        students = [f for f in os.listdir(class_path) if f.endswith(\".txt\")]\n        \n        # Return parameters for each student\n        return [{\"student\": student} for student in students]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Calculate and print class average.\"\"\"\n        class_name = self.params[\"class\"]\n        class_results = shared[\"results\"][class_name]\n        class_average = sum(class_results.values()) / len(class_results)\n        \n        print(f\"Class {class_name.split('_')[1].upper()} Average: {class_average:.2f}\\n\")\n        return \"default\"\n\nclass SchoolBatchFlow(BatchFlow):\n    \"\"\"BatchFlow for processing all classes in the school.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Generate parameters for each class.\"\"\"\n        # List all class folders\n        classes = [d for d in os.listdir(\"school\") if os.path.isdir(os.path.join(\"school\", d))]\n        \n        # Return parameters for each class\n        return [{\"class\": class_name} for class_name in classes]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Calculate and print school average.\"\"\"\n        all_grades = []\n        for class_results in shared[\"results\"].values():\n            all_grades.extend(class_results.values())\n            \n        school_average = sum(all_grades) / len(all_grades)\n        print(f\"School Average: {school_average:.2f}\")\n        return \"default\"\n\ndef create_flow():\n    \"\"\"Create the complete nested batch processing flow.\"\"\"\n    # Create base flow for single student\n    base_flow = create_base_flow()\n    \n    # Wrap in ClassBatchFlow for processing all students in a class\n    class_flow = ClassBatchFlow(start=base_flow)\n    \n    # Wrap in SchoolBatchFlow for processing all classes\n    school_flow = SchoolBatchFlow(start=class_flow)\n    \n    return school_flow ",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary Purpose:**  \n   This code defines a configurable, nested batch-processing data flow for calculating student, class, and school-wide grade averages using the PocketFlow framework. It enables automated computation by processing each student's grades, aggregating results per class, and then computing the overall school average.\n\n2. **Parameters:**  \n   - The main configuration parameters are `\"student\"` and `\"class\"`, which are used internally by the flows:\n     - `\"student\"`: filename for an individual student's grade data (in `ClassBatchFlow.prep`).\n     - `\"class\"`: the folder name of a class (in `SchoolBatchFlow.prep` and elsewhere).\n   - These parameters are expected to be provided in each batch's context by the framework.\n\n3. **Return Values:**  \n   - The `create_base_flow()` and `create_flow()` functions return a configured `Flow` instance and a `SchoolBatchFlow` instance, respectively.\n   - The `prep` methods return lists of dictionaries specifying the parameters for each batch run (students or classes).\n   - The `post` methods return a string (always \"default\"), which likely dictates the next action in the batch processing control flow.\n\n4. **Other Functions or Methods Called Internally:**\n   - `os.path.join()`\n   - `os.listdir()`\n   - `os.path.isdir()`\n   - `sum()`, `len()`, `print()`\n   - `Flow` and `BatchFlow` constructors from `pocketflow`\n   - `LoadGrades`, `CalculateAverage` (custom nodes, likely user-defined elsewhere)\n   - Node connection operators: `load - \"calculate\" >> calc`\n   - Base class method overrides: `prep`, `post`\n\n**In summary:**  \nThe file builds a hierarchical batch flow that efficiently computes averages at various levels (student, class, school) by leveraging modular definitions, directory traversal, and custom node processing within the PocketFlow batch-processing framework. The main entry point for creating this flow is the `create_flow()` function, which returns the full nested structure ready for execution by the framework."
    },
    "13": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-nested-batch/main.py",
      "code": "import os\nfrom flow import create_flow\n\ndef create_sample_data():\n    \"\"\"Create sample grade files.\"\"\"\n    # Create directory structure\n    os.makedirs(\"school/class_a\", exist_ok=True)\n    os.makedirs(\"school/class_b\", exist_ok=True)\n    \n    # Sample grades\n    data = {\n        \"class_a\": {\n            \"student1.txt\": [7.5, 8.0, 9.0],\n            \"student2.txt\": [8.5, 7.0, 9.5]\n        },\n        \"class_b\": {\n            \"student3.txt\": [6.5, 8.5, 7.0],\n            \"student4.txt\": [9.0, 9.5, 8.0]\n        }\n    }\n    \n    # Create files\n    for class_name, students in data.items():\n        for student, grades in students.items():\n            file_path = os.path.join(\"school\", class_name, student)\n            with open(file_path, 'w') as f:\n                for grade in grades:\n                    f.write(f\"{grade}\\n\")\n\ndef main():\n    \"\"\"Run the nested batch example.\"\"\"\n    # Create sample data\n    create_sample_data()\n    \n    print(\"Processing school grades...\\n\")\n    \n    # Create and run flow\n    flow = create_flow()\n    flow.run({})\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of main.py:**\n\n1. **Primary purpose:**  \n   The code sets up a demonstration for processing school grade files. It first generates a sample directory and file structure with student grades, then triggers a workflow (or \"flow\") to process this data, leveraging a function (`create_flow`) from an external module.\n\n2. **Parameters:**  \n   The main functions (`create_sample_data` and `main`) do **not** accept any input parameters.\n\n3. **Return value:**  \n   Neither `create_sample_data` nor `main` returns any values; both have a return type of `None`.\n\n4. **Internal function/method calls:**  \n   - `os.makedirs()` (for directory creation)\n   - `os.path.join()` (for path construction)\n   - `open()` (for file writing)\n   - `create_flow()` (imported from `flow`)\n   - `flow.run()` (method of the flow object)"
    },
    "14": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat/main.py",
      "code": "from pocketflow import Node, Flow\nfrom utils import call_llm\n\nclass ChatNode(Node):\n    def prep(self, shared):\n        # Initialize messages if this is the first run\n        if \"messages\" not in shared:\n            shared[\"messages\"] = []\n            print(\"Welcome to the chat! Type 'exit' to end the conversation.\")\n        \n        # Get user input\n        user_input = input(\"\\nYou: \")\n        \n        # Check if user wants to exit\n        if user_input.lower() == 'exit':\n            return None\n        \n        # Add user message to history\n        shared[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n        \n        # Return all messages for the LLM\n        return shared[\"messages\"]\n\n    def exec(self, messages):\n        if messages is None:\n            return None\n        \n        # Call LLM with the entire conversation history\n        response = call_llm(messages)\n        return response\n\n    def post(self, shared, prep_res, exec_res):\n        if prep_res is None or exec_res is None:\n            print(\"\\nGoodbye!\")\n            return None  # End the conversation\n        \n        # Print the assistant's response\n        print(f\"\\nAssistant: {exec_res}\")\n        \n        # Add assistant message to history\n        shared[\"messages\"].append({\"role\": \"assistant\", \"content\": exec_res})\n        \n        # Loop back to continue the conversation\n        return \"continue\"\n\n# Create the flow with self-loop\nchat_node = ChatNode()\nchat_node - \"continue\" >> chat_node  # Loop back to continue conversation\n\nflow = Flow(start=chat_node)\n\n# Start the chat\nif __name__ == \"__main__\":\n    shared = {}\n    flow.run(shared)\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-chat/main.py`:**\n\n1. **Primary purpose:**  \n   This code implements a command-line chatbot loop using the PocketFlow framework. It facilitates a conversational interaction between a user and a language model (LLM), maintaining the context of the conversation across turns, until the user decides to exit.\n\n2. **Parameters:**  \n   - The main code does not accept external parameters via command-line or function arguments.\n   - The `ChatNode` methods:\n     - `prep(self, shared)`: `shared` is a dictionary used to store and maintain the conversation history between turns.\n     - `exec(self, messages)`: `messages` is a list of dictionaries representing the conversation's message history.\n     - `post(self, shared, prep_res, exec_res)`: receives the `shared` dict, the result of `prep` (`prep_res`), and the result of `exec` (`exec_res`).\n\n3. **Return value:**  \n   - The script itself does not return a value; it runs an interactive loop.\n   - The `prep` method returns either `None` (to end) or the message list (to continue).\n   - The `exec` method returns the assistant's response string or `None`.\n   - The `post` method returns `\"continue\"` (to keep chatting) or `None` (to exit).\n\n4. **Internal function/method calls:**  \n   - `call_llm`: Invoked to get an LLM-generated response based on message history.\n   - Standard library calls: `input` (to read user input) and `print` (to display messages).\n   - PocketFlow framework methods: \n     - Instantiation and configuration of `Node` and `Flow`.\n     - `flow.run(shared)` to start the chat loop.\n   - Internal class method references: `prep`, `exec`, `post` of `ChatNode`.\n\n**In essence:** The code creates and runs a looped, interactive chat between a user and an LLM, handling conversation state and ending gracefully when the user types 'exit'."
    },
    "15": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat/utils.py",
      "code": "from openai import OpenAI\nimport os\n\ndef call_llm(messages):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test the LLM call\n    messages = [{\"role\": \"user\", \"content\": \"In a few words, what's the meaning of life?\"}]\n    response = call_llm(messages)\n    print(f\"Prompt: {messages[0]['content']}\")\n    print(f\"Response: {response}\")\n\n",
      "summary": "**Summary of Code Unit: `utils.py`**\n\n1. **Primary purpose:**  \n   The code provides a utility function to interact with OpenAI's large language models (LLMs) by sending a message and retrieving the model's textual response.\n\n2. **Parameters:**  \n   - The main function, `call_llm(messages)`, takes a single parameter:  \n     - `messages`: A list of message dictionaries following the OpenAI chat API format (each dict typically has \"role\" and \"content\" keys).\n\n3. **Return value:**  \n   - The function returns the content string of the first message in the response from the OpenAI model, which represents the model's reply.\n\n4. **Other functions or methods called internally:**  \n   - `OpenAI(...)`: Initializes the OpenAI client with an API key.\n   - `client.chat.completions.create(...)`: Sends the message(s) to the chat completion endpoint.\n   - Environment variable access: `os.environ.get(...)` for obtaining the API key."
    },
    "16": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-database/nodes.py",
      "code": "from pocketflow import Node\nfrom tools.database import execute_sql, init_db\n\nclass InitDatabaseNode(Node):\n    \"\"\"Node for initializing the database\"\"\"\n    \n    def exec(self, _):\n        init_db()\n        return \"Database initialized\"\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"db_status\"] = exec_res\n        return \"default\"\n\nclass CreateTaskNode(Node):\n    \"\"\"Node for creating a new task\"\"\"\n    \n    def prep(self, shared):\n        return (\n            shared.get(\"task_title\", \"\"),\n            shared.get(\"task_description\", \"\")\n        )\n        \n    def exec(self, inputs):\n        title, description = inputs\n        query = \"INSERT INTO tasks (title, description) VALUES (?, ?)\"\n        execute_sql(query, (title, description))\n        return \"Task created successfully\"\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"task_status\"] = exec_res\n        return \"default\"\n\nclass ListTasksNode(Node):\n    \"\"\"Node for listing all tasks\"\"\"\n    \n    def exec(self, _):\n        query = \"SELECT * FROM tasks\"\n        return execute_sql(query)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"tasks\"] = exec_res\n        return \"default\"\n",
      "summary": "**Summary of nodes.py**\n\n1. **Primary purpose of the code unit:**\n   - This code defines three workflow nodes for a PocketFlow-based system, enabling initialization of a database and basic task management operations (creating and listing tasks) via nodes.\n\n2. **Brief description of its parameters:**\n   - Methods in the node classes generally take the following parameters:\n     - `shared`: A dictionary-like object passed between nodes to share state and information.\n     - `prep_res`, `exec_res`: Results passed between the stages (`prep`, `exec`, `post`) of each node, reflecting their respective outputs.\n     - `inputs`: Used to convey parameters necessary for the node's main execution logic (e.g., task title and description).\n     - The `exec` method for listing and initializing tasks takes a single (unused) parameter conventionally named `_`.\n\n3. **Brief description of its return value:**\n   - Each node's `exec` method returns a status message or result:\n     - `InitDatabaseNode.exec` returns a confirmation string.\n     - `CreateTaskNode.exec` returns a task creation confirmation string.\n     - `ListTasksNode.exec` returns the query result from selecting all tasks.\n   - Each `post` method stores relevant results in the `shared` dictionary and returns a routing value (\"default\").\n\n4. **List of other functions or methods it calls internally:**\n   - `init_db()` \u2013 initializes the database.\n   - `execute_sql()` \u2013 executes SQL commands (both insertion and selection) on the database.\n   - Node lifecycle methods (`prep`, `exec`, `post`) are part of the custom node interface from `pocketflow.Node`.\n\n**In summary:**  \n`nodes.py` defines three node classes for initializing a database, creating tasks, and listing tasks, using utility functions from the application's database module. Each node follows a standard pattern for preparation, execution, and post-processing, interacting with SQL operations as required."
    },
    "17": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-database/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import InitDatabaseNode, CreateTaskNode, ListTasksNode\n\ndef create_database_flow():\n    \"\"\"Create a flow for database operations\"\"\"\n    \n    # Create nodes\n    init_db = InitDatabaseNode()\n    create_task = CreateTaskNode()\n    list_tasks = ListTasksNode()\n    \n    # Connect nodes\n    init_db >> create_task >> list_tasks\n    \n    # Create and return flow\n    return Flow(start=init_db)\n",
      "summary": "**Summary of `flow.py`**\n\n1. **Primary purpose:**  \n   This code defines a function to construct and return a workflow (\"flow\") that manages database operations, including initializing the database, creating a task, and listing tasks.\n\n2. **Parameters:**  \n   The main function `create_database_flow()` does **not** take any parameters.\n\n3. **Return value:**  \n   `create_database_flow()` returns an instance of `Flow`, which represents the entire constructed workflow beginning with the initialization node.\n\n4. **Other functions or methods called internally:**  \n   - `InitDatabaseNode()` \u2014 initializes the database node.\n   - `CreateTaskNode()` \u2014 creates a node responsible for making a new task in the database.\n   - `ListTasksNode()` \u2014 creates a node for listing existing tasks in the database.\n   - `Flow(start=init_db)` \u2014 initializes the `Flow` with the first node.\n   - The `>>` operator \u2014 connects the nodes in sequence (this likely uses an overloaded method, e.g., `__rshift__`, to define execution order)."
    },
    "18": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-database/main.py",
      "code": "from flow import create_database_flow\n\ndef main():\n    # Create the flow\n    flow = create_database_flow()\n    \n    # Prepare example task data\n    shared = {\n        \"task_title\": \"Example Task\",\n        \"task_description\": \"This is an example task created using PocketFlow\"\n    }\n    \n    # Run the flow\n    flow.run(shared)\n    \n    # Print results\n    print(\"Database Status:\", shared.get(\"db_status\"))\n    print(\"Task Status:\", shared.get(\"task_status\"))\n    print(\"\\nAll Tasks:\")\n    for task in shared.get(\"tasks\", []):\n        print(f\"- ID: {task[0]}\")\n        print(f\"  Title: {task[1]}\")\n        print(f\"  Description: {task[2]}\")\n        print(f\"  Status: {task[3]}\")\n        print(f\"  Created: {task[4]}\")\n        print()\n\nif __name__ == \"__main__\":\n    main()\n",
      "summary": "**Summary of main.py:**\n\n1. **Primary Purpose:**  \n   The code serves as an executable script to create and run a database-related workflow using PocketFlow. It initializes a sample task, executes the workflow, and prints the results, including database status, task status, and a list of all tasks.\n\n2. **Parameters:**  \n   The `main()` function and the script itself do not accept any parameters.\n\n3. **Return Value:**  \n   The `main()` function does not return any value; it performs its actions for side effects, such as running the workflow and printing outputs.\n\n4. **Internally Called Functions/Methods:**  \n   - `create_database_flow()` (imported from `flow`)\n   - `flow.run(shared)` (where `flow` is the object returned by `create_database_flow()`)\n   - Standard dictionary methods, e.g., `shared.get()`"
    },
    "19": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-database/utils/__init__.py",
      "code": "",
      "summary": "There is no code provided in the file '/tmp/PocketFlow/cookbook/pocketflow-tool-database/utils/__init__.py'\u2014the code unit appears to be empty.\n\nSummary:\n1. **Primary purpose:** The code unit, as presented, serves no functional purpose. If this is an actual `__init__.py` file with no content, its purpose is likely to mark the directory as a Python package.\n2. **Parameters:** No parameters are present.\n3. **Return value:** No return values are produced.\n4. **Internally called functions/methods:** None are called.\n\nIf you provide code content within the file, I can analyze it according to your requirements."
    },
    "20": {
      "unit_name": "database.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-database/tools/database.py",
      "code": "import sqlite3\nfrom typing import List, Tuple, Any\n\ndef execute_sql(query: str, params: Tuple = None) -> List[Tuple[Any, ...]]:\n    \"\"\"Execute a SQL query and return results\n    \n    Args:\n        query (str): SQL query to execute\n        params (tuple, optional): Query parameters to prevent SQL injection\n        \n    Returns:\n        list: Query results as a list of tuples\n    \"\"\"\n    conn = sqlite3.connect(\"example.db\")\n    try:\n        cursor = conn.cursor()\n        if params:\n            cursor.execute(query, params)\n        else:\n            cursor.execute(query)\n        result = cursor.fetchall()\n        conn.commit()\n        return result\n    finally:\n        conn.close()\n\ndef init_db():\n    \"\"\"Initialize database with example table\"\"\"\n    create_table_sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS tasks (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        title TEXT NOT NULL,\n        description TEXT,\n        status TEXT DEFAULT 'pending',\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    )\n    \"\"\"\n    execute_sql(create_table_sql)\n",
      "summary": "**Summary:**\n\n1. **Primary purpose:**  \n   This code unit provides basic utilities to initialize and interact with an SQLite database, specifically for executing SQL queries and setting up an example \"tasks\" table.\n\n2. **Parameters:**\n   - For `execute_sql`:  \n     - `query` (str): The SQL statement to be executed.  \n     - `params` (tuple, optional): Parameters to safely substitute into the SQL query and prevent SQL injection.\n   - For `init_db`:  \n     - No parameters.\n\n3. **Return value:**\n   - `execute_sql` returns a list of tuples, where each tuple represents a row from the query result.\n   - `init_db` does not return a value.\n\n4. **Other functions/methods called internally:**  \n   - Both functions call `execute_sql`.  \n   - Uses methods from Python\u2019s `sqlite3` module: `sqlite3.connect`, `cursor`, `execute`, `fetchall`, `commit`, and `close`."
    },
    "21": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch-flow/nodes.py",
      "code": "\"\"\"Node implementations for image processing.\"\"\"\n\nimport os\nfrom PIL import Image, ImageEnhance, ImageFilter\nfrom pocketflow import Node\n\nclass LoadImage(Node):\n    \"\"\"Node that loads an image file.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get image path from parameters.\"\"\"\n        return os.path.join(\"images\", self.params[\"input\"])\n    \n    def exec(self, image_path):\n        \"\"\"Load the image using PIL.\"\"\"\n        return Image.open(image_path)\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the image in shared store.\"\"\"\n        shared[\"image\"] = exec_res\n        return \"apply_filter\"\n\nclass ApplyFilter(Node):\n    \"\"\"Node that applies a filter to an image.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get image and filter type.\"\"\"\n        return shared[\"image\"], self.params[\"filter\"]\n    \n    def exec(self, inputs):\n        \"\"\"Apply the specified filter.\"\"\"\n        image, filter_type = inputs\n        \n        if filter_type == \"grayscale\":\n            return image.convert(\"L\")\n        elif filter_type == \"blur\":\n            return image.filter(ImageFilter.BLUR)\n        elif filter_type == \"sepia\":\n            # Sepia implementation\n            enhancer = ImageEnhance.Color(image)\n            grayscale = enhancer.enhance(0.3)\n            colorize = ImageEnhance.Brightness(grayscale)\n            return colorize.enhance(1.2)\n        else:\n            raise ValueError(f\"Unknown filter: {filter_type}\")\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the filtered image.\"\"\"\n        shared[\"filtered_image\"] = exec_res\n        return \"save\"\n\nclass SaveImage(Node):\n    \"\"\"Node that saves the processed image.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get filtered image and prepare output path.\"\"\"\n        # Create output directory if it doesn't exist\n        os.makedirs(\"output\", exist_ok=True)\n        \n        # Generate output filename\n        input_name = os.path.splitext(self.params[\"input\"])[0]\n        filter_name = self.params[\"filter\"]\n        output_path = os.path.join(\"output\", f\"{input_name}_{filter_name}.jpg\")\n        \n        return shared[\"filtered_image\"], output_path\n    \n    def exec(self, inputs):\n        \"\"\"Save the image to file.\"\"\"\n        image, output_path = inputs\n        image.save(output_path, \"JPEG\")\n        return output_path\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Print success message.\"\"\"\n        print(f\"Saved filtered image to: {exec_res}\")\n        return \"default\" ",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary purpose of the code unit:**  \n   This code defines a set of node classes for an image processing workflow, intended to be used as part of a batch-processing pipeline (likely in PocketFlow). The nodes handle loading an image, applying a filter, and saving the processed image.\n\n2. **Brief description of its parameters:**  \n   - Each node relies on `self.params`, a dictionary of parameters provided per node:\n     - `\"input\"`: the image filename to load/process.\n     - `\"filter\"`: the type of filter to apply (e.g., `\"grayscale\"`, `\"blur\"`, `\"sepia\"`).\n   - Methods also receive:\n     - `shared`: a dictionary for passing data between nodes.\n     - `inputs`: a tuple of function-specific values (e.g., image and filter type).\n\n3. **Brief description of its return values:**  \n   - Most `prep` and `exec` methods return intermediate data required for the next step (e.g., file paths, image objects).\n   - `post` methods typically update the shared data store and return a string indicating the next step in the workflow.\n   - `SaveImage.post` also prints a message and returns \"default\".\n   - The ultimate output is the processed (filtered) image saved to the disk.\n\n4. **List of functions or methods called internally:**\n   - `os.path.join`\n   - `os.path.splitext`\n   - `os.makedirs`\n   - `Image.open` (PIL)\n   - `Image.filter` (PIL)\n   - `Image.convert` (PIL)\n   - `ImageEnhance.Color` (PIL)\n   - `ImageEnhance.Brightness` (PIL)\n   - `enhancer.enhance`\n   - `colorize.enhance`\n   - `image.save` (PIL)\n   - `print`\n   - `ValueError` (raising exceptions)\n   \nThe code expects to operate inside a framework (PocketFlow) where each node gets parameters and can share data with others using a `shared` dictionary."
    },
    "22": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch-flow/flow.py",
      "code": "from pocketflow import Flow, BatchFlow\nfrom nodes import LoadImage, ApplyFilter, SaveImage\n\ndef create_base_flow():\n    \"\"\"Create the base Flow for processing a single image.\"\"\"\n    # Create nodes\n    load = LoadImage()\n    filter_node = ApplyFilter()\n    save = SaveImage()\n    \n    # Connect nodes\n    load - \"apply_filter\" >> filter_node\n    filter_node - \"save\" >> save\n    \n    # Create and return flow\n    return Flow(start=load)\n\nclass ImageBatchFlow(BatchFlow):\n    \"\"\"BatchFlow for processing multiple images with different filters.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Generate parameters for each image-filter combination.\"\"\"\n        # List of images to process\n        images = [\"cat.jpg\", \"dog.jpg\", \"bird.jpg\"]\n        \n        # List of filters to apply\n        filters = [\"grayscale\", \"blur\", \"sepia\"]\n        \n        # Generate all combinations\n        params = []\n        for img in images:\n            for f in filters:\n                params.append({\n                    \"input\": img,\n                    \"filter\": f\n                })\n        \n        return params\n\ndef create_flow():\n    \"\"\"Create the complete batch processing flow.\"\"\"\n    # Create base flow for single image processing\n    base_flow = create_base_flow()\n    \n    # Wrap in BatchFlow for multiple images\n    batch_flow = ImageBatchFlow(start=base_flow)\n    \n    return batch_flow ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-batch-flow/flow.py`:**\n\n1. **Primary Purpose:**  \n   This code defines a batch image-processing workflow using the PocketFlow framework. It sets up a single-image processing pipeline (loading, filtering, saving) and extends it to process multiple images, each with multiple filters, by generating all filter-image combinations.\n\n2. **Parameters:**  \n   - The functions themselves do not accept external parameters.\n   - Internally, the `prep` method of `ImageBatchFlow` receives a `shared` argument (not used within the method).\n   - The generated parameters for each batch item include `\"input\"` (the image filename) and `\"filter\"` (the filter name).\n\n3. **Return Values:**  \n   - `create_base_flow()`: Returns a `Flow` object representing the processing pipeline for a single image.\n   - `ImageBatchFlow.prep()`: Returns a list of parameter dictionaries, covering all image/filter combinations.\n   - `create_flow()`: Returns an `ImageBatchFlow` object for batch processing all combinations.\n\n4. **Functions/Methods Called Internally:**  \n   - `LoadImage()` (node instantiation)\n   - `ApplyFilter()` (node instantiation)\n   - `SaveImage()` (node instantiation)\n   - `Flow()` (to create a single-image flow)\n   - `ImageBatchFlow()` (to create the batch image processing flow)\n   - Flow composition operators (`-`, `>>`) to construct node connections.\n\n**Summary:**  \nThis code defines and returns a PocketFlow workflow for batch processing images, generating all permutations of three images and three filters, and connecting nodes to load, filter, and save each image."
    },
    "23": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch-flow/main.py",
      "code": "import os\nfrom PIL import Image\nimport numpy as np\nfrom flow import create_flow\n\ndef main():\n    # Create and run flow\n    print(\"Processing images with filters...\")\n    \n    flow = create_flow()\n    flow.run({}) \n    \n    print(\"\\nAll images processed successfully!\")\n    print(\"Check the 'output' directory for results.\")\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of main.py:**\n\n1. **Primary purpose:**  \n   This code unit serves as the entry point for an image processing pipeline. It initializes and executes a flow that processes images (presumably applying filters) and informs the user upon completion.\n\n2. **Parameters:**  \n   The `main` function, and the script itself, do not accept any parameters.\n\n3. **Return value:**  \n   There is no explicit return value from the `main` function or the script; its main effect is to carry out image processing and print informational messages.\n\n4. **Functions/methods called internally:**  \n   - `create_flow()` (imported from the `flow` module)  \n   - `flow.run({})` (method called on the object returned by `create_flow()`)  \n   - `print()` (standard Python print function)"
    },
    "24": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-websocket/nodes.py",
      "code": "import asyncio\nimport json\nfrom pocketflow import AsyncNode\nfrom utils.stream_llm import stream_llm\n\nclass StreamingChatNode(AsyncNode):\n    async def prep_async(self, shared):\n        user_message = shared.get(\"user_message\", \"\")\n        websocket = shared.get(\"websocket\")\n        \n        conversation_history = shared.get(\"conversation_history\", [])\n        conversation_history.append({\"role\": \"user\", \"content\": user_message})\n        \n        return conversation_history, websocket\n    \n    async def exec_async(self, prep_res):\n        messages, websocket = prep_res\n        \n        await websocket.send_text(json.dumps({\"type\": \"start\", \"content\": \"\"}))\n        \n        full_response = \"\"\n        async for chunk_content in stream_llm(messages):\n            full_response += chunk_content\n            await websocket.send_text(json.dumps({\n                \"type\": \"chunk\", \n                \"content\": chunk_content\n            }))\n        \n        await websocket.send_text(json.dumps({\"type\": \"end\", \"content\": \"\"}))\n        \n        return full_response, websocket\n    \n    async def post_async(self, shared, prep_res, exec_res):\n        full_response, websocket = exec_res\n        \n        conversation_history = shared.get(\"conversation_history\", [])\n        conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n        shared[\"conversation_history\"] = conversation_history ",
      "summary": "**Summary of `nodes.py` (unit: StreamingChatNode)**\n\n1. **Primary Purpose:**  \n   This code defines an asynchronous node, `StreamingChatNode`, for streaming chat responses (presumably from an LLM) over a WebSocket connection in a PocketFlow/FASTAPI application. It processes a user's message, streams the assistant's reply chunk by chunk to the client via WebSocket, and updates the conversation history.\n\n2. **Parameters:**  \n   - The methods use `shared`: a dictionary-like object with at least the following possible keys:\n     - `\"user_message\"`: the user's current message (string).\n     - `\"websocket\"`: the connected WebSocket object.\n     - `\"conversation_history\"`: a list of prior conversation turns (list of dicts).\n\n   - Internal workflow between methods passes:\n     - `prep_async` returns `(conversation_history, websocket)` to `exec_async`.\n     - `exec_async` returns `(full_response, websocket)` to `post_async`.\n\n3. **Return Value:**  \n   - `prep_async`: returns an updated conversation history (with the new user message added) and the websocket.\n   - `exec_async`: returns the full assistant-generated response and the websocket.\n   - `post_async`: updates the shared conversation history with the assistant's reply (no explicit return).\n\n4. **Internal Calls to Other Functions or Methods:**  \n   - `stream_llm(messages)`: Async generator to stream LLM response content.\n   - `websocket.send_text(...)`: Sends data (as JSON) over the WebSocket.\n   - `json.dumps(...)`: Serializes data to JSON format.\n   - Methods from the superclass (`AsyncNode`) may also be used as part of the larger framework, though not explicitly in this code snippet."
    },
    "25": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-websocket/flow.py",
      "code": "from pocketflow import AsyncFlow\nfrom nodes import StreamingChatNode\n\ndef create_streaming_chat_flow():\n    chat_node = StreamingChatNode()\n    return AsyncFlow(start=chat_node) ",
      "summary": "1. **Primary purpose:**  \nThe code defines a function to create and return an asynchronous flow for a streaming chat operation by initializing a chat node and passing it as the start point of the flow.\n\n2. **Parameters:**  \nThe function `create_streaming_chat_flow()` does not take any parameters.\n\n3. **Return value:**  \nThe function returns an `AsyncFlow` object, initialized with a `StreamingChatNode` as its starting node.\n\n4. **Other functions or methods called internally:**  \n- `StreamingChatNode()` constructor  \n- `AsyncFlow()` constructor"
    },
    "26": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-websocket/main.py",
      "code": "import json\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import FileResponse\nfrom flow import create_streaming_chat_flow\n\napp = FastAPI()\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n@app.get(\"/\")\nasync def get_chat_interface():\n    return FileResponse(\"static/index.html\")\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    \n    # Initialize conversation history for this connection\n    shared_store = {\n        \"websocket\": websocket,\n        \"conversation_history\": []\n    }\n    \n    try:\n        while True:\n            data = await websocket.receive_text()\n            message = json.loads(data)\n            \n            # Update only the current message, keep conversation history\n            shared_store[\"user_message\"] = message.get(\"content\", \"\")\n            \n            flow = create_streaming_chat_flow()\n            await flow.run_async(shared_store)\n            \n    except WebSocketDisconnect:\n        pass\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000) ",
      "summary": "**Summary of main.py**\n\n1. **Primary purpose of the code unit:**  \n   The code defines a FastAPI application that serves a simple web-based chat interface at the root URL and handles real-time, bi-directional chat via WebSockets. It uses a function (`create_streaming_chat_flow`) from an external `flow` module to process and respond to chat messages, maintaining per-connection conversation history.\n\n2. **Brief description of its parameters:**  \n   - The `websocket_endpoint` function accepts a `WebSocket` object (parameter: `websocket`) provided automatically by FastAPI for WebSocket connections.\n   - The root HTTP GET route (`get_chat_interface`) accepts no parameters.\n\n3. **Brief description of its return value:**  \n   - `get_chat_interface`: Returns a FileResponse serving the HTML chat interface (`static/index.html`).\n   - `websocket_endpoint`: Does not return a value but communicates interactively with the client over the WebSocket.\n\n4. **List of other functions or methods it calls internally:**\n   - `FileResponse()` (from fastapi.responses): Used to serve the HTML file.\n   - `json.loads()`: Decodes incoming WebSocket messages from JSON.\n   - `create_streaming_chat_flow()` (imported from flow): Creates a chat flow object.\n   - `flow.run_async()`: Runs the chat flow asynchronously with the shared store.\n   - `websocket.accept()`, `websocket.receive_text()`: FastAPI WebSocket methods for managing the WebSocket connection.\n   - `StaticFiles()` (from fastapi.staticfiles): Serves static files.\n   - `uvicorn.run()`: Runs the app when executed as the main program."
    },
    "27": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-websocket/utils/__init__.py",
      "code": "# Utils package for FastAPI WebSocket Chat Interface ",
      "summary": "Summary of code unit __init__.py:\n\n1. Primary purpose of the code unit:\nThis code unit acts as the initializer for the utils package within the FastAPI WebSocket Chat Interface project. It designates the folder as a Python package and provides a brief comment describing its intended usage.\n\n2. Description of parameters:\nThere are no parameters in this code unit.\n\n3. Description of return value:\nThere is no return value in this code unit.\n\n4. List of other functions or methods it calls internally:\nThis code unit does not call any functions or methods internally."
    },
    "28": {
      "unit_name": "stream_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-websocket/utils/stream_llm.py",
      "code": "import os\nfrom openai import AsyncOpenAI\n\nasync def stream_llm(messages):\n    client = AsyncOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    stream = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages,\n        stream=True,\n        temperature=0.7\n    )\n    \n    async for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n\nif __name__ == \"__main__\":\n    import asyncio\n    \n    async def test():\n        messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n        async for chunk in stream_llm(messages):\n            print(chunk, end=\"\", flush=True)\n        print()\n    \n    asyncio.run(test()) ",
      "summary": "**Summary of `stream_llm.py`:**\n\n1. **Primary Purpose:**  \n   This code defines an asynchronous generator function, `stream_llm`, that streams real-time responses from the OpenAI GPT-4o-mini language model given a sequence of messages. It allows for token-by-token (or chunked) output, suitable for interactive applications.\n\n2. **Parameters:**  \n   - `messages`: A list of dictionaries, each representing a message (typically with keys `\"role\"` and `\"content\"`), forming the history/context for the LLM conversation.\n\n3. **Return Value:**  \n   - The function is an async generator that `yield`s pieces (chunks) of the LLM's streamed response as they become available (specifically, the `.content` of each token/delta in the stream).\n\n4. **Internally Called Functions/Methods:**  \n   - `os.environ.get()` \u2014 to retrieve the OpenAI API key from environment variables.\n   - `AsyncOpenAI()` \u2014 to create an OpenAI asynchronous client.\n   - `client.chat.completions.create()` \u2014 to launch the streamed chat completion request.\n   - Asynchronous iteration (`async for`) over the stream object.\n   - If run as a script, uses `asyncio.run()` to execute a sample test function."
    },
    "29": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/nodes.py",
      "code": "from pocketflow import Node, BatchNode\nfrom tools.pdf import pdf_to_images\nfrom tools.vision import extract_text_from_image\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport os\n\nclass ProcessPDFBatchNode(BatchNode):\n    \"\"\"Node for processing multiple PDFs from a directory\"\"\"\n    \n    def prep(self, shared):\n        # Get PDF directory path\n        root_dir = Path(__file__).parent\n        pdf_dir = root_dir / \"pdfs\"\n        \n        # List all PDFs\n        pdf_files = []\n        for file in os.listdir(pdf_dir):\n            if file.lower().endswith('.pdf'):\n                pdf_files.append({\n                    \"pdf_path\": str(pdf_dir / file),\n                    \"extraction_prompt\": shared.get(\"extraction_prompt\", \n                        \"Extract all text from this document, preserving formatting and layout.\")\n                })\n        \n        if not pdf_files:\n            print(\"No PDF files found in 'pdfs' directory!\")\n            return []\n            \n        print(f\"Found {len(pdf_files)} PDF files\")\n        return pdf_files\n    \n    def exec(self, item):\n        # Create flow for single PDF\n        flow = create_single_pdf_flow()\n        \n        # Process PDF\n        print(f\"\\nProcessing: {os.path.basename(item['pdf_path'])}\")\n        print(\"-\" * 50)\n        \n        # Run flow\n        shared = item.copy()\n        flow.run(shared)\n        \n        return {\n            \"filename\": os.path.basename(item[\"pdf_path\"]),\n            \"text\": shared.get(\"final_text\", \"No text extracted\")\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        shared[\"results\"] = exec_res_list\n        return \"default\"\n\nclass LoadPDFNode(Node):\n    \"\"\"Node for loading and converting a single PDF to images\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"pdf_path\", \"\")\n        \n    def exec(self, pdf_path):\n        return pdf_to_images(pdf_path)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"page_images\"] = exec_res\n        return \"default\"\n\nclass ExtractTextNode(Node):\n    \"\"\"Node for extracting text from images using Vision API\"\"\"\n    \n    def prep(self, shared):\n        return (\n            shared.get(\"page_images\", []),\n            shared.get(\"extraction_prompt\", None)\n        )\n        \n    def exec(self, inputs):\n        images, prompt = inputs\n        results = []\n        \n        for img, page_num in images:\n            text = extract_text_from_image(img, prompt)\n            results.append({\n                \"page\": page_num,\n                \"text\": text\n            })\n            \n        return results\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"extracted_text\"] = exec_res\n        return \"default\"\n\nclass CombineResultsNode(Node):\n    \"\"\"Node for combining and formatting extracted text\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"extracted_text\", [])\n        \n    def exec(self, results):\n        # Sort by page number\n        sorted_results = sorted(results, key=lambda x: x[\"page\"])\n        \n        # Combine text with page numbers\n        combined = []\n        for result in sorted_results:\n            combined.append(f\"=== Page {result['page']} ===\\n{result['text']}\\n\")\n            \n        return \"\\n\".join(combined)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_text\"] = exec_res\n        return \"default\"\n\ndef create_single_pdf_flow():\n    \"\"\"Create a flow for processing a single PDF\"\"\"\n    from pocketflow import Flow\n    \n    # Create nodes\n    load_pdf = LoadPDFNode()\n    extract_text = ExtractTextNode()\n    combine_results = CombineResultsNode()\n    \n    # Connect nodes\n    load_pdf >> extract_text >> combine_results\n    \n    # Create and return flow\n    return Flow(start=load_pdf)\n",
      "summary": "**Summary of `nodes.py` from `/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/nodes.py`:**\n\n---\n\n**1. Primary Purpose:**\nThis code defines a sequence of PocketFlow nodes that automate the extraction of text from PDF files using vision-based OCR. It provides both batch and single-document workflows to convert PDFs to images, extract text from those images (potentially using a Vision API), and combine the results in a formatted output.\n\n---\n\n**2. Brief Description of Parameters:**\n- Most methods (`prep`, `exec`, `post`) receive or reference a shared context object (`shared`), which is a dictionary containing state and data for each step in the pipeline.\n- The `prep` and `exec` methods may take in specific items or inputs (such as the path to a PDF or a list of images).\n- Optionally, the `extraction_prompt` parameter in `shared` can define how text should be extracted.\n\n---\n\n**3. Brief Description of Return Values:**\n- Each node's `prep`, `exec`, and `post` methods return the next stage's input or update the shared context.\n- The batch node (`ProcessPDFBatchNode`) ultimately returns a list of dictionaries containing filenames and extracted texts for each processed PDF file.\n- Combined text output is formatted by page in a single string.\n- The flow's final result is accessible in the `shared[\"results\"]` field for batches, or `shared[\"final_text\"]` for a single PDF.\n\n---\n\n**4. List of Other Functions or Methods Called Internally:**\n- **`pdf_to_images(pdf_path)`** \u2014 Converts a PDF to a sequence of images (from `tools.pdf`).\n- **`extract_text_from_image(img, prompt)`** \u2014 Extracts text from a single image using a vision tool or API (from `tools.vision`).\n- **`create_single_pdf_flow()`** \u2014 Sets up a processing flow combining all node steps for one PDF.\n- **Standard library:** `os.listdir`, `os.path.basename`, `Path`, and list/dictionary manipulations.\n- **PocketFlow classes/methods:** `Node`, `BatchNode`, and `Flow`; also usage of the `>>` operator to chain nodes.\n\n---\n\n**In summary:**\nThis code is a modular pipeline for OCR text extraction from PDFs using PocketFlow nodes, with utilities for batch and single-file processing, parameterized prompts, and formatting of extracted data."
    },
    "30": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import ProcessPDFBatchNode\n\ndef create_vision_flow():\n    \"\"\"Create a flow for batch PDF processing with Vision API\"\"\"\n    return Flow(start=ProcessPDFBatchNode())\n",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary purpose:**  \n   The code defines a function to create and return a workflow (a \"Flow\") for batch processing of PDFs using a Vision API, with `ProcessPDFBatchNode` as the starting node.\n\n2. **Parameters:**  \n   The function `create_vision_flow()` takes no parameters.\n\n3. **Return value:**  \n   It returns a `Flow` object that begins execution with the `ProcessPDFBatchNode`.\n\n4. **Other functions or methods called internally:**  \n   - `Flow(...)` constructor (from the `pocketflow` module)  \n   - `ProcessPDFBatchNode()` constructor (from the `nodes` module)"
    },
    "31": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/main.py",
      "code": "from flow import create_vision_flow\n\ndef main():\n    # Create and run flow\n    flow = create_vision_flow()\n    shared = {}\n    flow.run(shared)\n    \n    # Print results\n    if \"results\" in shared:\n        for result in shared[\"results\"]:\n            print(f\"\\nFile: {result['filename']}\")\n            print(\"-\" * 50)\n            print(result[\"text\"])\n\nif __name__ == \"__main__\":\n    main()\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/main.py`:**\n\n1. **Primary Purpose:**  \n   This code is the main entry point for a script that orchestrates a computer vision \"flow\"\u2014likely for extracting text from PDF files or images. It sets up and runs a vision processing pipeline and prints out the resulting extracted text for each processed file.\n\n2. **Parameters:**  \n   The main function does not accept any parameters. It is designed to be executed directly (as a script).\n\n3. **Return Value:**  \n   The main function does not return a value. It performs its task via side effects: running the flow and printing output to the console.\n\n4. **Internally Called Functions/Methods:**  \n   - `create_vision_flow()` (imported from `flow`): Used to instantiate the vision processing flow.\n   - `flow.run(shared)`: Runs the processing flow, passing in a dictionary object (`shared`) for collecting results.\n   - Standard dictionary and list operations to print out results (e.g., iterating over `shared[\"results\"]`).\n\n**Summary:**  \nWhen run as a script, this file creates a vision processing flow, executes it, and prints the results (filename and extracted text) for each input file processed."
    },
    "32": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/utils/__init__.py",
      "code": "",
      "summary": "It appears that the code you referenced from /tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/utils/__init__.py is empty or not provided.\n\nSummary:\n\n1. Primary purpose:  \n   The file currently does not contain any code, so it serves as a package initializer. In Python, an empty __init__.py file is used to indicate that the directory is a Python package.\n\n2. Parameters:  \n   None, as no functions or classes are defined.\n\n3. Return value:  \n   None.\n\n4. Internal Function/Method Calls:  \n   None, since the file contains no functional code."
    },
    "33": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/utils/call_llm.py",
      "code": "import os\nfrom openai import OpenAI\nfrom pathlib import Path\n\n# Get the project root directory (parent of utils directory)\nROOT_DIR = Path(__file__).parent.parent\n\n# Initialize OpenAI client with API key from environment\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   This code unit is responsible for setting up the environment to interact with the OpenAI API by initializing an OpenAI client using an API key sourced from environment variables. It also determines the project\u2019s root directory, presumably for relative file management elsewhere in the project.\n\n2. **Parameters:**  \n   The code as shown does not define any functions or accept any parameters.\n\n3. **Return Value:**  \n   There is no return value; the script only performs initialization steps.\n\n4. **Called Functions/Methods:**  \n   - `Path(__file__).parent.parent` (from `pathlib`): Used to determine the project's root directory.\n   - `os.getenv(\"OPENAI_API_KEY\")`: Fetches the OpenAI API key from environment variables.\n   - `OpenAI(api_key=...)`: Initializes an instance of the OpenAI client with the API key."
    },
    "34": {
      "unit_name": "pdf.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/tools/pdf.py",
      "code": "import fitz  # PyMuPDF\nfrom PIL import Image\nimport io\nimport base64\nfrom typing import List, Tuple\n\ndef pdf_to_images(pdf_path: str, max_size: int = 2000) -> List[Tuple[Image.Image, int]]:\n    \"\"\"Convert PDF pages to PIL Images with size limit\n    \n    Args:\n        pdf_path (str): Path to PDF file\n        max_size (int): Maximum dimension (width/height) for images\n        \n    Returns:\n        list: List of tuples (PIL Image, page number)\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    images = []\n    \n    try:\n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            pix = page.get_pixmap()\n            \n            # Convert to PIL Image\n            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n            \n            # Resize if needed while maintaining aspect ratio\n            if max(img.size) > max_size:\n                ratio = max_size / max(img.size)\n                new_size = tuple(int(dim * ratio) for dim in img.size)\n                img = img.resize(new_size, Image.Resampling.LANCZOS)\n            \n            images.append((img, page_num + 1))\n            \n    finally:\n        doc.close()\n        \n    return images\n\ndef image_to_base64(image: Image.Image) -> str:\n    \"\"\"Convert PIL Image to base64 string\n    \n    Args:\n        image (PIL.Image): Image to convert\n        \n    Returns:\n        str: Base64 encoded image string\n    \"\"\"\n    buffer = io.BytesIO()\n    image.save(buffer, format=\"PNG\")\n    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/tools/pdf.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides utilities to convert PDF pages into resized PIL images and further encode those images as base64 strings. It is mainly used for extracting visual content from PDF files for downstream processing or embedding.\n\n2. **Parameters:**\n   - `pdf_to_images(pdf_path: str, max_size: int = 2000)`:  \n     - `pdf_path`: The filepath of the PDF document to convert.\n     - `max_size`: The maximum allowed width or height for output images (default: 2000 pixels).\n   - `image_to_base64(image: Image.Image)`:  \n     - `image`: The PIL Image object to be encoded as a base64 string.\n\n3. **Return Values:**\n   - `pdf_to_images`: Returns a list of tuples, each containing a PIL Image (one per PDF page, optionally resized) and its page number (1-based).\n   - `image_to_base64`: Returns a base64-encoded string representation of the provided image in PNG format.\n\n4. **Other Functions or Methods Called Internally:**\n   - `fitz.open`\n   - `len` (on document)\n   - `page.get_pixmap`\n   - `Image.frombytes`\n   - `img.resize`\n   - `Image.save`\n   - `base64.b64encode`\n   - `io.BytesIO`\n   - `doc.close`"
    },
    "35": {
      "unit_name": "vision.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-pdf-vision/tools/vision.py",
      "code": "from PIL import Image\nfrom utils.call_llm import client\nfrom tools.pdf import image_to_base64\n\ndef extract_text_from_image(image: Image.Image, prompt: str = None) -> str:\n    \"\"\"Extract text from image using OpenAI Vision API\n    \n    Args:\n        image (PIL.Image): Image to process\n        prompt (str, optional): Custom prompt for extraction. Defaults to general OCR.\n        \n    Returns:\n        str: Extracted text from image\n    \"\"\"\n    # Convert image to base64\n    img_base64 = image_to_base64(image)\n    \n    # Default prompt for general OCR\n    if prompt is None:\n        prompt = \"Please extract all text from this image.\"\n    \n    # Call Vision API\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"}}\n            ]\n        }]\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test vision processing\n    test_image = Image.open(\"example.png\")\n    result = extract_text_from_image(test_image)\n    print(\"Extracted text:\", result)\n",
      "summary": "**Summary of `vision.py`**\n\n1. **Primary purpose:**  \n   The code defines a function to extract text from an image using the OpenAI Vision (GPT-4o) API by sending a base64-encoded image and an optional prompt.\n\n2. **Parameters:**  \n   - `image` (PIL.Image.Image): The image object from which to extract text.\n   - `prompt` (str, optional): A custom instruction for the extraction process. If not provided, a default prompt requests all text from the image.\n\n3. **Return value:**  \n   - Returns a `str` containing the text extracted from the image by the Vision API.\n\n4. **Functions/methods called internally:**  \n   - `image_to_base64` (from `tools.pdf`): Converts the image to a base64-encoded string.\n   - `client.chat.completions.create` (from `utils.call_llm`): Calls the OpenAI Vision API for text extraction.\n   - Standard functions from the PIL library (`Image.open` for testing in the `__main__` block)."
    },
    "36": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-search/nodes.py",
      "code": "from pocketflow import Node\nfrom tools.search import SearchTool\nfrom tools.parser import analyze_results\nfrom typing import List, Dict\n\nclass SearchNode(Node):\n    \"\"\"Node to perform web search using SerpAPI\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"query\"), shared.get(\"num_results\", 5)\n        \n    def exec(self, inputs):\n        query, num_results = inputs\n        if not query:\n            return []\n            \n        searcher = SearchTool()\n        return searcher.search(query, num_results)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_results\"] = exec_res\n        return \"default\"\n\nclass AnalyzeResultsNode(Node):\n    \"\"\"Node to analyze search results using LLM\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"query\"), shared.get(\"search_results\", [])\n        \n    def exec(self, inputs):\n        query, results = inputs\n        if not results:\n            return {\n                \"summary\": \"No search results to analyze\",\n                \"key_points\": [],\n                \"follow_up_queries\": []\n            }\n            \n        return analyze_results(query, results)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"analysis\"] = exec_res\n        \n        # Print analysis\n        print(\"\\nSearch Analysis:\")\n        print(\"\\nSummary:\", exec_res[\"summary\"])\n        \n        print(\"\\nKey Points:\")\n        for point in exec_res[\"key_points\"]:\n            print(f\"- {point}\")\n            \n        print(\"\\nSuggested Follow-up Queries:\")\n        for query in exec_res[\"follow_up_queries\"]:\n            print(f\"- {query}\")\n            \n        return \"default\"\n",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary Purpose:**  \n   The code defines two custom node classes, `SearchNode` and `AnalyzeResultsNode`, designed to be integrated into a workflow system (using a base `Node` class). These nodes perform automated web search tasks using SerpAPI results and then analyze those results using an LLM, making them suitable for information retrieval and summarization pipelines.\n\n2. **Parameters:**  \n   - Both node classes operate on a shared data object (`shared`), which acts as a dictionary-like storage for inputs (such as `\"query\"` and `\"num_results\"`).\n   - The number of search results can be customized via the `\"num_results\"` key in the shared object.\n\n3. **Return Values:**  \n   - The `exec` method in `SearchNode` returns a list of web search results (via `SearchTool.search`).\n   - The `exec` method in `AnalyzeResultsNode` returns a dictionary with a `\"summary\"`, `\"key_points\"`, and `\"follow_up_queries\"` (via `analyze_results`), or a message indicating no results were available.\n   - Both nodes\u2019 `post` methods update the shared state and return the string `\"default\"`.\n\n4. **Functions/Methods Called Internally:**\n   - `SearchTool.search(query, num_results)` (from `tools.search`)\n   - `analyze_results(query, results)` (from `tools.parser`)\n   - Standard Python dictionary methods (e.g., `shared.get`)\n   - Standard `print` function for displaying results in `AnalyzeResultsNode.post`"
    },
    "37": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-search/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import SearchNode, AnalyzeResultsNode\n\ndef create_flow() -> Flow:\n    \"\"\"Create and configure the search flow\n    \n    Returns:\n        Flow: Configured flow ready to run\n    \"\"\"\n    # Create nodes\n    search = SearchNode()\n    analyze = AnalyzeResultsNode()\n    \n    # Connect nodes\n    search >> analyze\n    \n    # Create flow starting with search\n    return Flow(start=search)\n",
      "summary": "**Summary of flow.py (from '/tmp/PocketFlow/cookbook/pocketflow-tool-search/flow.py'):**\n\n1. **Primary purpose:**  \n   The code defines a function to create and configure a simple processing flow for a search operation, chaining a search step with an analysis step.\n\n2. **Parameters:**  \n   The `create_flow` function does **not** take any parameters.\n\n3. **Return value:**  \n   The function returns a `Flow` object (from the `pocketflow` library) that starts with the `SearchNode`.\n\n4. **Other functions/methods called internally:**  \n   - `SearchNode()` constructor  \n   - `AnalyzeResultsNode()` constructor  \n   - Overloaded `>>` operator to connect `search` to `analyze`  \n   - `Flow(start=search)` constructor"
    },
    "38": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-search/main.py",
      "code": "import os\nfrom flow import create_flow\n\ndef main():\n    \"\"\"Run the web search flow\"\"\"\n    \n    # Get search query from user\n    query = input(\"Enter search query: \")\n    if not query:\n        print(\"Error: Query is required\")\n        return\n        \n    # Initialize shared data\n    shared = {\n        \"query\": query,\n        \"num_results\": 5\n    }\n    \n    # Create and run flow\n    flow = create_flow()\n    flow.run(shared)\n    \n    # Results are in shared[\"analysis\"]\n    \nif __name__ == \"__main__\":\n    main()\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tool-search/main.py`:**\n\n1. **Primary purpose:**  \n   The code serves as the entry point for a web search tool. It prompts the user for a search query, initializes required data, creates a search flow using the `create_flow()` function, and runs this flow to perform the search.\n\n2. **Parameters:**  \n   The main function does not accept any parameters. User input (the search query) is collected interactively within the function via the `input()` call.\n\n3. **Return value:**  \n   The main function does not explicitly return a value. All results and outputs are managed via print statements or by updating the shared data dictionary passed internally.\n\n4. **Internal function/method calls:**  \n   - `input()` (standard Python function to get user input)\n   - `print()` (standard Python function for output)\n   - `create_flow()` (imported from `flow`; creates and returns a flow object)\n   - `flow.run(shared)` (method called on the returned flow object to execute the search logic)\n\n**Note:** The actual search logic and result analysis are handled within the flow object, and results are expected to be stored in the `shared[\"analysis\"]` key."
    },
    "39": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-search/utils/__init__.py",
      "code": "",
      "summary": "Since you have provided only the unit name (__init__.py) and not included any code from the file, here's an analysis based solely on the information given:\n\n- Primary purpose: The __init__.py file in a Python package directory (in this case, utils) is typically used to mark the directory as a Python package and to initialize the package. Sometimes, it imports key functions, classes, or variables from submodules to expose them at the package level, or it may set up package-level configuration.\n- Parameters: No parameters are applicable because no functions or classes are described in the absence of code.\n- Return value: None, as there's no indication of functions, classes, or return statements without the file's contents.\n- Internal calls: None can be identified without seeing the code inside the file.\n\nIf you provide the actual code from __init__.py, I can give you a detailed, code-specific summary as per your request."
    },
    "40": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-search/utils/call_llm.py",
      "code": "import os\nfrom openai import OpenAI\nfrom pathlib import Path\n\n# Get the project root directory (parent of utils directory)\nROOT_DIR = Path(__file__).parent.parent\n\n# Initialize OpenAI client with API key from environment\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef call_llm(prompt: str) -> str:\n    \"\"\"Call OpenAI API to analyze text\n    \n    Args:\n        prompt (str): Input prompt for the model\n        \n    Returns:\n        str: Model response\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.choices[0].message.content\n        \n    except Exception as e:\n        print(f\"Error calling LLM API: {str(e)}\")\n        return \"\"\n\nif __name__ == \"__main__\":\n    # Test LLM call\n    response = call_llm(\"What is web search?\")\n    print(\"Response:\", response)\n",
      "summary": "**Summary of call_llm.py**\n\n1. **Primary purpose:**  \n   This code unit provides a utility function to send a prompt to OpenAI's GPT-4o model using the OpenAI API and returns the model's response as text. It is designed to facilitate easy calling of the language model within a project.\n\n2. **Parameters:**  \n   - The main function `call_llm` takes a single parameter:\n     - `prompt` (str): The input prompt text that will be sent to the language model.\n\n3. **Return value:**  \n   - The `call_llm` function returns a string containing the model's response to the input prompt. If an error occurs during the API call, it returns an empty string.\n\n4. **Internal functions/methods called:**  \n   - `os.getenv`: Retrieves the OpenAI API key from environment variables.\n   - `OpenAI` (constructor): Initializes an OpenAI API client.\n   - `client.chat.completions.create`: Calls the OpenAI Chat API to get a completion from the GPT-4o model.\n   - Standard Python functions: `print` (for output) and `str` (in exception handling)."
    },
    "41": {
      "unit_name": "parser.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-search/tools/parser.py",
      "code": "from typing import Dict, List\nfrom utils.call_llm import call_llm\n\ndef analyze_results(query: str, results: List[Dict]) -> Dict:\n    \"\"\"Analyze search results using LLM\n    \n    Args:\n        query (str): Original search query\n        results (List[Dict]): Search results to analyze\n        \n    Returns:\n        Dict: Analysis including summary and key points\n    \"\"\"\n    # Format results for prompt\n    formatted_results = []\n    for i, result in enumerate(results, 1):\n        formatted_results.append(f\"\"\"\nResult {i}:\nTitle: {result['title']}\nSnippet: {result['snippet']}\nURL: {result['link']}\n\"\"\")\n    \n    prompt = f\"\"\"\nAnalyze these search results for the query: \"{query}\"\n\n{'\\n'.join(formatted_results)}\n\nPlease provide:\n1. A concise summary of the findings (2-3 sentences)\n2. Key points or facts (up to 5 bullet points)\n3. Suggested follow-up queries (2-3)\n\nOutput in YAML format:\n```yaml\nsummary: >\n    brief summary here\nkey_points:\n    - point 1\n    - point 2\nfollow_up_queries:\n    - query 1\n    - query 2\n```\n\"\"\"\n    \n    try:\n        response = call_llm(prompt)\n        # Extract YAML between code fences\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        \n        import yaml\n        analysis = yaml.safe_load(yaml_str)\n        \n        # Validate required fields\n        assert \"summary\" in analysis\n        assert \"key_points\" in analysis\n        assert \"follow_up_queries\" in analysis\n        assert isinstance(analysis[\"key_points\"], list)\n        assert isinstance(analysis[\"follow_up_queries\"], list)\n        \n        return analysis\n        \n    except Exception as e:\n        print(f\"Error analyzing results: {str(e)}\")\n        return {\n            \"summary\": \"Error analyzing results\",\n            \"key_points\": [],\n            \"follow_up_queries\": []\n        }",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tool-search/tools/parser.py`**\n\n1. **Primary Purpose**  \nThe code defines a function, `analyze_results`, which uses a language model (LLM) to analyze and summarize web search results in response to a user query. It formats the search results, prompts the LLM for a concise analysis, and parses the structured output.\n\n2. **Parameters**\n- `query` (str): The original search query string provided by the user.\n- `results` (List[Dict]): A list of dictionaries, each representing a search result with fields such as 'title', 'snippet', and 'link'.\n\n3. **Return Value**\n- Returns a dictionary with three keys:\n  - `'summary'`: A short summary of the search results.\n  - `'key_points'`: A list of up to five key points or facts extracted from the results.\n  - `'follow_up_queries'`: 2-3 suggested follow-up search queries.\n  - In case of failure, returns a dictionary with an error message and empty lists.\n\n4. **Internal Function and Method Calls**\n- `call_llm` (imported from `utils.call_llm`): Sends a prompt to the language model and retrieves the response.\n- `yaml.safe_load`: Parses YAML-formatted string output.\n- Standard Python functions: `enumerate`, `assert`, string formatting and splitting, `print`.\n\n**In summary:**  \nThis code unit formats search results, prompts an LLM for analysis, parses its YAML output, and returns a summary, key points, and suggested follow-up queries, given a search query and a list of results."
    },
    "42": {
      "unit_name": "search.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-search/tools/search.py",
      "code": "import os\nfrom serpapi import GoogleSearch\nfrom typing import Dict, List, Optional\n\nclass SearchTool:\n    \"\"\"Tool for performing web searches using SerpAPI\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        \"\"\"Initialize search tool with API key\n        \n        Args:\n            api_key (str, optional): SerpAPI key. Defaults to env var SERPAPI_API_KEY.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"SERPAPI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"SerpAPI key not found. Set SERPAPI_API_KEY env var.\")\n            \n    def search(self, query: str, num_results: int = 5) -> List[Dict]:\n        \"\"\"Perform Google search via SerpAPI\n        \n        Args:\n            query (str): Search query\n            num_results (int, optional): Number of results to return. Defaults to 5.\n            \n        Returns:\n            List[Dict]: Search results with title, snippet, and link\n        \"\"\"\n        # Configure search parameters\n        params = {\n            \"engine\": \"google\",\n            \"q\": query,\n            \"api_key\": self.api_key,\n            \"num\": num_results\n        }\n        \n        try:\n            # Execute search\n            search = GoogleSearch(params)\n            results = search.get_dict()\n            \n            # Extract organic results\n            if \"organic_results\" not in results:\n                return []\n                \n            processed_results = []\n            for result in results[\"organic_results\"][:num_results]:\n                processed_results.append({\n                    \"title\": result.get(\"title\", \"\"),\n                    \"snippet\": result.get(\"snippet\", \"\"),\n                    \"link\": result.get(\"link\", \"\")\n                })\n                \n            return processed_results\n            \n        except Exception as e:\n            print(f\"Search error: {str(e)}\")\n            return []\n",
      "summary": "**Summary of `search.py` from `/tmp/PocketFlow/cookbook/pocketflow-tool-search/tools/`:**\n\n1. **Primary purpose:**  \n   The code defines a `SearchTool` class that enables performing web searches using the SerpAPI Google Search API and returns structured search results.\n\n2. **Parameters:**  \n   - `api_key` (optional, `str`): The SerpAPI authentication key. If not provided, it defaults to the environment variable `SERPAPI_API_KEY`.\n   - The `search` method accepts:\n     - `query` (`str`): The text string to search for.\n     - `num_results` (optional, `int`): The maximum number of results to return (default is 5).\n\n3. **Return value:**  \n   - The `search` method returns a list of dictionaries. Each dictionary includes a `title`, `snippet`, and `link` for one search result. If an error occurs or there are no results, it returns an empty list.\n\n4. **Other functions/methods called internally:**  \n   - `os.getenv()` \u2013 to retrieve the API key from environment variables.\n   - `GoogleSearch` from the `serpapi` package \u2013 to perform the search.\n   - `get_dict()` \u2013 a method of `GoogleSearch` that fetches search results as a dictionary.\n   - Python built-in methods such as `dict.get()`, list slicing, and basic exception handling (`try`/`except`)."
    },
    "43": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-map-reduce/nodes.py",
      "code": "from pocketflow import Node, BatchNode\nfrom utils import call_llm\nimport yaml\nimport os\n\nclass ReadResumesNode(Node):\n    \"\"\"Map phase: Read all resumes from the data directory into shared storage.\"\"\"\n    \n    def exec(self, _):\n        resume_files = {}\n        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\n        \n        for filename in os.listdir(data_dir):\n            if filename.endswith(\".txt\"):\n                file_path = os.path.join(data_dir, filename)\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    resume_files[filename] = file.read()\n        \n        return resume_files\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"resumes\"] = exec_res\n        return \"default\"\n\n\nclass EvaluateResumesNode(BatchNode):\n    \"\"\"Batch processing: Evaluate each resume to determine if the candidate qualifies.\"\"\"\n    \n    def prep(self, shared):\n        return list(shared[\"resumes\"].items())\n    \n    def exec(self, resume_item):\n        \"\"\"Evaluate a single resume.\"\"\"\n        filename, content = resume_item\n        \n        prompt = f\"\"\"\nEvaluate the following resume and determine if the candidate qualifies for an advanced technical role.\nCriteria for qualification:\n- At least a bachelor's degree in a relevant field\n- At least 3 years of relevant work experience\n- Strong technical skills relevant to the position\n\nResume:\n{content}\n\nReturn your evaluation in YAML format:\n```yaml\ncandidate_name: [Name of the candidate]\nqualifies: [true/false]\nreasons:\n  - [First reason for qualification/disqualification]\n  - [Second reason, if applicable]\n```\n\"\"\"\n        response = call_llm(prompt)\n        \n        # Extract YAML content\n        yaml_content = response.split(\"```yaml\")[1].split(\"```\")[0].strip() if \"```yaml\" in response else response\n        result = yaml.safe_load(yaml_content)\n        \n        return (filename, result)\n\n    def post(self, shared, prep_res, exec_res_list):\n        shared[\"evaluations\"] = {filename: result for filename, result in exec_res_list}\n        return \"default\"\n\n\nclass ReduceResultsNode(Node):\n    \"\"\"Reduce node: Count and print out how many candidates qualify.\"\"\"\n    \n    def prep(self, shared):\n        return shared[\"evaluations\"]\n    \n    def exec(self, evaluations):\n        qualified_count = 0\n        total_count = len(evaluations)\n        qualified_candidates = []\n        \n        for filename, evaluation in evaluations.items():\n            if evaluation.get(\"qualifies\", False):\n                qualified_count += 1\n                qualified_candidates.append(evaluation.get(\"candidate_name\", \"Unknown\"))\n        \n        summary = {\n            \"total_candidates\": total_count,\n            \"qualified_count\": qualified_count,\n            \"qualified_percentage\": round(qualified_count / total_count * 100, 1) if total_count > 0 else 0,\n            \"qualified_names\": qualified_candidates\n        }\n        \n        return summary\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"summary\"] = exec_res\n        \n        print(\"\\n===== Resume Qualification Summary =====\")\n        print(f\"Total candidates evaluated: {exec_res['total_candidates']}\")\n        print(f\"Qualified candidates: {exec_res['qualified_count']} ({exec_res['qualified_percentage']}%)\")\n        \n        if exec_res['qualified_names']:\n            print(\"\\nQualified candidates:\")\n            for name in exec_res['qualified_names']:\n                print(f\"- {name}\")\n        \n        return \"default\" ",
      "summary": "**Summary of /tmp/PocketFlow/cookbook/pocketflow-map-reduce/nodes.py:**\n\n1. **Primary Purpose:**\n   - This code unit defines a Map-Reduce workflow for evaluating a set of resume text files to determine which candidates qualify for an advanced technical role. The workflow is split into three processing nodes: reading resumes, evaluating them using an LLM, and aggregating the results.\n\n2. **Parameters:**\n   - The core classes (`ReadResumesNode`, `EvaluateResumesNode`, and `ReduceResultsNode`) are derived from `Node` or `BatchNode` and are designed to be used within a dataflow system, where their methods (`exec`, `prep`, and `post`) interact with a shared state dictionary (`shared`) or receive input data relevant to each phase.  \n   - Method signatures:\n     - `exec(self, _)` in `ReadResumesNode`: Takes an unused parameter (denoted `_`).\n     - `prep(self, shared)`, `exec(self, resume_item)`, `post(self, shared, prep_res, exec_res)` in `EvaluateResumesNode`: Parameters relate to workflow state and items being processed.\n     - `prep(self, shared)`, `exec(self, evaluations)`, `post(self, shared, prep_res, exec_res)` in `ReduceResultsNode`: Parameters manage inputs and results at each compute step.\n\n3. **Return Values:**\n   - `ReadResumesNode.exec`: Returns a dictionary mapping resume filenames to their text content.\n   - `EvaluateResumesNode.prep`: Returns a list of `(filename, content)` tuples for each resume.\n   - `EvaluateResumesNode.exec`: Returns a tuple of filename and the parsed evaluation result from the LLM (as a dict).\n   - `EvaluateResumesNode.post`: Stores a dictionary of evaluations in shared state; returns string `\"default\"`.\n   - `ReduceResultsNode.exec`: Returns a summary dictionary containing total, qualified count, percentage, and names of qualified candidates.\n   - `post` methods generally update shared state and/or print results, returning `\"default\"` as a status.\n\n4. **Internal Functions/Methods Called:**\n   - `os.path.join`, `os.path.dirname`, `os.path.abspath`, `os.listdir`: For file and directory operations.\n   - `open`: For reading resume files.\n   - `call_llm`: Custom utility to send prompts to a language model.\n   - `yaml.safe_load`: For parsing YAML output returned from the LLM.\n   - Standard Python dictionary and list methods.\n   - Printing to console (`print`).\n\n**In summary:**  \nThe code provides a map-reduce pipeline for automated resume evaluation, reading raw text files, batch-evaluating them using an LLM (via prompts and YAML parsing), and then counting and displaying which candidates qualify according to preset criteria. The code heavily relies on file I/O, a custom LLM interface, and YAML parsing."
    },
    "44": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-map-reduce/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import ReadResumesNode, EvaluateResumesNode, ReduceResultsNode\n\ndef create_resume_processing_flow():\n    \"\"\"Create a map-reduce flow for processing resumes.\"\"\"\n    # Create nodes\n    read_resumes_node = ReadResumesNode()\n    evaluate_resumes_node = EvaluateResumesNode()\n    reduce_results_node = ReduceResultsNode()\n    \n    # Connect nodes\n    read_resumes_node >> evaluate_resumes_node >> reduce_results_node\n    \n    # Create flow\n    return Flow(start=read_resumes_node)",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary Purpose:**  \n   This code defines a function to construct a data processing pipeline (flow) that reads, evaluates, and aggregates results of resume data using a map-reduce pattern.\n\n2. **Parameters:**  \n   The main function, `create_resume_processing_flow`, does **not** take any parameters.\n\n3. **Return Value:**  \n   The function returns a `Flow` object initialized with the `ReadResumesNode` as the starting point of the pipeline.\n\n4. **Internally Called Functions/Methods:**  \n   - Constructors for:\n     - `ReadResumesNode()`\n     - `EvaluateResumesNode()`\n     - `ReduceResultsNode()`\n   - The chaining operator (`>>`) to connect nodes (internally this likely calls a method like `__rshift__` for node chaining).\n   - `Flow(start=...)` constructor."
    },
    "45": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-map-reduce/main.py",
      "code": "from flow import create_resume_processing_flow\n\ndef main():\n    # Initialize shared store\n    shared = {}\n    \n    # Create the resume processing flow\n    resume_flow = create_resume_processing_flow()\n    \n    # Run the flow\n    print(\"Starting resume qualification processing...\")\n    resume_flow.run(shared)\n    \n    # Display final summary information (additional to what's already printed in ReduceResultsNode)\n    if \"summary\" in shared:\n        print(\"\\nDetailed evaluation results:\")\n        for filename, evaluation in shared.get(\"evaluations\", {}).items():\n            qualified = \"\u2713\" if evaluation.get(\"qualifies\", False) else \"\u2717\"\n            name = evaluation.get(\"candidate_name\", \"Unknown\")\n            print(f\"{qualified} {name} ({filename})\")\n    \n    print(\"\\nResume processing complete!\")\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of main.py:**\n\n1. **Primary Purpose:**  \n   The code serves as the entry point for a resume qualification processing workflow. It initializes resources, runs the resume evaluation flow, and displays a summary of the results.\n\n2. **Parameters:**  \n   The main function does not take any parameters.\n\n3. **Return Value:**  \n   The main function does not return any value.\n\n4. **Internally Called Functions/Methods:**  \n   - `create_resume_processing_flow()` (imported from the flow module): Creates and configures the resume processing flow object.  \n   - `resume_flow.run(shared)`: Starts the workflow, processing resumes with a shared dictionary for communication and results.  \n   - Built-in `print()` function: Used for logging progress and results.  \n   - Standard dictionary methods like `get()` and `items()` are used for accessing shared results."
    },
    "46": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-map-reduce/utils.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-map-reduce/utils.py`:**\n\n1. **Primary Purpose:**  \n   The code unit defines a utility function to send a prompt to OpenAI's GPT-4o language model and retrieve its response, making it easy to interact with the model using a simple function call.\n\n2. **Parameters:**  \n   - The main function, `call_llm(prompt)`, takes a single parameter:\n     - `prompt`: A string containing the user's message or request to be sent to the language model.\n\n3. **Return Value:**  \n   - The function returns a string containing the content of the model's reply to the provided prompt.\n\n4. **Functions/Methods Called Internally:**  \n   - `OpenAI(api_key=...)`: Initializes an OpenAI API client using an API key from the environment.\n   - `os.environ.get(...)`: Retrieves the OpenAI API key from environment variables.\n   - `client.chat.completions.create(...)`: Sends the prompt to the GPT-4o model and requests a response.\n   - Accessing `r.choices[0].message.content`: Retrieves the generated message content from the response.  \n   - (In the example block) `print()`: Outputs the LLM response to the console."
    },
    "47": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-background/nodes.py",
      "code": "import yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.call_llm import call_llm\n\nclass GenerateOutline(Node):\n    def prep(self, shared):\n        return shared[\"topic\"]\n    \n    def exec(self, topic):\n        prompt = f\"\"\"\nCreate a simple outline for an article about {topic}.\nInclude at most 3 main sections (no subsections).\n\nOutput the sections in YAML format as shown below:\n\n```yaml\nsections:\n    - First section title\n    - Second section title  \n    - Third section title\n```\"\"\"\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        structured_result = yaml.safe_load(yaml_str)\n        return structured_result\n    \n    def post(self, shared, prep_res, exec_res):\n        sections = exec_res[\"sections\"]\n        shared[\"sections\"] = sections\n        \n        # Send progress update via SSE queue\n        progress_msg = {\"step\": \"outline\", \"progress\": 33, \"data\": {\"sections\": sections}}\n        shared[\"sse_queue\"].put_nowait(progress_msg)\n        \n        return \"default\"\n\nclass WriteContent(BatchNode):\n    def prep(self, shared):\n        # Store sections and sse_queue for use in exec\n        self.sections = shared.get(\"sections\", [])\n        self.sse_queue = shared[\"sse_queue\"]\n        return self.sections\n    \n    def exec(self, section):\n        prompt = f\"\"\"\nWrite a short paragraph (MAXIMUM 100 WORDS) about this section:\n\n{section}\n\nRequirements:\n- Explain the idea in simple, easy-to-understand terms\n- Use everyday language, avoiding jargon\n- Keep it very concise (no more than 100 words)\n- Include one brief example or analogy\n\"\"\"\n        content = call_llm(prompt)\n        \n        # Send progress update for this section\n        current_section_index = self.sections.index(section) if section in self.sections else 0\n        total_sections = len(self.sections)\n        \n        # Progress from 33% (after outline) to 66% (before styling)\n        # Each section contributes (66-33)/total_sections = 33/total_sections percent\n        section_progress = 33 + ((current_section_index + 1) * 33 // total_sections)\n        \n        progress_msg = {\n            \"step\": \"content\", \n            \"progress\": section_progress, \n            \"data\": {\n                \"section\": section,\n                \"completed_sections\": current_section_index + 1,\n                \"total_sections\": total_sections\n            }\n        }\n        self.sse_queue.put_nowait(progress_msg)\n        \n        return f\"## {section}\\n\\n{content}\\n\"\n    \n    def post(self, shared, prep_res, exec_res_list):\n        draft = \"\\n\".join(exec_res_list)\n        shared[\"draft\"] = draft\n        return \"default\"\n\nclass ApplyStyle(Node):\n    def prep(self, shared):\n        return shared[\"draft\"]\n    \n    def exec(self, draft):\n        prompt = f\"\"\"\nRewrite the following draft in a conversational, engaging style:\n\n{draft}\n\nMake it:\n- Conversational and warm in tone\n- Include rhetorical questions that engage the reader\n- Add analogies and metaphors where appropriate\n- Include a strong opening and conclusion\n\"\"\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_article\"] = exec_res\n        \n        # Send completion update via SSE queue\n        progress_msg = {\"step\": \"complete\", \"progress\": 100, \"data\": {\"final_article\": exec_res}}\n        shared[\"sse_queue\"].put_nowait(progress_msg)\n        \n        return \"default\" ",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary purpose:**  \n   The code defines a workflow for generating an article from a user-provided topic using large language models (LLMs), managing the steps of outline generation, section writing, and style refinement. It is designed to be used as part of a background processing system (likely for a FastAPI app), with progress updates sent to the frontend via server-sent events (SSE).\n\n2. **Parameters:**  \n   - Each node receives a `shared` dictionary (holding workflow state, e.g., topic, sections, draft, and SSE queue) as input to its `prep` and `post` methods.\n   - The `GenerateOutline` node's `exec` method takes a `topic` string.\n   - The `WriteContent` batch node's `exec` method takes one `section` title at a time.\n   - The `ApplyStyle` node's `exec` method takes the full article `draft`.\n\n3. **Return value:**  \n   - The `exec` methods return either structured data (YAML-loaded dictionary for the outline), a formatted string (for content sections), or the final styled article string.\n   - The `post` methods update the `shared` state and return a workflow signal (\"default\").\n\n4. **Internally called functions/methods:**\n   - `call_llm` (from `utils.call_llm`): Invokes a language model to perform generation based on LLM prompts in all three nodes.\n   - `yaml.safe_load`: Parses YAML-formatted output from the LLM in `GenerateOutline`.\n   - Methods on the SSE queue:  \n     - `put_nowait`: Sends progress updates to the frontend in all nodes.\n   - Standard Python string methods:  \n     - `split`, `strip`, `join`, `format` (f-strings) for prompt and result formatting.\n   - Data structure methods:  \n     - `dict.get`, `list.index`, `len`.\n\n**In essence:**  \nThe code implements an automated article writer workflow, structured as three processing nodes. It interacts with LLMs for natural language generation, updates the workflow state among steps, and communicates progress asynchronously."
    },
    "48": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-background/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import GenerateOutline, WriteContent, ApplyStyle\n\ndef create_article_flow():\n    \"\"\"\n    Create and configure the article writing workflow\n    \"\"\"\n    # Create node instances\n    outline_node = GenerateOutline()\n    content_node = WriteContent()\n    style_node = ApplyStyle()\n    \n    # Connect nodes in sequence\n    outline_node >> content_node >> style_node\n    \n    # Create flow starting with outline node\n    article_flow = Flow(start=outline_node)\n    \n    return article_flow ",
      "summary": "**Summary of `flow.py` code unit:**\n\n1. **Primary Purpose:**  \n   The code defines a function to create and configure a workflow for generating an article, consisting of outlining, content writing, and styling steps.\n\n2. **Parameters:**  \n   The main function `create_article_flow()` does not take any parameters.\n\n3. **Return Value:**  \n   The function returns an `article_flow` object, which is an instance of `Flow` representing the configured workflow starting with the outline generation node.\n\n4. **Internally Called Functions/Methods:**  \n   - `GenerateOutline()` (class instantiation, likely from `nodes`)\n   - `WriteContent()` (class instantiation)\n   - `ApplyStyle()` (class instantiation)\n   - `Flow(start=outline_node)` (class instantiation from `pocketflow`)\n   - The `>>` operator, indicating chaining or sequencing between nodes (relies on the implementation of `__rshift__` in node classes)"
    },
    "49": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-background/main.py",
      "code": "import asyncio\nimport json\nimport uuid\nfrom fastapi import FastAPI, BackgroundTasks, Form\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import FileResponse\nfrom flow import create_article_flow\n\napp = FastAPI()\n\n# Mount static files\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Store active jobs and their SSE queues\nactive_jobs = {}\n\ndef run_article_workflow(job_id: str, topic: str):\n    \"\"\"Run the article workflow in background\"\"\"\n    try:\n        # Get the pre-created queue from active_jobs\n        sse_queue = active_jobs[job_id]\n        shared = {\n            \"topic\": topic,\n            \"sse_queue\": sse_queue,\n            \"sections\": [],\n            \"draft\": \"\",\n            \"final_article\": \"\"\n        }\n        \n        # Run the workflow\n        flow = create_article_flow()\n        flow.run(shared)\n        \n    except Exception as e:\n        # Send error message\n        error_msg = {\"step\": \"error\", \"progress\": 0, \"data\": {\"error\": str(e)}}\n        if job_id in active_jobs:\n            active_jobs[job_id].put_nowait(error_msg)\n\n@app.post(\"/start-job\")\nasync def start_job(background_tasks: BackgroundTasks, topic: str = Form(...)):\n    \"\"\"Start a new article generation job\"\"\"\n    job_id = str(uuid.uuid4())\n    \n    # Create SSE queue and register job immediately\n    sse_queue = asyncio.Queue()\n    active_jobs[job_id] = sse_queue\n    \n    # Start background task\n    background_tasks.add_task(run_article_workflow, job_id, topic)\n    \n    return {\"job_id\": job_id, \"topic\": topic, \"status\": \"started\"}\n\n@app.get(\"/progress/{job_id}\")\nasync def get_progress(job_id: str):\n    \"\"\"Stream progress updates via SSE\"\"\"\n    \n    async def event_stream():\n        if job_id not in active_jobs:\n            yield f\"data: {json.dumps({'error': 'Job not found'})}\\n\\n\"\n            return\n            \n        sse_queue = active_jobs[job_id]\n        \n        # Send initial connection confirmation\n        yield f\"data: {json.dumps({'step': 'connected', 'progress': 0, 'data': {'message': 'Connected to job progress'}})}\\n\\n\"\n        \n        try:\n            while True:\n                # Wait for next progress update\n                try:\n                    # Use asyncio.wait_for to avoid blocking forever\n                    progress_msg = await asyncio.wait_for(sse_queue.get(), timeout=1.0)\n                    yield f\"data: {json.dumps(progress_msg)}\\n\\n\"\n                    \n                    # If job is complete, clean up and exit\n                    if progress_msg.get(\"step\") == \"complete\":\n                        del active_jobs[job_id]\n                        break\n                        \n                except asyncio.TimeoutError:\n                    # Send heartbeat to keep connection alive\n                    yield f\"data: {json.dumps({'heartbeat': True})}\\n\\n\"\n                    \n        except Exception as e:\n            yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n    \n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/plain\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Content-Type\": \"text/event-stream\"\n        }\n    )\n\n@app.get(\"/\")\nasync def get_index():\n    \"\"\"Serve the main page\"\"\"\n    return FileResponse(\"static/index.html\")\n\n@app.get(\"/progress.html\")\nasync def get_progress_page():\n    \"\"\"Serve the progress page\"\"\"\n    return FileResponse(\"static/progress.html\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000) ",
      "summary": "Certainly! Here\u2019s a concise summary of the provided code unit:\n\n---\n\n### 1. Primary Purpose\n\nThe code implements a FastAPI backend that starts and manages background article-generation jobs, providing real-time progress updates to clients via Server-Sent Events (SSE). It also serves static HTML pages for the frontend interface.\n\n---\n\n### 2. Brief Description of Parameters\n\n- **start_job endpoint**:  \n  - `background_tasks`: FastAPI\u2019s BackgroundTasks manager, used to schedule background jobs.\n  - `topic` (form field): The topic for article generation (provided as form-data).\n\n- **get_progress endpoint**:  \n  - `job_id` (path parameter): The unique identifier for the article generation job to stream progress for.\n\n---\n\n### 3. Brief Description of Return Value\n\n- **start_job**:  \n  Returns a JSON response containing the newly created job\u2019s ID, the topic, and the status (\"started\").\n\n- **get_progress**:  \n  Returns a streaming HTTP response (SSE) that continually pushes JSON-formatted progress updates for the specified job.\n\n- **get_index & get_progress_page**:  \n  Returns an HTML file (main or progress page) using FileResponse.\n\n---\n\n### 4. Functions or Methods Called Internally\n\n- **FastAPI / starlette internals**:\n  - `FastAPI`, `BackgroundTasks`, `Form`, `StreamingResponse`, `StaticFiles`, `FileResponse`\n- **Standard libraries**:\n  - `asyncio.Queue`, `asyncio.wait_for`, `json.dumps`, `uuid.uuid4`\n- **Application internal**:\n  - `create_article_flow()` (from a local `flow` module): Instantiates and runs the article-generation workflow.\n  - Background task function: `run_article_workflow(job_id, topic)`\n\n---\n\nIf you need more detail on any specific part, let me know!"
    },
    "50": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-background/utils/__init__.py",
      "code": "",
      "summary": "There is no code provided in the /tmp/PocketFlow/cookbook/pocketflow-fastapi-background/utils/__init__.py file (it is either empty or its contents were not included). \n\nSummary:\n1. Primary purpose of the code unit:  \n   As presented, this code unit appears to have no implemented functionality. Typically, an __init__.py file is used to mark a directory as a Python package and may contain package initialization code, but here it contains nothing.\n\n2. Parameters:  \n   None.\n\n3. Return value:  \n   None.\n\n4. Other functions or methods it calls internally:  \n   None."
    },
    "51": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-background/utils/call_llm.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) ",
      "summary": "**Summary of call_llm.py:**\n\n1. **Primary Purpose:**  \n   The code defines a utility function for sending a text prompt to OpenAI's GPT-4o model and retrieving its response using the OpenAI Python SDK.\n\n2. **Parameters:**  \n   - The main function, `call_llm`, takes a single parameter:\n     - `prompt`: A string containing the user's input or question to be passed to the language model.\n\n3. **Return Value:**  \n   - The `call_llm` function returns a string containing the model-generated response to the provided prompt.\n\n4. **Other Functions/Methods Called Internally:**\n   - `OpenAI(api_key=...)`: Initializes the OpenAI client.\n   - `os.environ.get(...)`: Reads the OpenAI API key from environment variables.\n   - `client.chat.completions.create(...)`: Sends the prompt and requests a completion from the GPT-4o model.\n   - Access to `r.choices[0].message.content`: Extracts the generated text from the API response.\n\nAdditionally, if the script is run directly, it demonstrates usage by printing the result of asking the model for a short joke."
    },
    "52": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-workflow/nodes.py",
      "code": "import re\nfrom pocketflow import Node, BatchNode\nfrom utils.call_llm import call_llm\nimport yaml\n\nclass GenerateOutline(Node):\n    def prep(self, shared):\n        return shared[\"topic\"]\n    \n    def exec(self, topic):\n        prompt = f\"\"\"\nCreate a simple outline for an article about {topic}.\nInclude at most 3 main sections (no subsections).\n\nOutput the sections in YAML format as shown below:\n\n```yaml\nsections:\n    - |\n        First section \n    - |\n        Second section\n    - |\n        Third section\n```\"\"\"\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        structured_result = yaml.safe_load(yaml_str)\n        return structured_result\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the structured data\n        shared[\"outline_yaml\"] = exec_res\n        \n        # Extract sections\n        sections = exec_res[\"sections\"]\n        shared[\"sections\"] = sections\n        \n        # Format for display\n        formatted_outline = \"\\n\".join([f\"{i+1}. {section}\" for i, section in enumerate(sections)])\n        shared[\"outline\"] = formatted_outline\n        \n        # Display the results\n        print(\"\\n===== OUTLINE (YAML) =====\\n\")\n        print(yaml.dump(exec_res, default_flow_style=False))\n        print(\"\\n===== PARSED OUTLINE =====\\n\")\n        print(formatted_outline)\n        print(\"\\n=========================\\n\")\n        \n        return \"default\"\n\nclass WriteSimpleContent(BatchNode):\n    def prep(self, shared):\n        # Get the list of sections to process and store for progress tracking\n        self.sections = shared.get(\"sections\", [])\n        return self.sections\n    \n    def exec(self, section):\n        prompt = f\"\"\"\nWrite a short paragraph (MAXIMUM 100 WORDS) about this section:\n\n{section}\n\nRequirements:\n- Explain the idea in simple, easy-to-understand terms\n- Use everyday language, avoiding jargon\n- Keep it very concise (no more than 100 words)\n- Include one brief example or analogy\n\"\"\"\n        content = call_llm(prompt)\n        \n        # Show progress for this section\n        current_section_index = self.sections.index(section) if section in self.sections else 0\n        total_sections = len(self.sections)\n        print(f\"\u2713 Completed section {current_section_index + 1}/{total_sections}: {section}\")\n        \n        return section, content\n    \n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains [(section, content), (section, content), ...]\n        section_contents = {}\n        all_sections_content = []\n        \n        for section, content in exec_res_list:\n            section_contents[section] = content\n            all_sections_content.append(f\"## {section}\\n\\n{content}\\n\")\n        \n        draft = \"\\n\".join(all_sections_content)\n        \n        # Store the section contents and draft\n        shared[\"section_contents\"] = section_contents\n        shared[\"draft\"] = draft\n        \n        print(\"\\n===== SECTION CONTENTS =====\\n\")\n        for section, content in section_contents.items():\n            print(f\"--- {section} ---\")\n            print(content)\n            print()\n        print(\"===========================\\n\")\n        \n        return \"default\"\n\nclass ApplyStyle(Node):\n    def prep(self, shared):\n        \"\"\"\n        Get the draft from shared data\n        \"\"\"\n        return shared[\"draft\"]\n    \n    def exec(self, draft):\n        \"\"\"\n        Apply a specific style to the article\n        \"\"\"\n        prompt = f\"\"\"\n        Rewrite the following draft in a conversational, engaging style:\n        \n        {draft}\n        \n        Make it:\n        - Conversational and warm in tone\n        - Include rhetorical questions that engage the reader\n        - Add analogies and metaphors where appropriate\n        - Include a strong opening and conclusion\n        \"\"\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"\n        Store the final article in shared data\n        \"\"\"\n        shared[\"final_article\"] = exec_res\n        print(\"\\n===== FINAL ARTICLE =====\\n\")\n        print(exec_res)\n        print(\"\\n========================\\n\")\n        return \"default\" ",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary Purpose:**  \n   This code defines nodes for an article-writing workflow that leverages LLMs (Large Language Models) to automatically generate an outline, draft content for each section, and stylize the final article. It modularizes the process into three steps\u2014outline creation, section content writing, and style application\u2014by implementing custom nodes compatible with the PocketFlow workflow framework.\n\n2. **Parameters:**  \n   - The main classes (`GenerateOutline`, `WriteSimpleContent`, `ApplyStyle`) use data passed via a shared dictionary (shared state between nodes) and function parameters as follows:\n     - `prep(self, shared)`: Receives `shared`, a dict for passing state through the workflow.\n     - `exec(self, ...)`: Receives the topic string, section string, or draft string as appropriate for the node's step.\n     - `post(self, shared, prep_res, exec_res/exe_res_list)`: Receives the shared dict and results of the `prep` and `exec` steps.\n   - Node-specific parameters:\n     - `GenerateOutline.exec(topic)`: topic (str) for the article.\n     - `WriteSimpleContent.exec(section)`: section (str), one at a time.\n     - `ApplyStyle.exec(draft)`: draft (str), the full initial article.\n\n3. **Return Values:**  \n   - `prep()` methods: Return the key data needed for each node's processing step (topic, list of sections, or draft string).\n   - `exec()` methods: Produce raw output for that processing step (outline YAML, section content, or styled article text).\n   - `post()` methods: Store processed results back into `shared`; all return the string \"default\" to signal normal completion.\n\n4. **Internal Functions/Methods Called:**\n   - `call_llm(...)`: Invokes the LLM to generate outlines, content, or rewrite drafts.\n   - `yaml.safe_load(...)`: Parses YAML-formatted outlines.\n   - `yaml.dump(...)`: Outputs YAML for display.\n   - Standard Python methods: `split`, `strip`, `enumerate`, `print`, dictionary/list methods, etc.\n   - Inherits from framework base classes: `Node`, `BatchNode`.\n\n**In summary:**  \nThe module is a workflow component for automated article writing, using LLM prompts at each step: generating a simple outline, writing concise content for sections, and applying a conversational style, with data flow managed through a shared dictionary. It internally calls `call_llm` for all LLM-based text generation and uses `yaml` utilities for outline parsing and display."
    },
    "53": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-workflow/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import GenerateOutline, WriteSimpleContent, ApplyStyle\n\ndef create_article_flow():\n    \"\"\"\n    Create and configure the article writing workflow\n    \"\"\"\n    # Create node instances\n    outline_node = GenerateOutline()\n    write_node = WriteSimpleContent()\n    style_node = ApplyStyle()\n    \n    # Connect nodes in sequence\n    outline_node >> write_node >> style_node\n    \n    # Create flow starting with outline node\n    article_flow = Flow(start=outline_node)\n    \n    return article_flow",
      "summary": "**Summary for `flow.py`**\n\n1. **Primary Purpose:**  \n   This code defines and configures a workflow for automated article writing by creating and connecting processing nodes (for outline creation, content writing, and styling) into a linear sequence using the `pocketflow` framework.\n\n2. **Parameters:**  \n   The main function, `create_article_flow`, does not take any parameters.\n\n3. **Return Value:**  \n   The function returns an instance of a `Flow` object that represents the complete article-writing workflow, beginning with the outline generation node.\n\n4. **Internal Functions/Methods Called:**  \n   - `GenerateOutline()`  \n   - `WriteSimpleContent()`  \n   - `ApplyStyle()`  \n   - The `>>` operator is used to connect nodes in sequence.  \n   - `Flow(start=outline_node)` (constructor for creating the workflow)"
    },
    "54": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-workflow/main.py",
      "code": "from flow import create_article_flow\n\ndef run_flow(topic=\"AI Safety\"):\n    \"\"\"\n    Run the article writing workflow with a specific topic\n    \n    Args:\n        topic (str): The topic for the article\n    \"\"\"\n    # Initialize shared data with the topic\n    shared = {\"topic\": topic}\n    \n    # Print starting message\n    print(f\"\\n=== Starting Article Workflow on Topic: {topic} ===\\n\")\n    \n    # Run the flow\n    flow = create_article_flow()\n    flow.run(shared)\n    \n    # Output summary\n    print(\"\\n=== Workflow Completed ===\\n\")\n    print(f\"Topic: {shared['topic']}\")\n    print(f\"Outline Length: {len(shared['outline'])} characters\")\n    print(f\"Draft Length: {len(shared['draft'])} characters\")\n    print(f\"Final Article Length: {len(shared['final_article'])} characters\")\n    \n    return shared\n\nif __name__ == \"__main__\":\n    import sys\n    \n    # Get topic from command line if provided\n    topic = \"AI Safety\"  # Default topic\n    if len(sys.argv) > 1:\n        topic = \" \".join(sys.argv[1:])\n    \n    run_flow(topic)",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-workflow/main.py`:**\n\n1. **Primary Purpose:**  \n   The code is designed to serve as an entry point for running an automated article writing workflow. It initializes a topic, executes the workflow, and displays a summary of the process and results.\n\n2. **Parameters:**  \n   - The main function, `run_flow`, accepts a single optional parameter:\n     - `topic` (str): The subject for the article to be generated. Defaults to `\"AI Safety\"` if not provided.\n\n3. **Return Value:**  \n   - The `run_flow` function returns the `shared` dictionary, which contains:\n     - The input topic, plus results of the workflow such as the article outline, draft, and final article.\n\n4. **Internally Called Functions/Methods:**  \n   - `create_article_flow()` (imported from `flow`)\n   - `flow.run(shared)`\n   - Standard library functions: `print()`, `len()`\n   - Additionally, if executed as the main script, it calls `run_flow(topic)` after optional command-line parsing."
    },
    "55": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-workflow/utils/call_llm.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) ",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   The code provides a utility function to send a user-supplied prompt to OpenAI's GPT-4o model using the OpenAI API, and return the model's response.\n\n2. **Parameters:**  \n   - The main function, `call_llm`, takes a single parameter:\n     - `prompt` (string): The text prompt to be sent to the language model.\n\n3. **Return Value:**  \n   - The function returns the generated text response (string) from the model, specifically the content of the message chosen by the API.\n\n4. **Other Functions or Methods Called Internally:**  \n   - `os.environ.get()` (to obtain the API key)\n   - `OpenAI()` (constructor for the OpenAI API client)\n   - `client.chat.completions.create()` (to send the prompt and receive the completion)\n   - `print()` (used in the example usage block)\n\n**Note:** The script includes an example usage block that demonstrates calling the function with a simple prompt."
    },
    "56": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-flow/flow.py",
      "code": "from pocketflow import Node, Flow\n\nclass TextInput(Node):\n    def prep(self, shared):\n        \"\"\"Get text input from user.\"\"\"\n        if \"text\" not in shared:\n            text = input(\"\\nEnter text to convert: \")\n            shared[\"text\"] = text\n        return shared[\"text\"]\n\n    def post(self, shared, prep_res, exec_res):\n        print(\"\\nChoose transformation:\")\n        print(\"1. Convert to UPPERCASE\")\n        print(\"2. Convert to lowercase\")\n        print(\"3. Reverse text\")\n        print(\"4. Remove extra spaces\")\n        print(\"5. Exit\")\n        \n        choice = input(\"\\nYour choice (1-5): \")\n        \n        if choice == \"5\":\n            return \"exit\"\n        \n        shared[\"choice\"] = choice\n        return \"transform\"\n\nclass TextTransform(Node):\n    def prep(self, shared):\n        return shared[\"text\"], shared[\"choice\"]\n    \n    def exec(self, inputs):\n        text, choice = inputs\n        \n        if choice == \"1\":\n            return text.upper()\n        elif choice == \"2\":\n            return text.lower()\n        elif choice == \"3\":\n            return text[::-1]\n        elif choice == \"4\":\n            return \" \".join(text.split())\n        else:\n            return \"Invalid option!\"\n    \n    def post(self, shared, prep_res, exec_res):\n        print(\"\\nResult:\", exec_res)\n        \n        if input(\"\\nConvert another text? (y/n): \").lower() == 'y':\n            shared.pop(\"text\", None)  # Remove previous text\n            return \"input\"\n        return \"exit\"\n\nclass EndNode(Node):\n    pass\n\n# Create nodes\ntext_input = TextInput()\ntext_transform = TextTransform()\nend_node = EndNode()\n\n# Connect nodes\ntext_input - \"transform\" >> text_transform\ntext_transform - \"input\" >> text_input\ntext_transform - \"exit\" >> end_node\n\n# Create flow\nflow = Flow(start=text_input) ",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary Purpose:**  \n   This code defines a simple interactive text transformation workflow using the PocketFlow framework. It allows users to input text, select a transformation (uppercase, lowercase, reverse, or remove extra spaces), see the result, and optionally process more texts in a loop until they choose to exit.\n\n2. **Parameters:**  \n   The primary parameters used within the nodes are items stored in a shared dictionary (`shared`), notably:\n   - `\"text\"`: Stores the current input text from the user.\n   - `\"choice\"`: Stores the user's selected transformation option.\n\n   Additionally, node methods follow the PocketFlow signature and use:\n   - `prep(self, shared)`: Receives a shared dictionary.\n   - `post(self, shared, prep_res, exec_res)`: Receives the shared dictionary, the preparation result, and the execution result (where applicable).\n   - `exec(self, inputs)`: Receives inputs such as text and transformation choice.\n\n3. **Return Values:**  \n   - The nodes' methods return either new workflow state strings (e.g., `\"transform\"`, `\"input\"`, `\"exit\"`) to guide the next step, or transformed text to be printed.\n   - The interaction is orchestrated via the PocketFlow Node connection mechanism, utilizing these return values to transition between steps.\n\n4. **Other Functions or Methods Called Internally:**\n   - `input()`: For user text and menu input.\n   - `print()`: For outputting menus and transformation results.\n   - `str.upper()`, `str.lower()`, `str[::-1]` (slicing to reverse), and `\" \".join(str.split())`: For transforming the text.\n   - `shared.pop()`: To remove \"text\" from the shared dictionary when looping.\n  \n   Additionally, the nodes inherit from `Node` and are connected via PocketFlow\u2019s custom operators (`-`, `>>`), and they are composed into a `Flow` object to manage execution.\n\n**In summary:**  \nThis module sets up a command-line text transformation workflow where each step\u2014input, transform, and end\u2014is managed by a separate Node, using user inputs to move through the stages of the flow."
    },
    "57": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-flow/main.py",
      "code": "from flow import flow\n\ndef main():\n    print(\"\\nWelcome to Text Converter!\")\n    print(\"=========================\")\n    \n    # Initialize shared store\n    shared = {}\n    \n    # Run the flow\n    flow.run(shared)\n    \n    print(\"\\nThank you for using Text Converter!\")\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of code unit: `/tmp/PocketFlow/cookbook/pocketflow-flow/main.py`**\n\n1. **Primary purpose:**  \n   The code serves as the main entry point for a text conversion application called \"Text Converter\". It initializes a shared data store and starts the application's main process by invoking a flow defined in another module.\n\n2. **Parameters:**  \n   The `main()` function does not take any parameters.\n\n3. **Return value:**  \n   The `main()` function does not return any value.\n\n4. **Internally called functions or methods:**  \n   - `flow.run(shared)` (from the imported `flow` module)\n   - `print()` (standard Python function)"
    },
    "58": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/nodes.py",
      "code": "from pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass AnswerQuestion(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n        \n        # We're done - no need to continue the flow\n        return \"done\" \n",
      "summary": "**Summary of nodes.py**\n\n1. **Primary Purpose:**  \nThis code defines three nodes\u2014`DecideAction`, `SearchWeb`, and `AnswerQuestion`\u2014that collectively implement a web research agent workflow. The agent decides whether to search the web or answer a user\u2019s question directly, performs web searches as needed, and composes a final answer using a language model. These nodes are designed to work as steps in an automated decision and information retrieval pipeline.\n\n2. **Parameters:**  \nEach node operates on a shared dictionary (named `shared`) for passing context, queries, and results between steps.  \n- `prep` methods take the shared dictionary as input and extract relevant information for the node.\n- `exec` methods receive the output from `prep` (either as a value or tuple) and perform the node\u2019s core logic.\n- `post` methods update the shared store and determine the next step in the flow.\n\n3. **Return Value:**  \n- `prep` returns the data needed for execution (e.g., question/context tuple, search query).\n- `exec` returns the result of the node\u2019s main computation (e.g., the LLM\u2019s decision, web search results, or generated answer).\n- `post` updates the shared context and returns a string indicating the next node (e.g., `\"search\"`, `\"decide\"`, `\"done\"`), which directs the control flow.\n\n4. **Internally Called Functions/Methods:**  \n- **`call_llm`**: Invokes a language model to either decide on an action or generate an answer.\n- **`search_web`**: Executes a web search using a query string.\n- **`yaml.safe_load`**: Parses the language model\u2019s YAML-formatted response.\n- **Standard Python dictionary methods**: such as `get`, indexing, and string manipulation.\n\n**In brief:**  \nThe code orchestrates an agent workflow using LLM-based decision-making, web search, and answer synthesis, coordinated through a shared data store and modular node classes."
    },
    "59": {
      "unit_name": "task_manager.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/task_manager.py",
      "code": "# FILE: pocketflow_a2a_agent/task_manager.py\nimport logging\nfrom typing import AsyncIterable, Union\nimport asyncio\n\n# Import from the common code you copied\nfrom common.server.task_manager import InMemoryTaskManager\nfrom common.types import (\n    JSONRPCResponse, SendTaskRequest, SendTaskResponse,\n    SendTaskStreamingRequest, SendTaskStreamingResponse, Task, TaskSendParams,\n    TaskState, TaskStatus, TextPart, Artifact, UnsupportedOperationError,\n    InternalError, InvalidParamsError, \n    Message\n)\nimport common.server.utils as server_utils\n\n# Import directly from your original PocketFlow files\nfrom flow import create_agent_flow\n\nlogger = logging.getLogger(__name__)\n\nclass PocketFlowTaskManager(InMemoryTaskManager):\n    \"\"\" TaskManager implementation that runs the PocketFlow agent. \"\"\"\n\n    SUPPORTED_CONTENT_TYPES = [\"text\", \"text/plain\"] # Define what the agent accepts/outputs\n\n    async def on_send_task(self, request: SendTaskRequest) -> SendTaskResponse:\n        \"\"\"Handles non-streaming task requests.\"\"\"\n        logger.info(f\"Received task send request: {request.params.id}\")\n\n        # Validate output modes\n        if not server_utils.are_modalities_compatible(\n            request.params.acceptedOutputModes, self.SUPPORTED_CONTENT_TYPES\n        ):\n            logger.warning(\n                \"Unsupported output mode. Received %s, Support %s\",\n                request.params.acceptedOutputModes, self.SUPPORTED_CONTENT_TYPES\n            )\n            return SendTaskResponse(id=request.id, error=server_utils.new_incompatible_types_error(request.id).error)\n\n        # Upsert the task in the store (initial state: submitted)\n        # We create the task first so its state can be tracked, even if the sync execution fails\n        await self.upsert_task(request.params)\n        # Update state to working before running\n        await self.update_store(request.params.id, TaskStatus(state=TaskState.WORKING), [])\n\n\n        # --- Run the PocketFlow logic ---\n        task_params: TaskSendParams = request.params\n        query = self._get_user_query(task_params)\n        if query is None:\n            fail_status = TaskStatus(state=TaskState.FAILED, message=Message(role=\"agent\", parts=[TextPart(text=\"No text query found\")]))\n            await self.update_store(task_params.id, fail_status, [])\n            return SendTaskResponse(id=request.id, error=InvalidParamsError(message=\"No text query found in message parts\"))\n\n        shared_data = {\"question\": query}\n        agent_flow = create_agent_flow() # Create the flow instance\n\n        try:\n            # Run the synchronous PocketFlow\n            # In a real async server, you might run this in a separate thread/process\n            # executor to avoid blocking the event loop. For simplicity here, we run it directly.\n            # Consider adding a timeout if flows can hang.\n            logger.info(f\"Running PocketFlow for task {task_params.id}...\")\n            agent_flow.run(shared_data) # Run the flow, modifying shared_data in place\n            logger.info(f\"PocketFlow completed for task {task_params.id}\")\n            # Access the original shared_data dictionary, which was modified by the flow\n            answer_text = shared_data.get(\"answer\", \"Agent did not produce a final answer text.\")\n\n            # --- Package result into A2A Task ---\n            final_task_status = TaskStatus(state=TaskState.COMPLETED)\n            # Package the answer as an artifact\n            final_artifact = Artifact(parts=[TextPart(text=answer_text)])\n\n            # Update the task in the store with final status and artifact\n            final_task = await self.update_store(\n                task_params.id, final_task_status, [final_artifact]\n            )\n\n            # Prepare and return the A2A response\n            task_result = self.append_task_history(final_task, task_params.historyLength)\n            return SendTaskResponse(id=request.id, result=task_result)\n\n        except Exception as e:\n            logger.error(f\"Error executing PocketFlow for task {task_params.id}: {e}\", exc_info=True)\n            # Update task state to FAILED\n            fail_status = TaskStatus(\n                state=TaskState.FAILED,\n                message=Message(role=\"agent\", parts=[TextPart(text=f\"Agent execution failed: {e}\")])\n            )\n            await self.update_store(task_params.id, fail_status, [])\n            return SendTaskResponse(id=request.id, error=InternalError(message=f\"Agent error: {e}\"))\n\n    async def on_send_task_subscribe(\n        self, request: SendTaskStreamingRequest\n    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:\n        \"\"\"Handles streaming requests - Not implemented for this synchronous agent.\"\"\"\n        logger.warning(f\"Streaming requested for task {request.params.id}, but not supported by this PocketFlow agent implementation.\")\n        # Return an error indicating streaming is not supported\n        return JSONRPCResponse(id=request.id, error=UnsupportedOperationError(message=\"Streaming not supported by this agent\"))\n\n    def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:\n        \"\"\"Extracts the first text part from the user message.\"\"\"\n        if not task_send_params.message or not task_send_params.message.parts:\n            logger.warning(f\"No message parts found for task {task_send_params.id}\")\n            return None\n        for part in task_send_params.message.parts:\n            # Ensure part is treated as a dictionary if it came from JSON\n            part_dict = part if isinstance(part, dict) else part.model_dump()\n            if part_dict.get(\"type\") == \"text\" and \"text\" in part_dict:\n                 return part_dict[\"text\"]\n        logger.warning(f\"No text part found in message for task {task_send_params.id}\")\n        return None # No text part found",
      "summary": "**Summary of `task_manager.py`:**\n\n1. **Primary Purpose:**  \n   The `task_manager.py` code unit defines a `PocketFlowTaskManager` class, which handles AI Agent-to-Agent (A2A) task requests for the PocketFlow agent. It integrates the PocketFlow agent logic with a server task management interface, processing incoming tasks, running the agent, and managing task state and results.\n\n2. **Parameters (for main entry points):**  \n   - `on_send_task(request: SendTaskRequest)`: Receives a task execution request, where `request` contains task parameters such as task ID, output modes, user message, and more.\n   - `on_send_task_subscribe(request: SendTaskStreamingRequest)`: Receives a request to stream task results (not supported in this agent; included for interface completeness).\n\n3. **Return Value:**  \n   - `on_send_task`: Returns a `SendTaskResponse` containing either the completed task result or error details.\n   - `on_send_task_subscribe`: Returns either an error response (since streaming is unsupported) or an async iterable (never produced here).\n\n4. **Other Functions or Methods Called Internally:**\n   - `server_utils.are_modalities_compatible`  \n   - `server_utils.new_incompatible_types_error`\n   - `self.upsert_task`\n   - `self.update_store`\n   - `self.append_task_history`\n   - `create_agent_flow` (from `flow.py`)\n   - `agent_flow.run`\n   - Exception and logging methods (`logger.info`, `logger.warning`, `logger.error`)\n   - Helper: `self._get_user_query`\n   - Various class methods/properties from imported data types (`TaskStatus`, `Message`, `TextPart`, `Artifact`, `InternalError`, etc.)\n\n**Overall**, this code provides the glue between generic server task management and the specific execution of the PocketFlow agent, handling request validation, execution, error handling, and results packaging."
    },
    "60": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    \"\"\"\n    Create and connect the nodes to form a complete agent flow.\n    \n    The flow works like this:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node\n    3. If answer, go to AnswerQuestion node\n    4. After SearchWeb completes, go back to DecideAction\n    \n    Returns:\n        Flow: A complete research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n    \n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n    \n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n    \n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n    \n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-a2a/flow.py`:**\n\n1. **Primary Purpose:**  \n   This code defines the structure of a research agent's flow, orchestrating how decisions, web searches, and answering tasks are chained together in a loop using the `Flow` orchestration tool.\n\n2. **Parameters:**  \n   The `create_agent_flow` function does not take any parameters.\n\n3. **Return Value:**  \n   The function returns a `Flow` object that represents the complete agent workflow, beginning with the `DecideAction` node and outlining transitions among `DecideAction`, `SearchWeb`, and `AnswerQuestion`.\n\n4. **Internally Called Functions/Methods:**  \n   - `DecideAction()` (from `nodes` module): Instantiates the node that decides whether to search or answer.\n   - `SearchWeb()` (from `nodes`): Instantiates the node responsible for web search.\n   - `AnswerQuestion()` (from `nodes`): Instantiates the node responsible for answering questions.\n   - `Flow(start=...)` (from `pocketflow`): Creates the flow, specifying the starting node.\n   - Uses custom operators (e.g., `-`, `>>`) to define directed transitions between nodes based on output labels.\n\n**In summary:**  \nThe code sets up an automated agent workflow that decides, searches, and answers in a loop, and returns this flow as a reusable object."
    },
    "61": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/main.py",
      "code": "import sys\nfrom flow import create_agent_flow\n\ndef main():\n    \"\"\"Simple function to process a question.\"\"\"\n    # Default question\n    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    # Create the agent flow\n    agent_flow = create_agent_flow()\n    \n    # Process the question\n    shared = {\"question\": question}\n    print(f\"\ud83e\udd14 Processing question: {question}\")\n    agent_flow.run(shared)\n    print(\"\\n\ud83c\udfaf Final Answer:\")\n    print(shared.get(\"answer\", \"No answer found\"))\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-a2a/main.py`:**\n\n1. **Primary Purpose:**  \n   The code acts as an entry-point script that processes a user\u2019s question\u2014either a default Nobel Prize question or one provided via the command line\u2014by sending it to an agent flow (presumably an AI or NLP model), then outputs the final answer.\n\n2. **Parameters:**  \n   - The script takes an optional command-line argument: if any argument prefixed with `--` is provided (e.g., `--What is AI?`), the text after `--` is used as the question instead of the default.  \n   - The `main()` function itself takes no parameters.\n\n3. **Return Value:**  \n   - The script and the `main()` function do not return any explicit value; they perform their operations and print output to the console.\n\n4. **Other Functions/Methods Called:**  \n   - `sys.argv`: to access command-line arguments.\n   - `create_agent_flow()` (from the imported `flow` module): to create the agent flow object.\n   - `agent_flow.run(shared)`: processes the question in the `shared` dictionary.\n   - `print()`: displays the question being processed and the final answer.\n\n**In summary:**  \nThe code is a simple command-line interface that takes a question, processes it through an agent workflow, and prints out the answer, relying internally on functions and methods imported from a `flow` module."
    },
    "62": {
      "unit_name": "a2a_server.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/a2a_server.py",
      "code": "import click\nimport logging\nimport os\n\n# Import from the common code you copied\nfrom common.server import A2AServer\nfrom common.types import AgentCard, AgentCapabilities, AgentSkill, MissingAPIKeyError\n\n# Import your custom TaskManager (which now imports from your original files)\nfrom task_manager import PocketFlowTaskManager\n\n# --- Configure logging ---\n# Set level to INFO to see server start, requests, responses\n# Set level to DEBUG to see raw response bodies from client\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n# Optionally silence overly verbose libraries\n# logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n# logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n# logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n\nlogger = logging.getLogger(__name__)\n\n@click.command()\n@click.option(\"--host\", \"host\", default=\"localhost\")\n@click.option(\"--port\", \"port\", default=10003) # Use a different port from other agents\ndef main(host, port):\n    \"\"\"Starts the PocketFlow A2A Agent server.\"\"\"\n    try:\n        # Check for necessary API keys (add others if needed)\n        if not os.getenv(\"OPENAI_API_KEY\"):\n            raise MissingAPIKeyError(\"OPENAI_API_KEY environment variable not set.\")\n\n        # --- Define the Agent Card ---\n        capabilities = AgentCapabilities(\n            streaming=False, # This simple implementation is synchronous\n            pushNotifications=False,\n            stateTransitionHistory=False # PocketFlow state isn't exposed via A2A history\n        )\n        skill = AgentSkill(\n            id=\"web_research_qa\",\n            name=\"Web Research and Answering\",\n            description=\"Answers questions using web search results when necessary.\",\n            tags=[\"research\", \"qa\", \"web search\"],\n            examples=[\n                \"Who won the Nobel Prize in Physics 2024?\",\n                \"What is quantum computing?\",\n                \"Summarize the latest news about AI.\",\n            ],\n            # Input/Output modes defined in the TaskManager\n            inputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,\n            outputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,\n        )\n        agent_card = AgentCard(\n            name=\"PocketFlow Research Agent (A2A Wrapped)\",\n            description=\"A simple research agent based on PocketFlow, made accessible via A2A.\",\n            url=f\"http://{host}:{port}/\", # The endpoint A2A clients will use\n            version=\"0.1.0-a2a\",\n            capabilities=capabilities,\n            skills=[skill],\n            # Assuming no specific provider or auth for this example\n            provider=None,\n            authentication=None,\n            defaultInputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,\n            defaultOutputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,\n        )\n\n        # --- Initialize and Start Server ---\n        task_manager = PocketFlowTaskManager() # Instantiate your custom manager\n        server = A2AServer(\n            agent_card=agent_card,\n            task_manager=task_manager,\n            host=host,\n            port=port,\n        )\n\n        logger.info(f\"Starting PocketFlow A2A server on http://{host}:{port}\")\n        server.start()\n\n    except MissingAPIKeyError as e:\n        logger.error(f\"Configuration Error: {e}\")\n        exit(1)\n    except Exception as e:\n        logger.error(f\"An error occurred during server startup: {e}\", exc_info=True)\n        exit(1)\n\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of /tmp/PocketFlow/cookbook/pocketflow-a2a/a2a_server.py**\n\n1. **Primary purpose:**  \n   This code unit defines and launches an HTTP server for the PocketFlow agent, exposing its \"web research and answering\" capabilities via an Agent-to-Agent (A2A) interface. The server advertises its capabilities using an agent card and uses a custom task manager to handle incoming tasks.\n\n2. **Parameters:**  \n   The main function accepts two command-line options:\n   - `host`: The network host/IP address where the server will listen (default: \"localhost\").\n   - `port`: The TCP port for the server to accept connections (default: 10003).\n\n3. **Return value:**  \n   The `main` function does not return a value. Instead, it starts the server and continues running, or exits with an error code if startup fails.\n\n4. **Internally called functions/methods/classes:**\n   - `os.getenv`: Retrieves environment variables (used to check for API keys).\n   - `logging.getLogger` & `logging.basicConfig`: Used for logging setup.\n   - `AgentCapabilities`, `AgentSkill`, `AgentCard`: Classes used to define the agent\u2019s features.\n   - `PocketFlowTaskManager`: Instantiated to handle tasks for the agent.\n   - `A2AServer`: Instantiated and started to serve HTTP requests.\n   - `server.start()`: Launches the HTTP server.\n   - Error handling uses `logger.error` and `exit`.\n   - The code uses the `click` library's decorators and command-line parsing features. \n\n**Note:**  \nThere are no functions defined other than `main()`, which is used as the entry point; execution is controlled by the `if __name__ == \"__main__\":` block. The actual business logic for handling tasks resides in the imported `PocketFlowTaskManager`."
    },
    "63": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/utils.py",
      "code": "from openai import OpenAI\nimport os\nfrom duckduckgo_search import DDGS\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef search_web(query):\n    results = DDGS().text(query, max_results=5)\n    # Convert results to a string\n    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n    return results_str\n    \nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n    print(\"## Testing search_web\")\n    query = \"Who won the Nobel Prize in Physics 2024?\"\n    print(f\"## Query: {query}\")\n    results = search_web(query)\n    print(f\"## Results: {results}\")",
      "summary": "**Summary of code unit: `utils.py`**\n\n1. **Primary Purpose**  \n   This code unit provides utility functions for interacting with the OpenAI language model and for searching the web using DuckDuckGo. It enables generating text completions from a language model via a prompt, and fetching top web search results for a query.\n\n2. **Parameters**  \n   - `call_llm(prompt)`:  \n     - `prompt` (str): The text prompt sent to the OpenAI language model.\n   - `search_web(query)`:  \n     - `query` (str): The search query string to be used for web search.\n\n3. **Return Value**  \n   - `call_llm`: Returns a string, which is the OpenAI model's generated response for the input prompt.\n   - `search_web`: Returns a string representation of the top 5 DuckDuckGo search results (each with title, URL, and snippet).\n\n4. **Other Functions or Methods Called Internally**  \n   - `OpenAI`: Class from `openai` used to create the API client.\n   - `client.chat.completions.create`: Method used to create a chat completion (generate a response) from OpenAI.\n   - `os.environ.get`: Fetches the environment variable for the OpenAI API key.\n   - `DDGS().text`: Method from the `duckduckgo_search` package to perform a web search.\n   - Standard Python methods: string formatting, joining lists.\n\nAdditionally, in the `if __name__ == \"__main__\"` block, both utility functions are invoked with example inputs to demonstrate/test their behavior."
    },
    "64": {
      "unit_name": "a2a_client.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/a2a_client.py",
      "code": "import asyncio\nimport asyncclick as click # Using asyncclick for async main\nfrom uuid import uuid4\nimport json # For potentially inspecting raw errors\nimport anyio\nimport functools\nimport logging\n\n# Import from the common directory placed alongside this script\nfrom common.client import A2AClient\nfrom common.types import (\n    TaskState,\n    A2AClientError,\n    TextPart, # Used to construct the message\n    JSONRPCResponse # Potentially useful for error checking\n)\n\n# --- Configure logging ---\n# Set level to INFO to see client requests and responses\n# Set level to DEBUG to see raw response bodies and SSE data lines\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n# Optionally silence overly verbose libraries\n# logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n# logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n\n# --- ANSI Colors (Optional but helpful) ---\nC_RED = \"\\x1b[31m\"\nC_GREEN = \"\\x1b[32m\"\nC_YELLOW = \"\\x1b[33m\"\nC_BLUE = \"\\x1b[34m\"\nC_MAGENTA = \"\\x1b[35m\"\nC_CYAN = \"\\x1b[36m\"\nC_WHITE = \"\\x1b[37m\"\nC_GRAY = \"\\x1b[90m\"\nC_BRIGHT_MAGENTA = \"\\x1b[95m\"\nC_RESET = \"\\x1b[0m\"\nC_BOLD = \"\\x1b[1m\"\n\ndef colorize(color, text):\n    return f\"{color}{text}{C_RESET}\"\n\n@click.command()\n@click.option(\n    \"--agent-url\",\n    default=\"http://localhost:10003\", # Default to the port used in server __main__\n    help=\"URL of the PocketFlow A2A agent server.\",\n)\nasync def cli(agent_url: str):\n    \"\"\"Minimal CLI client to interact with an A2A agent.\"\"\"\n\n    print(colorize(C_BRIGHT_MAGENTA, f\"Connecting to agent at: {agent_url}\"))\n\n    # Instantiate the client - only URL is needed if not fetching card first\n    # Note: The PocketFlow wrapper doesn't expose much via the AgentCard,\n    # so we skip fetching it for this minimal client.\n    client = A2AClient(url=agent_url)\n\n    sessionId = uuid4().hex # Generate a new session ID for this run\n    print(colorize(C_GRAY, f\"Using Session ID: {sessionId}\"))\n\n    while True:\n        taskId = uuid4().hex # Generate a new task ID for each interaction\n        try:\n            # Use functools.partial to prepare the prompt function call\n            prompt_func = functools.partial(\n                click.prompt,\n                colorize(C_CYAN, \"\\nEnter your question (:q or quit to exit)\"),\n                prompt_suffix=\" > \",\n                type=str\n            )\n            # Run the synchronous prompt function in a worker thread\n            prompt = await anyio.to_thread.run_sync(prompt_func)\n        except (EOFError, RuntimeError, KeyboardInterrupt):\n            # Catch potential errors during input or if stdin closes\n            print(colorize(C_RED, \"\\nInput closed or interrupted. Exiting.\"))\n            break\n\n        if prompt.lower() in [\":q\", \"quit\"]:\n            print(colorize(C_YELLOW, \"Exiting client.\"))\n            break\n\n        # --- Construct A2A Request Payload ---\n        payload = {\n            \"id\": taskId,\n            \"sessionId\": sessionId,\n            \"message\": {\n                \"role\": \"user\",\n                \"parts\": [\n                    {\n                        \"type\": \"text\", # Explicitly match TextPart structure\n                        \"text\": prompt,\n                    }\n                ],\n            },\n            \"acceptedOutputModes\": [\"text\", \"text/plain\"], # What the client wants back\n            # historyLength could be added if needed\n        }\n\n        print(colorize(C_GRAY, f\"Sending task {taskId}...\"))\n\n        try:\n            # --- Send Task (Non-Streaming) ---\n            response = await client.send_task(payload)\n\n            # --- Process Response ---\n            if response.error:\n                print(colorize(C_RED, f\"Error from agent (Code: {response.error.code}): {response.error.message}\"))\n                if response.error.data:\n                    print(colorize(C_GRAY, f\"Error Data: {response.error.data}\"))\n            elif response.result:\n                task_result = response.result\n                print(colorize(C_GREEN, f\"Task {task_result.id} finished with state: {task_result.status.state}\"))\n\n                final_answer = \"Agent did not provide a final artifact.\"\n                # Extract answer from artifacts (as implemented in PocketFlowTaskManager)\n                if task_result.artifacts:\n                    try:\n                        # Find the first text part in the first artifact\n                        first_artifact = task_result.artifacts[0]\n                        first_text_part = next(\n                            (p for p in first_artifact.parts if isinstance(p, TextPart)),\n                            None\n                        )\n                        if first_text_part:\n                            final_answer = first_text_part.text\n                        else:\n                             final_answer = f\"(Non-text artifact received: {first_artifact.parts})\"\n                    except (IndexError, AttributeError, TypeError) as e:\n                        final_answer = f\"(Error parsing artifact: {e})\"\n                elif task_result.status.message and task_result.status.message.parts:\n                     # Fallback to status message if no artifact\n                     try:\n                        first_text_part = next(\n                             (p for p in task_result.status.message.parts if isinstance(p, TextPart)),\n                             None\n                         )\n                        if first_text_part:\n                            final_answer = f\"(Final status message: {first_text_part.text})\"\n\n                     except (AttributeError, TypeError) as e:\n                         final_answer = f\"(Error parsing status message: {e})\"\n\n\n                print(colorize(C_BOLD + C_WHITE, f\"\\nAgent Response:\\n{final_answer}\"))\n\n            else:\n                # Should not happen if error is None\n                print(colorize(C_YELLOW, \"Received response with no result and no error.\"))\n\n        except A2AClientError as e:\n            print(colorize(C_RED, f\"\\nClient Error: {e}\"))\n        except Exception as e:\n            print(colorize(C_RED, f\"\\nAn unexpected error occurred: {e}\"))\n\nif __name__ == \"__main__\":\n    asyncio.run(cli())",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-a2a/a2a_client.py`:**\n\n1. **Primary Purpose**  \nThis code implements a minimal asynchronous command-line client for interacting with a PocketFlow A2A (Agent-to-Agent) server. It allows users to enter a question, sends it as a task to the agent API, and prints the agent's response in a colored CLI.\n\n2. **Parameters**  \n- The script defines a single CLI option:\n    - `--agent-url`: The URL of the PocketFlow A2A agent server. Defaults to `http://localhost:10003`.\n\n3. **Return Value**  \n- The main CLI function does not return a value; instead, it prints results, colored outputs, or error messages directly to the terminal.\n\n4. **Functions / Methods Invoked Internally**  \n- `asyncio.run()` \u2014 to start the async CLI.\n- `A2AClient()` **(from `common.client`)** \u2014 to instantiate the client for sending tasks.\n- `uuid4().hex` \u2014 to generate session and task IDs.\n- `functools.partial()` \u2014 for preparing the synchronous prompt call.\n- `anyio.to_thread.run_sync()` \u2014 for running the blocking user prompt in a thread worker asynchronously.\n- `click.prompt()` (via asyncclick) \u2014 prompts the user for input.\n- `client.send_task()` **(from `A2AClient`)** \u2014 sends the constructed task payload to the agent.\n- Python built-ins and stdlib:\n    - `print()`\n    - Exception handling (`try/except`)\n    - Logging configuration\n- Utility/color functions:\n    - `colorize()` \u2014 helper function to print colored terminal output.\n\n**Flow:**  \n- Prompts user for input until exit,\n- Builds and sends the input as a task to the agent,\n- Receives and parses the agent's JSONRPC response,\n- Extracts and prints the final answer/artifact or error,\n- Handles errors (client/server/keyboard).\n\n**Note:**  \nAll I/O is asynchronous; underlying prompt is handled in a thread. The minimal design skips features like card fetching or streaming."
    },
    "65": {
      "unit_name": "types.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/types.py",
      "code": "from typing import Union, Any\nfrom pydantic import BaseModel, Field, TypeAdapter\nfrom typing import Literal, List, Annotated, Optional\nfrom datetime import datetime\nfrom pydantic import model_validator, ConfigDict, field_serializer\nfrom uuid import uuid4\nfrom enum import Enum\nfrom typing_extensions import Self\n\n\nclass TaskState(str, Enum):\n    SUBMITTED = \"submitted\"\n    WORKING = \"working\"\n    INPUT_REQUIRED = \"input-required\"\n    COMPLETED = \"completed\"\n    CANCELED = \"canceled\"\n    FAILED = \"failed\"\n    UNKNOWN = \"unknown\"\n\n\nclass TextPart(BaseModel):\n    type: Literal[\"text\"] = \"text\"\n    text: str\n    metadata: dict[str, Any] | None = None\n\n\nclass FileContent(BaseModel):\n    name: str | None = None\n    mimeType: str | None = None\n    bytes: str | None = None\n    uri: str | None = None\n\n    @model_validator(mode=\"after\")\n    def check_content(self) -> Self:\n        if not (self.bytes or self.uri):\n            raise ValueError(\"Either 'bytes' or 'uri' must be present in the file data\")\n        if self.bytes and self.uri:\n            raise ValueError(\n                \"Only one of 'bytes' or 'uri' can be present in the file data\"\n            )\n        return self\n\n\nclass FilePart(BaseModel):\n    type: Literal[\"file\"] = \"file\"\n    file: FileContent\n    metadata: dict[str, Any] | None = None\n\n\nclass DataPart(BaseModel):\n    type: Literal[\"data\"] = \"data\"\n    data: dict[str, Any]\n    metadata: dict[str, Any] | None = None\n\n\nPart = Annotated[Union[TextPart, FilePart, DataPart], Field(discriminator=\"type\")]\n\n\nclass Message(BaseModel):\n    role: Literal[\"user\", \"agent\"]\n    parts: List[Part]\n    metadata: dict[str, Any] | None = None\n\n\nclass TaskStatus(BaseModel):\n    state: TaskState\n    message: Message | None = None\n    timestamp: datetime = Field(default_factory=datetime.now)\n\n    @field_serializer(\"timestamp\")\n    def serialize_dt(self, dt: datetime, _info):\n        return dt.isoformat()\n\n\nclass Artifact(BaseModel):\n    name: str | None = None\n    description: str | None = None\n    parts: List[Part]\n    metadata: dict[str, Any] | None = None\n    index: int = 0\n    append: bool | None = None\n    lastChunk: bool | None = None\n\n\nclass Task(BaseModel):\n    id: str\n    sessionId: str | None = None\n    status: TaskStatus\n    artifacts: List[Artifact] | None = None\n    history: List[Message] | None = None\n    metadata: dict[str, Any] | None = None\n\n\nclass TaskStatusUpdateEvent(BaseModel):\n    id: str\n    status: TaskStatus\n    final: bool = False\n    metadata: dict[str, Any] | None = None\n\n\nclass TaskArtifactUpdateEvent(BaseModel):\n    id: str\n    artifact: Artifact    \n    metadata: dict[str, Any] | None = None\n\n\nclass AuthenticationInfo(BaseModel):\n    model_config = ConfigDict(extra=\"allow\")\n\n    schemes: List[str]\n    credentials: str | None = None\n\n\nclass PushNotificationConfig(BaseModel):\n    url: str\n    token: str | None = None\n    authentication: AuthenticationInfo | None = None\n\n\nclass TaskIdParams(BaseModel):\n    id: str\n    metadata: dict[str, Any] | None = None\n\n\nclass TaskQueryParams(TaskIdParams):\n    historyLength: int | None = None\n\n\nclass TaskSendParams(BaseModel):\n    id: str\n    sessionId: str = Field(default_factory=lambda: uuid4().hex)\n    message: Message\n    acceptedOutputModes: Optional[List[str]] = None\n    pushNotification: PushNotificationConfig | None = None\n    historyLength: int | None = None\n    metadata: dict[str, Any] | None = None\n\n\nclass TaskPushNotificationConfig(BaseModel):\n    id: str\n    pushNotificationConfig: PushNotificationConfig\n\n\n## RPC Messages\n\n\nclass JSONRPCMessage(BaseModel):\n    jsonrpc: Literal[\"2.0\"] = \"2.0\"\n    id: int | str | None = Field(default_factory=lambda: uuid4().hex)\n\n\nclass JSONRPCRequest(JSONRPCMessage):\n    method: str\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCError(BaseModel):\n    code: int\n    message: str\n    data: Any | None = None\n\n\nclass JSONRPCResponse(JSONRPCMessage):\n    result: Any | None = None\n    error: JSONRPCError | None = None\n\n\nclass SendTaskRequest(JSONRPCRequest):\n    method: Literal[\"tasks/send\"] = \"tasks/send\"\n    params: TaskSendParams\n\n\nclass SendTaskResponse(JSONRPCResponse):\n    result: Task | None = None\n\n\nclass SendTaskStreamingRequest(JSONRPCRequest):\n    method: Literal[\"tasks/sendSubscribe\"] = \"tasks/sendSubscribe\"\n    params: TaskSendParams\n\n\nclass SendTaskStreamingResponse(JSONRPCResponse):\n    result: TaskStatusUpdateEvent | TaskArtifactUpdateEvent | None = None\n\n\nclass GetTaskRequest(JSONRPCRequest):\n    method: Literal[\"tasks/get\"] = \"tasks/get\"\n    params: TaskQueryParams\n\n\nclass GetTaskResponse(JSONRPCResponse):\n    result: Task | None = None\n\n\nclass CancelTaskRequest(JSONRPCRequest):\n    method: Literal[\"tasks/cancel\",] = \"tasks/cancel\"\n    params: TaskIdParams\n\n\nclass CancelTaskResponse(JSONRPCResponse):\n    result: Task | None = None\n\n\nclass SetTaskPushNotificationRequest(JSONRPCRequest):\n    method: Literal[\"tasks/pushNotification/set\",] = \"tasks/pushNotification/set\"\n    params: TaskPushNotificationConfig\n\n\nclass SetTaskPushNotificationResponse(JSONRPCResponse):\n    result: TaskPushNotificationConfig | None = None\n\n\nclass GetTaskPushNotificationRequest(JSONRPCRequest):\n    method: Literal[\"tasks/pushNotification/get\",] = \"tasks/pushNotification/get\"\n    params: TaskIdParams\n\n\nclass GetTaskPushNotificationResponse(JSONRPCResponse):\n    result: TaskPushNotificationConfig | None = None\n\n\nclass TaskResubscriptionRequest(JSONRPCRequest):\n    method: Literal[\"tasks/resubscribe\",] = \"tasks/resubscribe\"\n    params: TaskIdParams\n\n\nA2ARequest = TypeAdapter(\n    Annotated[\n        Union[\n            SendTaskRequest,\n            GetTaskRequest,\n            CancelTaskRequest,\n            SetTaskPushNotificationRequest,\n            GetTaskPushNotificationRequest,\n            TaskResubscriptionRequest,\n            SendTaskStreamingRequest,\n        ],\n        Field(discriminator=\"method\"),\n    ]\n)\n\n## Error types\n\n\nclass JSONParseError(JSONRPCError):\n    code: int = -32700\n    message: str = \"Invalid JSON payload\"\n    data: Any | None = None\n\n\nclass InvalidRequestError(JSONRPCError):\n    code: int = -32600\n    message: str = \"Request payload validation error\"\n    data: Any | None = None\n\n\nclass MethodNotFoundError(JSONRPCError):\n    code: int = -32601\n    message: str = \"Method not found\"\n    data: None = None\n\n\nclass InvalidParamsError(JSONRPCError):\n    code: int = -32602\n    message: str = \"Invalid parameters\"\n    data: Any | None = None\n\n\nclass InternalError(JSONRPCError):\n    code: int = -32603\n    message: str = \"Internal error\"\n    data: Any | None = None\n\n\nclass TaskNotFoundError(JSONRPCError):\n    code: int = -32001\n    message: str = \"Task not found\"\n    data: None = None\n\n\nclass TaskNotCancelableError(JSONRPCError):\n    code: int = -32002\n    message: str = \"Task cannot be canceled\"\n    data: None = None\n\n\nclass PushNotificationNotSupportedError(JSONRPCError):\n    code: int = -32003\n    message: str = \"Push Notification is not supported\"\n    data: None = None\n\n\nclass UnsupportedOperationError(JSONRPCError):\n    code: int = -32004\n    message: str = \"This operation is not supported\"\n    data: None = None\n\n\nclass ContentTypeNotSupportedError(JSONRPCError):\n    code: int = -32005\n    message: str = \"Incompatible content types\"\n    data: None = None\n\n\nclass AgentProvider(BaseModel):\n    organization: str\n    url: str | None = None\n\n\nclass AgentCapabilities(BaseModel):\n    streaming: bool = False\n    pushNotifications: bool = False\n    stateTransitionHistory: bool = False\n\n\nclass AgentAuthentication(BaseModel):\n    schemes: List[str]\n    credentials: str | None = None\n\n\nclass AgentSkill(BaseModel):\n    id: str\n    name: str\n    description: str | None = None\n    tags: List[str] | None = None\n    examples: List[str] | None = None\n    inputModes: List[str] | None = None\n    outputModes: List[str] | None = None\n\n\nclass AgentCard(BaseModel):\n    name: str\n    description: str | None = None\n    url: str\n    provider: AgentProvider | None = None\n    version: str\n    documentationUrl: str | None = None\n    capabilities: AgentCapabilities\n    authentication: AgentAuthentication | None = None\n    defaultInputModes: List[str] = [\"text\"]\n    defaultOutputModes: List[str] = [\"text\"]\n    skills: List[AgentSkill]\n\n\nclass A2AClientError(Exception):\n    pass\n\n\nclass A2AClientHTTPError(A2AClientError):\n    def __init__(self, status_code: int, message: str):\n        self.status_code = status_code\n        self.message = message\n        super().__init__(f\"HTTP Error {status_code}: {message}\")\n\n\nclass A2AClientJSONError(A2AClientError):\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(f\"JSON Error: {message}\")\n\n\nclass MissingAPIKeyError(Exception):\n    \"\"\"Exception for missing API key.\"\"\"\n\n    pass",
      "summary": "Sure! Here\u2019s a concise summary addressing each of your points:\n\n---\n\n### 1. Primary Purpose\n\nThe code unit `types.py` defines core data structures, enums, error types, and schemas for a task-based agent-to-agent (A2A) communication system\u2014likely for use with messaging, tasks, artifacts, and notifications\u2014using [Pydantic](https://docs.pydantic.dev/) for data validation and serialization. Its goal is to provide strongly-typed, validated models for messages (especially in JSON-RPC format), tasks, artifacts, events, agent metadata, and errors exchanged within the system.\n\n---\n\n### 2. Parameters\n\nAs a definitions file, it primarily declares **Pydantic model classes** with typed fields (parameters). These parameters represent:\n\n- **Task, Message, Artifact, Push Notification, and Agent metadata** (e.g., `id`, `name`, `description`, `parts`, `metadata`, `status`, timing info, etc.).\n- **Messages and Events** (e.g., `TaskStatusUpdateEvent`, `TaskArtifactUpdateEvent`).\n- **RPC requests and responses**, e.g., `method`, `params`, `result`, `error`.\n- **File and data content**, e.g., bytes, URI, file name, mime type.\n- **Agent capability and authentication details**.\n- **Various error types** with codes and messages.\n\nEach model's parameters correspond to fields relevant to the entities they represent (for example, `Task` has `id`, `status`, `artifacts`, etc.).\n\n---\n\n### 3. Return Value\n\nThe code unit **does not define functions with return values** in the usual sense. Instead:\n\n- **Validation logic** is provided via Pydantic model validators (e.g., in `FileContent`), which raise exceptions on invalid or inconsistent data.\n- **RPC and data exchange is expected to be modeled using these classes**, so \"return values\" are more about the typed objects that could be serialized and deserialized when the models are used elsewhere.\n\n---\n\n### 4. Other Functions or Methods Called Internally\n\nInternally, the code unit uses:\n\n- **Pydantic model features**:\n  - `@model_validator(mode=\"after\")` for field validation (`FileContent.check_content`)\n  - `@field_serializer` for custom serialization (`TaskStatus.serialize_dt`)\n  - Pydantic `BaseModel`, `Field`, and advanced typing features (e.g., `Annotated`, discriminators)\n- **Utility functions**:\n  - `uuid4()` from `uuid` for generating unique IDs\n  - `datetime.now` for timestamp fields\n\nNo custom free functions are defined and called internally; rather, the file uses standard Pydantic hooks and Python's built-in/type utility functions as described above.\n\n---\n\n#### Summary Table\n\n| Purpose      | Parameters                      | Return Value                  | Internally Calls                   |\n|--------------|---------------------------------|-------------------------------|-------------------------------------|\n| Typed models for A2A data & messages | Model fields (task, message, etc.) | Validated/serialized model instances | Pydantic validation & serialization<br/>uuid4(), datetime.now |\n\n---\n\n**In short:**  \nThis file is the type/model backbone for a messaging/task-oriented agent API, using Pydantic for type safety, validation, and (de)serialization, and does not itself provide callable logic or functions beyond model validators and serializers."
    },
    "66": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/__init__.py",
      "code": "",
      "summary": "The provided code unit is /tmp/PocketFlow/cookbook/pocketflow-a2a/common/__init__.py, but you have not given any code from this file.\n\nSummary:\n\n1. Primary purpose:  \nCannot be determined because the code content is empty or missing.\n\n2. Parameters:  \nNone, as no code is provided.\n\n3. Return value:  \nNone, as no code is provided.\n\n4. Other functions or methods it calls:  \nNone, as no code is provided.\n\nIf you provide the actual code, I can give a detailed, accurate summary as requested."
    },
    "67": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/client/__init__.py",
      "code": "from .client import A2AClient\nfrom .card_resolver import A2ACardResolver\n\n__all__ = [\"A2AClient\", \"A2ACardResolver\"]\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-a2a/common/client/__init__.py`:**\n\n1. **Primary Purpose:**  \n   This code unit serves as an initializer for the `client` package. It exposes two classes, `A2AClient` and `A2ACardResolver`, making them available for import when the package is imported elsewhere.\n\n2. **Parameters:**  \n   There are no parameters defined in this code unit.\n\n3. **Return Value:**  \n   There is no explicit return value in this code unit.\n\n4. **Other Functions or Methods Called Internally:**  \n   Internally, the code performs import statements only:  \n   - Imports `A2AClient` from `.client`\n   - Imports `A2ACardResolver` from `.card_resolver`  \n   No functions or methods are called; only module attributes are defined.\n\nIn summary, this `__init__.py` file is for package organization and does not contain any function or method logic itself."
    },
    "68": {
      "unit_name": "card_resolver.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/client/card_resolver.py",
      "code": "import httpx\nfrom common.types import (\n    AgentCard,\n    A2AClientJSONError,\n)\nimport json\n\n\nclass A2ACardResolver:\n    def __init__(self, base_url, agent_card_path=\"/.well-known/agent.json\"):\n        self.base_url = base_url.rstrip(\"/\")\n        self.agent_card_path = agent_card_path.lstrip(\"/\")\n\n    def get_agent_card(self) -> AgentCard:\n        with httpx.Client() as client:\n            response = client.get(self.base_url + \"/\" + self.agent_card_path)\n            response.raise_for_status()\n            try:\n                return AgentCard(**response.json())\n            except json.JSONDecodeError as e:\n                raise A2AClientJSONError(str(e)) from e\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-a2a/common/client/card_resolver.py`:**\n\n1. **Primary Purpose:**  \n   The code defines a class (`A2ACardResolver`) that retrieves and parses an agent configuration (\"agent card\") in JSON format from a remote URL, using HTTP GET requests.\n\n2. **Parameters:**  \n   - `base_url` (constructor): The base URL of the remote server from which to fetch the agent card.\n   - `agent_card_path` (constructor, optional): The path (default `/.well-known/agent.json`) appended to `base_url` to construct the full URL to fetch the agent card.\n\n3. **Return Value:**  \n   - The `get_agent_card` method returns an instance of `AgentCard` initialized with the JSON data retrieved from the remote server.\n   - If JSON decoding fails, it raises an `A2AClientJSONError`.\n\n4. **Internally Called Functions/Methods:**\n   - `httpx.Client()`\n   - `client.get(...)`\n   - `response.raise_for_status()`\n   - `response.json()`\n   - `AgentCard(**...)` (constructor)\n   - `json.JSONDecodeError`\n   - `A2AClientJSONError(...)`"
    },
    "69": {
      "unit_name": "client.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/client/client.py",
      "code": "import httpx\nfrom httpx_sse import connect_sse\nfrom typing import Any, AsyncIterable\nfrom common.types import (\n    AgentCard,\n    GetTaskRequest,\n    SendTaskRequest,\n    SendTaskResponse,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    JSONRPCError,\n    GetTaskResponse,\n    CancelTaskResponse,\n    CancelTaskRequest,\n    SetTaskPushNotificationRequest,\n    SetTaskPushNotificationResponse,\n    GetTaskPushNotificationRequest,\n    GetTaskPushNotificationResponse,\n    A2AClientHTTPError,\n    A2AClientJSONError,\n    SendTaskStreamingRequest,\n    SendTaskStreamingResponse,\n    Task,\n    TaskPushNotificationConfig,\n    TaskStatusUpdateEvent,\n    TaskArtifactUpdateEvent,\n)\nimport json\nimport logging\n\n# Configure a logger specific to the client\nlogger = logging.getLogger(\"A2AClient\")\n\nclass A2AClientError(Exception):\n    \"\"\"Base class for A2A client errors\"\"\"\n    def __init__(self, message):\n        super().__init__(message)\n\nclass RpcError(Exception):\n    code: int\n    data: Any = None\n    def __init__(self, code: int, message: str, data: Any = None):\n        super().__init__(message)\n        self.name = \"RpcError\"\n        self.code = code\n        self.data = data\n\nclass A2AClient:\n    def __init__(self, agent_card: AgentCard = None, url: str = None):\n        if agent_card:\n            self.url = agent_card.url.rstrip(\"/\")\n        elif url:\n            self.url = url.rstrip(\"/\")\n        else:\n            raise ValueError(\"Must provide either agent_card or url\")\n        self.fetchImpl = httpx.AsyncClient(timeout=None)\n\n    def _generateRequestId(self):\n        import time\n        return int(time.time() * 1000)\n\n    async def _send_request(self, request: JSONRPCRequest) -> dict[str, Any]:\n        req_id = request.id\n        req_method = request.method\n        req_dump = request.model_dump(exclude_none=True)\n\n        logger.info(f\"-> Sending Request (ID: {req_id}, Method: {req_method}):\\n{json.dumps(req_dump, indent=2)}\")\n\n        try:\n            response = await self.fetchImpl.post(\n                self.url, json=req_dump, timeout=60.0\n            )\n            logger.info(f\"<- Received HTTP Status {response.status_code} for Request (ID: {req_id})\")\n            response_text = await response.aread()\n            logger.debug(f\"Raw Response Body (ID: {req_id}):\\n{response_text.decode('utf-8', errors='replace')}\")\n\n            response.raise_for_status()\n\n            try:\n                json_response = json.loads(response_text)\n            except json.JSONDecodeError as e:\n                logger.error(f\"Failed to decode JSON response (ID: {req_id}): {e}\")\n                raise A2AClientJSONError(f\"Failed to decode JSON: {e}\") from e\n\n            if \"error\" in json_response and json_response[\"error\"] is not None:\n                rpc_error = json_response[\"error\"]\n                logger.warning(f\"<- Received JSON-RPC Error (ID: {req_id}): Code={rpc_error.get('code')}, Msg='{rpc_error.get('message')}'\")\n                raise RpcError(rpc_error.get(\"code\", -32000), rpc_error.get(\"message\", \"Unknown RPC Error\"), rpc_error.get(\"data\"))\n\n            logger.info(f\"<- Received Success Response (ID: {req_id}):\\n{json.dumps(json_response, indent=2)}\")\n            return json_response\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP Error for Request (ID: {req_id}): {e.response.status_code} - {e.request.url}\")\n            error_body = await e.response.aread()\n            raise A2AClientHTTPError(e.response.status_code, f\"{e}. Body: {error_body.decode('utf-8', errors='replace')}\") from e\n        except httpx.RequestError as e:\n            logger.error(f\"Request Error for (ID: {req_id}): {e}\")\n            raise A2AClientError(f\"Network or request error: {e}\") from e\n        except RpcError:\n             raise\n        except Exception as e:\n             logger.error(f\"Unexpected error during request (ID: {req_id}): {e}\", exc_info=True)\n             raise A2AClientError(f\"Unexpected error: {e}\") from e\n\n    async def send_task(self, payload: dict[str, Any]) -> SendTaskResponse:\n        request = SendTaskRequest(params=payload)\n        response_dict = await self._send_request(request)\n        return SendTaskResponse(**response_dict)\n\n    async def send_task_streaming(\n        self, payload: dict[str, Any]\n    ) -> AsyncIterable[SendTaskStreamingResponse]:\n        request = SendTaskStreamingRequest(params=payload)\n        req_id = request.id\n        req_dump = request.model_dump(exclude_none=True)\n\n        logger.info(f\"-> Sending Streaming Request (ID: {req_id}, Method: {request.method}):\\n{json.dumps(req_dump, indent=2)}\")\n\n        try:\n            async with self.fetchImpl.stream(\"POST\", self.url, json=req_dump, timeout=None) as response:\n                 logger.info(f\"<- Received HTTP Status {response.status_code} for Streaming Request (ID: {req_id})\")\n                 response.raise_for_status()\n\n                 buffer = \"\"\n                 async for line in response.aiter_lines():\n                     if not line:\n                         if buffer.startswith(\"data:\"):\n                             data_str = buffer[len(\"data:\"):].strip()\n                             logger.debug(f\"Received SSE Data Line (ID: {req_id}): {data_str}\")\n                             try:\n                                 sse_data_dict = json.loads(data_str)\n                                 yield SendTaskStreamingResponse(**sse_data_dict)\n                             except json.JSONDecodeError as e:\n                                 logger.error(f\"Failed to decode SSE JSON (ID: {req_id}): {e}. Data: '{data_str}'\")\n                             except Exception as e:\n                                 logger.error(f\"Error processing SSE data (ID: {req_id}): {e}. Data: '{data_str}'\", exc_info=True)\n                         elif buffer:\n                             logger.debug(f\"Received non-data SSE line (ID: {req_id}): {buffer}\")\n                         buffer = \"\"\n                     else:\n                         buffer += line + \"\\n\"\n\n                 if buffer:\n                     logger.warning(f\"SSE stream ended with partial data in buffer (ID: {req_id}): {buffer}\")\n\n                 logger.info(f\"SSE Stream ended for request ID: {req_id}\")\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP Error during streaming connection (ID: {req_id}): {e.response.status_code} - {e.request.url}\")\n            error_body = await e.response.aread()\n            raise A2AClientHTTPError(e.response.status_code, f\"{e}. Body: {error_body.decode('utf-8', errors='replace')}\") from e\n        except httpx.RequestError as e:\n             logger.error(f\"Request Error during streaming (ID: {req_id}): {e}\")\n             raise A2AClientError(f\"Network or request error during streaming: {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error during streaming (ID: {req_id}): {e}\", exc_info=True)\n            raise A2AClientError(f\"Unexpected streaming error: {e}\") from e\n\n    async def get_task(self, payload: dict[str, Any]) -> GetTaskResponse:\n        request = GetTaskRequest(params=payload)\n        response_dict = await self._send_request(request)\n        return GetTaskResponse(**response_dict)\n\n    async def cancel_task(self, payload: dict[str, Any]) -> CancelTaskResponse:\n        request = CancelTaskRequest(params=payload)\n        response_dict = await self._send_request(request)\n        return CancelTaskResponse(**response_dict)\n\n    async def set_task_callback(\n        self, payload: dict[str, Any]\n    ) -> SetTaskPushNotificationResponse:\n        request = SetTaskPushNotificationRequest(params=payload)\n        response_dict = await self._send_request(request)\n        return SetTaskPushNotificationResponse(**response_dict)\n\n    async def get_task_callback(\n        self, payload: dict[str, Any]\n    ) -> GetTaskPushNotificationResponse:\n        request = GetTaskPushNotificationRequest(params=payload)\n        response_dict = await self._send_request(request)\n        return GetTaskPushNotificationResponse(**response_dict)\n",
      "summary": "**Summary of `client.py`:**\n\n1. **Primary Purpose**\n   \n   This code defines an asynchronous HTTP client (`A2AClient`) for interacting with a server that supports JSON-RPC and server-sent events (SSE). It is designed to submit tasks, stream results, retrieve results, cancel tasks, and manage task notifications for an agent, typically in the context of an agent-to-agent architecture.\n\n2. **Parameters**\n   \n   - **A2AClient constructor** takes either:\n     - `agent_card` (an object containing agent metadata, especially a URL); or\n     - `url` (direct string for server endpoint).\n   - Most public methods (e.g., `send_task`, `get_task`, `cancel_task`, etc.) take a single parameter:\n     - `payload` (a dictionary containing request parameters for the specific RPC call).\n\n3. **Return Values**\n   \n   - Methods like `send_task`, `get_task`, `cancel_task`, `set_task_callback`, and `get_task_callback` return typed response objects (e.g., `SendTaskResponse`, `GetTaskResponse`, etc.) containing results from the server.\n   - `send_task_streaming` is an async generator yielding `SendTaskStreamingResponse` objects as they arrive via SSE.\n\n4. **Internal Functions or Methods Called**\n   \n   - **Within the class:**  \n     - `_send_request`: Used by most public methods to send the appropriate RPC request and handle responses/errors.\n     - `_generateRequestId`: Generates unique request IDs (used in JSON-RPC requests).\n   - **Standard / third-party libraries:**  \n     - `httpx.AsyncClient` methods for HTTP requests (`post`, `stream`)\n     - JSON manipulation: `json.dumps`, `json.loads`, `json.JSONDecodeError`\n     - Logging: `logger.info`, `logger.error`, `logger.warning`, `logger.debug`\n     - Exception classes: `httpx.HTTPStatusError`, `httpx.RequestError`\n   - **Other types:**  \n     - Several request and response types from `common.types` (e.g., `SendTaskRequest`, `GetTaskResponse`, etc.).\n     - Error types from `common.types` (e.g., `A2AClientHTTPError`, `A2AClientJSONError`).\n\n---\n\nIn summary, this code provides an async Python client for a JSON-RPC (and SSE) service, handling typical agent operations (send, stream, get, cancel, callback), with structured request/response types, robust error handling, and detailed logging. Public API methods expect input payloads as dictionaries and return structured response objects or async streams."
    },
    "70": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/server/__init__.py",
      "code": "from .server import A2AServer\nfrom .task_manager import TaskManager, InMemoryTaskManager\n\n__all__ = [\"A2AServer\", \"TaskManager\", \"InMemoryTaskManager\"]\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-a2a/common/server/__init__.py`:**\n\n1. **Primary Purpose:**  \n   This code unit serves as the package initializer for the `server` module. It exposes key classes\u2014`A2AServer`, `TaskManager`, and `InMemoryTaskManager`\u2014from submodules to make them directly accessible when importing from `server`.\n\n2. **Parameters:**  \n   This code unit does not define any functions or classes itself, so it takes no parameters.\n\n3. **Return Value:**  \n   No return value; its purpose is to manage imports and module namespace.\n\n4. **Functions or Methods Called Internally:**  \n   The unit does not call any functions or methods directly. It only re-exports objects from the `.server` and `.task_manager` submodules.\n\n**Summary:**  \nThis file is a package-level initializer that re-exports selected classes (`A2AServer`, `TaskManager`, and `InMemoryTaskManager`) from its submodules. It organizes the namespace for easier import, does not accept parameters or return anything, and does not internally invoke any functions or methods."
    },
    "71": {
      "unit_name": "task_manager.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/server/task_manager.py",
      "code": "from abc import ABC, abstractmethod\nfrom typing import Union, AsyncIterable, List\nfrom common.types import Task\nfrom common.types import (\n    JSONRPCResponse,\n    TaskIdParams,\n    TaskQueryParams,\n    GetTaskRequest,\n    TaskNotFoundError,\n    SendTaskRequest,\n    CancelTaskRequest,\n    TaskNotCancelableError,\n    SetTaskPushNotificationRequest,\n    GetTaskPushNotificationRequest,\n    GetTaskResponse,\n    CancelTaskResponse,\n    SendTaskResponse,\n    SetTaskPushNotificationResponse,\n    GetTaskPushNotificationResponse,\n    PushNotificationNotSupportedError,\n    TaskSendParams,\n    TaskStatus,\n    TaskState,\n    TaskResubscriptionRequest,\n    SendTaskStreamingRequest,\n    SendTaskStreamingResponse,\n    Artifact,\n    PushNotificationConfig,\n    TaskStatusUpdateEvent,\n    JSONRPCError,\n    TaskPushNotificationConfig,\n    InternalError,\n)\nfrom common.server.utils import new_not_implemented_error\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TaskManager(ABC):\n    @abstractmethod\n    async def on_get_task(self, request: GetTaskRequest) -> GetTaskResponse:\n        pass\n\n    @abstractmethod\n    async def on_cancel_task(self, request: CancelTaskRequest) -> CancelTaskResponse:\n        pass\n\n    @abstractmethod\n    async def on_send_task(self, request: SendTaskRequest) -> SendTaskResponse:\n        pass\n\n    @abstractmethod\n    async def on_send_task_subscribe(\n        self, request: SendTaskStreamingRequest\n    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:\n        pass\n\n    @abstractmethod\n    async def on_set_task_push_notification(\n        self, request: SetTaskPushNotificationRequest\n    ) -> SetTaskPushNotificationResponse:\n        pass\n\n    @abstractmethod\n    async def on_get_task_push_notification(\n        self, request: GetTaskPushNotificationRequest\n    ) -> GetTaskPushNotificationResponse:\n        pass\n\n    @abstractmethod\n    async def on_resubscribe_to_task(\n        self, request: TaskResubscriptionRequest\n    ) -> Union[AsyncIterable[SendTaskResponse], JSONRPCResponse]:\n        pass\n\n\nclass InMemoryTaskManager(TaskManager):\n    def __init__(self):\n        self.tasks: dict[str, Task] = {}\n        self.push_notification_infos: dict[str, PushNotificationConfig] = {}\n        self.lock = asyncio.Lock()\n        self.task_sse_subscribers: dict[str, List[asyncio.Queue]] = {}\n        self.subscriber_lock = asyncio.Lock()\n\n    async def on_get_task(self, request: GetTaskRequest) -> GetTaskResponse:\n        logger.info(f\"Getting task {request.params.id}\")\n        task_query_params: TaskQueryParams = request.params\n\n        async with self.lock:\n            task = self.tasks.get(task_query_params.id)\n            if task is None:\n                return GetTaskResponse(id=request.id, error=TaskNotFoundError())\n\n            task_result = self.append_task_history(\n                task, task_query_params.historyLength\n            )\n\n        return GetTaskResponse(id=request.id, result=task_result)\n\n    async def on_cancel_task(self, request: CancelTaskRequest) -> CancelTaskResponse:\n        logger.info(f\"Cancelling task {request.params.id}\")\n        task_id_params: TaskIdParams = request.params\n\n        async with self.lock:\n            task = self.tasks.get(task_id_params.id)\n            if task is None:\n                return CancelTaskResponse(id=request.id, error=TaskNotFoundError())\n\n        return CancelTaskResponse(id=request.id, error=TaskNotCancelableError())\n\n    @abstractmethod\n    async def on_send_task(self, request: SendTaskRequest) -> SendTaskResponse:\n        pass\n\n    @abstractmethod\n    async def on_send_task_subscribe(\n        self, request: SendTaskStreamingRequest\n    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:\n        pass\n\n    async def set_push_notification_info(self, task_id: str, notification_config: PushNotificationConfig):\n        async with self.lock:\n            task = self.tasks.get(task_id)\n            if task is None:\n                raise ValueError(f\"Task not found for {task_id}\")\n\n            self.push_notification_infos[task_id] = notification_config\n\n        return\n    \n    async def get_push_notification_info(self, task_id: str) -> PushNotificationConfig:\n        async with self.lock:\n            task = self.tasks.get(task_id)\n            if task is None:\n                raise ValueError(f\"Task not found for {task_id}\")\n\n            return self.push_notification_infos[task_id]\n            \n        return\n    \n    async def has_push_notification_info(self, task_id: str) -> bool:\n        async with self.lock:\n            return task_id in self.push_notification_infos\n            \n\n    async def on_set_task_push_notification(\n        self, request: SetTaskPushNotificationRequest\n    ) -> SetTaskPushNotificationResponse:\n        logger.info(f\"Setting task push notification {request.params.id}\")\n        task_notification_params: TaskPushNotificationConfig = request.params\n\n        try:\n            await self.set_push_notification_info(task_notification_params.id, task_notification_params.pushNotificationConfig)\n        except Exception as e:\n            logger.error(f\"Error while setting push notification info: {e}\")\n            return JSONRPCResponse(\n                id=request.id,\n                error=InternalError(\n                    message=\"An error occurred while setting push notification info\"\n                ),\n            )\n            \n        return SetTaskPushNotificationResponse(id=request.id, result=task_notification_params)\n\n    async def on_get_task_push_notification(\n        self, request: GetTaskPushNotificationRequest\n    ) -> GetTaskPushNotificationResponse:\n        logger.info(f\"Getting task push notification {request.params.id}\")\n        task_params: TaskIdParams = request.params\n\n        try:\n            notification_info = await self.get_push_notification_info(task_params.id)\n        except Exception as e:\n            logger.error(f\"Error while getting push notification info: {e}\")\n            return GetTaskPushNotificationResponse(\n                id=request.id,\n                error=InternalError(\n                    message=\"An error occurred while getting push notification info\"\n                ),\n            )\n        \n        return GetTaskPushNotificationResponse(id=request.id, result=TaskPushNotificationConfig(id=task_params.id, pushNotificationConfig=notification_info))\n\n    async def upsert_task(self, task_send_params: TaskSendParams) -> Task:\n        logger.info(f\"Upserting task {task_send_params.id}\")\n        async with self.lock:\n            task = self.tasks.get(task_send_params.id)\n            if task is None:\n                task = Task(\n                    id=task_send_params.id,\n                    sessionId = task_send_params.sessionId,\n                    messages=[task_send_params.message],\n                    status=TaskStatus(state=TaskState.SUBMITTED),\n                    history=[task_send_params.message],\n                )\n                self.tasks[task_send_params.id] = task\n            else:\n                task.history.append(task_send_params.message)\n\n            return task\n\n    async def on_resubscribe_to_task(\n        self, request: TaskResubscriptionRequest\n    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:\n        return new_not_implemented_error(request.id)\n\n    async def update_store(\n        self, task_id: str, status: TaskStatus, artifacts: list[Artifact]\n    ) -> Task:\n        async with self.lock:\n            try:\n                task = self.tasks[task_id]\n            except KeyError:\n                logger.error(f\"Task {task_id} not found for updating the task\")\n                raise ValueError(f\"Task {task_id} not found\")\n\n            task.status = status\n\n            if status.message is not None:\n                task.history.append(status.message)\n\n            if artifacts is not None:\n                if task.artifacts is None:\n                    task.artifacts = []\n                task.artifacts.extend(artifacts)\n\n            return task\n\n    def append_task_history(self, task: Task, historyLength: int | None):\n        new_task = task.model_copy()\n        if historyLength is not None and historyLength > 0:\n            new_task.history = new_task.history[-historyLength:]\n        else:\n            new_task.history = []\n\n        return new_task        \n\n    async def setup_sse_consumer(self, task_id: str, is_resubscribe: bool = False):\n        async with self.subscriber_lock:\n            if task_id not in self.task_sse_subscribers:\n                if is_resubscribe:\n                    raise ValueError(\"Task not found for resubscription\")\n                else:\n                    self.task_sse_subscribers[task_id] = []\n\n            sse_event_queue = asyncio.Queue(maxsize=0) # <=0 is unlimited\n            self.task_sse_subscribers[task_id].append(sse_event_queue)\n            return sse_event_queue\n\n    async def enqueue_events_for_sse(self, task_id, task_update_event):\n        async with self.subscriber_lock:\n            if task_id not in self.task_sse_subscribers:\n                return\n\n            current_subscribers = self.task_sse_subscribers[task_id]\n            for subscriber in current_subscribers:\n                await subscriber.put(task_update_event)\n\n    async def dequeue_events_for_sse(\n        self, request_id, task_id, sse_event_queue: asyncio.Queue\n    ) -> AsyncIterable[SendTaskStreamingResponse] | JSONRPCResponse:\n        try:\n            while True:                \n                event = await sse_event_queue.get()\n                if isinstance(event, JSONRPCError):\n                    yield SendTaskStreamingResponse(id=request_id, error=event)\n                    break\n                                                \n                yield SendTaskStreamingResponse(id=request_id, result=event)\n                if isinstance(event, TaskStatusUpdateEvent) and event.final:\n                    break\n        finally:\n            async with self.subscriber_lock:\n                if task_id in self.task_sse_subscribers:\n                    self.task_sse_subscribers[task_id].remove(sse_event_queue)\n\n",
      "summary": "Certainly! Here\u2019s a concise, natural language summary of the code unit **task_manager.py** from `/tmp/PocketFlow/cookbook/pocketflow-a2a/common/server/`:\n\n---\n\n### 1. Primary Purpose\n\nThe code defines an abstract interface (**TaskManager**) and an in-memory implementation (**InMemoryTaskManager**) for managing tasks in a server environment. This includes CRUD operations for tasks, handling push notification configurations, and managing Server-Sent Events (SSE) subscriptions for real-time task updates. The services are designed for asynchronous operation, suitable for modern async servers.\n\n---\n\n### 2. Brief Description of Parameters\n\n- **General**: Most methods take request objects (e.g., `GetTaskRequest`, `SetTaskPushNotificationRequest`) containing relevant IDs and parameters for the operation, such as task IDs or configuration data.\n- **Task Management**: Task-related methods use IDs (`TaskIdParams`), query parameters (`TaskQueryParams`), or task details (`TaskSendParams`).\n- **Push Notification**: Methods take a task ID and push notification configuration or use composite objects like `TaskPushNotificationConfig`.\n- **SSE (Streaming)**: SSE methods use task IDs and asyncio Queues for event delivery.\n\n---\n\n### 3. Brief Description of Return Value\n\n- Most methods return response objects corresponding to the operation, such as:\n    - `GetTaskResponse`, `CancelTaskResponse`, `SetTaskPushNotificationResponse`, etc. containing either the result or a descriptive error.\n- Methods related to subscriptions return asynchronous iterables of streaming responses, or a standard JSONRPC error.\n- Some internal helpers return domain objects (e.g., a `Task`), configuration info, or booleans (for configuration existence checks).\n\n---\n\n### 4. Other Functions or Methods Called Internally\n\nKey internal functions and external functions used:\n- **Domain/Helper Methods:**\n    - `self.append_task_history`\n    - `self.set_push_notification_info`\n    - `self.get_push_notification_info`\n    - `self.has_push_notification_info`\n    - `self.upsert_task`\n    - `self.update_store`\n    - `self.setup_sse_consumer`\n    - `self.enqueue_events_for_sse`\n    - `self.dequeue_events_for_sse`\n- **Error/utils from imports:**\n    - `new_not_implemented_error` (from `common.server.utils`)\n    - Response/error classes like `TaskNotFoundError`, `TaskNotCancelableError`, `InternalError`, `JSONRPCResponse`, `JSONRPCError`\n- **Standard Library:**\n    - `asyncio.Lock`, `asyncio.Queue`\n    - `logging`\n\n---\n\n**Summary**:  \nThis code unit provides an async interface and a basic in-memory implementation for managing tasks and their associated notifications in a server setting, including live updates via subscriptions, with thread-safe access and error handling. All core domain logic (CRUD, notifications, streaming) is encapsulated in the class, using request/response objects and asyncio primitives."
    },
    "72": {
      "unit_name": "server.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/server/server.py",
      "code": "from starlette.applications import Starlette\nfrom starlette.responses import JSONResponse\nfrom sse_starlette.sse import EventSourceResponse\nfrom starlette.requests import Request\nfrom common.types import (\n    A2ARequest,\n    JSONRPCResponse,\n    InvalidRequestError,\n    JSONParseError,\n    GetTaskRequest,\n    CancelTaskRequest,\n    SendTaskRequest,\n    SetTaskPushNotificationRequest,\n    GetTaskPushNotificationRequest,\n    InternalError,\n    AgentCard,\n    TaskResubscriptionRequest,\n    SendTaskStreamingRequest,\n    Message,\n)\nfrom pydantic import ValidationError\nimport json\nfrom typing import AsyncIterable, Any\nfrom common.server.task_manager import TaskManager\n\nimport logging\n\n# Configure a logger specific to the server\nlogger = logging.getLogger(\"A2AServer\")\n\n\nclass A2AServer:\n    def __init__(\n        self,\n        host=\"0.0.0.0\",\n        port=5000,\n        endpoint=\"/\",\n        agent_card: AgentCard = None,\n        task_manager: TaskManager = None,\n    ):\n        self.host = host\n        self.port = port\n        self.endpoint = endpoint\n        self.task_manager = task_manager\n        self.agent_card = agent_card\n        self.app = Starlette()\n        self.app.add_route(self.endpoint, self._process_request, methods=[\"POST\"])\n        self.app.add_route(\n            \"/.well-known/agent.json\", self._get_agent_card, methods=[\"GET\"]\n        )\n\n    def start(self):\n        if self.agent_card is None:\n            raise ValueError(\"agent_card is not defined\")\n\n        if self.task_manager is None:\n            raise ValueError(\"request_handler is not defined\")\n\n        import uvicorn\n\n        # Basic logging config moved to __main__.py for application-level control\n        uvicorn.run(self.app, host=self.host, port=self.port)\n\n    def _get_agent_card(self, request: Request) -> JSONResponse:\n        logger.info(\"Serving Agent Card request\")\n        return JSONResponse(self.agent_card.model_dump(exclude_none=True))\n\n    async def _process_request(self, request: Request):\n        request_id_for_log = \"N/A\"  # Default if parsing fails early\n        raw_body = b\"\"\n        try:\n            # Log raw body first\n            raw_body = await request.body()\n            body = json.loads(raw_body)  # Attempt parsing\n            request_id_for_log = body.get(\"id\", \"N/A\")  # Get ID if possible\n            logger.info(f\"<- Received Request (ID: {request_id_for_log}):\\n{json.dumps(body, indent=2)}\")\n\n            json_rpc_request = A2ARequest.validate_python(body)\n\n            # Route based on method (same as before)\n            if isinstance(json_rpc_request, GetTaskRequest):\n                result = await self.task_manager.on_get_task(json_rpc_request)\n            elif isinstance(json_rpc_request, SendTaskRequest):\n                result = await self.task_manager.on_send_task(json_rpc_request)\n            elif isinstance(json_rpc_request, SendTaskStreamingRequest):\n                result = await self.task_manager.on_send_task_subscribe(\n                    json_rpc_request\n                )\n            elif isinstance(json_rpc_request, CancelTaskRequest):\n                result = await self.task_manager.on_cancel_task(json_rpc_request)\n            elif isinstance(json_rpc_request, SetTaskPushNotificationRequest):\n                result = await self.task_manager.on_set_task_push_notification(json_rpc_request)\n            elif isinstance(json_rpc_request, GetTaskPushNotificationRequest):\n                result = await self.task_manager.on_get_task_push_notification(json_rpc_request)\n            elif isinstance(json_rpc_request, TaskResubscriptionRequest):\n                result = await self.task_manager.on_resubscribe_to_task(\n                    json_rpc_request\n                )\n            else:\n                logger.warning(f\"Unexpected request type: {type(json_rpc_request)}\")\n                raise ValueError(f\"Unexpected request type: {type(request)}\")\n\n            return self._create_response(result)  # Pass result to response creation\n\n        except json.decoder.JSONDecodeError as e:\n            logger.error(f\"JSON Parse Error for Request body: <<<{raw_body.decode('utf-8', errors='replace')}>>>\\nError: {e}\")\n            return self._handle_exception(e, request_id_for_log)  # Pass ID if known\n        except ValidationError as e:\n             logger.error(f\"Request Validation Error (ID: {request_id_for_log}): {e.json()}\")\n             return self._handle_exception(e, request_id_for_log)\n        except Exception as e:\n             logger.error(f\"Unhandled Exception processing request (ID: {request_id_for_log}): {e}\", exc_info=True)\n             return self._handle_exception(e, request_id_for_log)  # Pass ID if known\n\n    def _handle_exception(self, e: Exception, req_id=None) -> JSONResponse:  # Accept req_id\n        if isinstance(e, json.decoder.JSONDecodeError):\n            json_rpc_error = JSONParseError()\n        elif isinstance(e, ValidationError):\n            json_rpc_error = InvalidRequestError(data=json.loads(e.json()))\n        else:\n            # Log the full exception details\n            logger.error(f\"Internal Server Error (ReqID: {req_id}): {e}\", exc_info=True)\n            json_rpc_error = InternalError(message=f\"Internal Server Error: {type(e).__name__}\")\n\n        response = JSONRPCResponse(id=req_id, error=json_rpc_error)\n        response_dump = response.model_dump(exclude_none=True)\n        logger.info(f\"-> Sending Error Response (ReqID: {req_id}):\\n{json.dumps(response_dump, indent=2)}\")\n        # A2A errors are still sent with HTTP 200\n        return JSONResponse(response_dump, status_code=200)\n\n    def _create_response(self, result: Any) -> JSONResponse | EventSourceResponse:\n        if isinstance(result, AsyncIterable):\n            # Streaming response\n            async def event_generator(result_stream) -> AsyncIterable[dict[str, str]]:\n                stream_request_id = None  # Capture ID from the first event if possible\n                try:\n                    async for item in result_stream:\n                        # Log each streamed item\n                        response_json = item.model_dump_json(exclude_none=True)\n                        stream_request_id = item.id  # Update ID\n                        logger.info(f\"-> Sending SSE Event (ID: {stream_request_id}):\\n{json.dumps(json.loads(response_json), indent=2)}\")\n                        yield {\"data\": response_json}\n                    logger.info(f\"SSE Stream ended for request ID: {stream_request_id}\")\n                except Exception as e:\n                    logger.error(f\"Error during SSE generation (ReqID: {stream_request_id}): {e}\", exc_info=True)\n                    # Optionally yield an error event if the protocol allows/requires it\n                    # error_payload = JSONRPCResponse(id=stream_request_id, error=InternalError(message=f\"SSE Error: {e}\"))\n                    # yield {\"data\": error_payload.model_dump_json(exclude_none=True)}\n\n            logger.info(\"Starting SSE stream...\")  # Log stream start\n            return EventSourceResponse(event_generator(result))\n        elif isinstance(result, JSONRPCResponse):\n            # Standard JSON response\n            response_dump = result.model_dump(exclude_none=True)\n            log_id = result.id if result.id is not None else \"N/A (Notification?)\"\n            log_prefix = \"->\"\n            log_type = \"Response\"\n            if result.error:\n                 log_prefix = \"-> Sending Error\"\n                 log_type = \"Error Response\"\n\n            logger.info(f\"{log_prefix} {log_type} (ID: {log_id}):\\n{json.dumps(response_dump, indent=2)}\")\n            return JSONResponse(response_dump)\n        else:\n            # This should ideally not happen if task manager returns correctly\n            logger.error(f\"Task manager returned unexpected type: {type(result)}\")\n            err_resp = JSONRPCResponse(id=None, error=InternalError(message=\"Invalid internal response type\"))\n            return JSONResponse(err_resp.model_dump(exclude_none=True), status_code=500)\n",
      "summary": "**Summary of `server.py` (A2AServer) in `/tmp/PocketFlow/cookbook/pocketflow-a2a/common/server/server.py`:**\n\n---\n\n### 1. Primary Purpose\n\nThe primary purpose of this code unit is to implement an asynchronous JSON-RPC server for an \"Agent-to-Agent\" (A2A) system using Starlette and SSE (Server-Sent Events). The `A2AServer` class handles standardized agent requests and responses, manages task-related operations via a `TaskManager`, provides detailed logging, and can stream responses to clients when needed.\n\n---\n\n### 2. Brief Description of Parameters\n\n- `host` (str, default `\"0.0.0.0\"`): IP address the server will bind to.\n- `port` (int, default `5000`): Port number the server will listen on.\n- `endpoint` (str, default `\"/\"`): HTTP endpoint for receiving JSON-RPC requests.\n- `agent_card` (AgentCard): Metadata describing the agent, used to serve the agent's capabilities.\n- `task_manager` (TaskManager): An object responsible for processing various task-related requests.\n\nThese parameters configure networking, routing, and core agent behaviors.\n\n---\n\n### 3. Brief Description of Return Value\n\n- The main return value is a Starlette web server application. \n- Server endpoint handlers (`_process_request`, `_get_agent_card`) return HTTP JSON responses (`JSONResponse`) or streaming SSE responses (`EventSourceResponse`) depending on the type of request and the task manager result.\n- For JSON-RPC errors, a JSON object with the error details is returned (via `JSONRPCResponse`).\n\n---\n\n### 4. Internal Function/Method Calls\n\nThe code makes the following notable internal function and method calls:\n\n- **TaskManager methods** (for business logic):\n  - `on_get_task`\n  - `on_send_task`\n  - `on_send_task_subscribe`\n  - `on_cancel_task`\n  - `on_set_task_push_notification`\n  - `on_get_task_push_notification`\n  - `on_resubscribe_to_task`\n- **A2AServer methods**:\n  - `start`: To launch the server using Uvicorn.\n  - `_process_request`: Central handler for all POST requests, parsing, validating, dispatching, and error handling.\n  - `_get_agent_card`: Handles serving the agent metadata.\n  - `_create_response`: Formats responses as standard JSON or as a live SSE stream.\n  - `_handle_exception`: Generates formatted error responses depending on the failure mode.\n- **Model/validation/static calls**:\n  - `A2ARequest.validate_python`: Validates and parses incoming JSON-RPC requests.\n  - Pydantic validation/error reporting utilities.\n  - Logging methods for detailed event tracking.\n\n---\n\n**In Short:**  \nThis code builds a robust, async, JSON-RPC web server for agent communication in an agent-to-agent system, driven by a manager object, and capable of advanced streaming and standardized error reporting. It routes task-related JSON-RPC requests, serializes/deserializes using Pydantic, and handles all error and logging within Starlette's HTTP interface."
    },
    "73": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/server/utils.py",
      "code": "from common.types import (\n    JSONRPCResponse,\n    ContentTypeNotSupportedError,\n    UnsupportedOperationError,\n)\nfrom typing import List\n\n\ndef are_modalities_compatible(\n    server_output_modes: List[str], client_output_modes: List[str]\n):\n    \"\"\"Modalities are compatible if they are both non-empty\n    and there is at least one common element.\"\"\"\n    if client_output_modes is None or len(client_output_modes) == 0:\n        return True\n\n    if server_output_modes is None or len(server_output_modes) == 0:\n        return True\n\n    return any(x in server_output_modes for x in client_output_modes)\n\n\ndef new_incompatible_types_error(request_id):\n    return JSONRPCResponse(id=request_id, error=ContentTypeNotSupportedError())\n\n\ndef new_not_implemented_error(request_id):\n    return JSONRPCResponse(id=request_id, error=UnsupportedOperationError())\n",
      "summary": "**Summary of code unit `utils.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides utility functions to check the compatibility of output modalities between a server and client, and to generate standardized JSON-RPC error responses for unsupported content types or operations.\n\n2. **Parameters:**\n   - `are_modalities_compatible(server_output_modes: List[str], client_output_modes: List[str])`:  \n     - `server_output_modes`: List of modality strings supported by the server.\n     - `client_output_modes`: List of modality strings requested by the client.\n   - `new_incompatible_types_error(request_id)`:  \n     - `request_id`: The JSON-RPC request ID for which the error response is being generated.\n   - `new_not_implemented_error(request_id)`:  \n     - `request_id`: The JSON-RPC request ID for which the error response is being generated.\n\n3. **Return Value:**\n   - `are_modalities_compatible(...)`: Returns `True` if either list is empty or if they share at least one element; otherwise returns `False`.\n   - `new_incompatible_types_error(...)`: Returns a `JSONRPCResponse` object containing a `ContentTypeNotSupportedError`.\n   - `new_not_implemented_error(...)`: Returns a `JSONRPCResponse` object containing an `UnsupportedOperationError`.\n\n4. **Other Functions or Methods Called Internally:**\n   - `JSONRPCResponse` (constructor)\n   - `ContentTypeNotSupportedError` (constructor)\n   - `UnsupportedOperationError` (constructor)\n   - Standard Python built-ins: `any`, `len`, list operations"
    },
    "74": {
      "unit_name": "push_notification_auth.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/utils/push_notification_auth.py",
      "code": "from jwcrypto import jwk\nimport uuid\nfrom starlette.responses import JSONResponse\nfrom starlette.requests import Request\nfrom typing import Any\n\nimport jwt\nimport time\nimport json\nimport hashlib\nimport httpx\nimport logging\n\nfrom jwt import PyJWK, PyJWKClient\n\nlogger = logging.getLogger(__name__)\nAUTH_HEADER_PREFIX = 'Bearer '\n\nclass PushNotificationAuth:\n    def _calculate_request_body_sha256(self, data: dict[str, Any]):\n        \"\"\"Calculates the SHA256 hash of a request body.\n\n        This logic needs to be same for both the agent who signs the payload and the client verifier.\n        \"\"\"\n        body_str = json.dumps(\n            data,\n            ensure_ascii=False,\n            allow_nan=False,\n            indent=None,\n            separators=(\",\", \":\"),\n        )\n        return hashlib.sha256(body_str.encode()).hexdigest()\n\nclass PushNotificationSenderAuth(PushNotificationAuth):\n    def __init__(self):\n        self.public_keys = []\n        self.private_key_jwk: PyJWK = None\n\n    @staticmethod\n    async def verify_push_notification_url(url: str) -> bool:\n        async with httpx.AsyncClient(timeout=10) as client:\n            try:\n                validation_token = str(uuid.uuid4())\n                response = await client.get(\n                    url,\n                    params={\"validationToken\": validation_token}\n                )\n                response.raise_for_status()\n                is_verified = response.text == validation_token\n\n                logger.info(f\"Verified push-notification URL: {url} => {is_verified}\")            \n                return is_verified                \n            except Exception as e:\n                logger.warning(f\"Error during sending push-notification for URL {url}: {e}\")\n\n        return False\n\n    def generate_jwk(self):\n        key = jwk.JWK.generate(kty='RSA', size=2048, kid=str(uuid.uuid4()), use=\"sig\")\n        self.public_keys.append(key.export_public(as_dict=True))\n        self.private_key_jwk = PyJWK.from_json(key.export_private())\n    \n    def handle_jwks_endpoint(self, _request: Request):\n        \"\"\"Allow clients to fetch public keys.\n        \"\"\"\n        return JSONResponse({\n            \"keys\": self.public_keys\n        })\n    \n    def _generate_jwt(self, data: dict[str, Any]):\n        \"\"\"JWT is generated by signing both the request payload SHA digest and time of token generation.\n\n        Payload is signed with private key and it ensures the integrity of payload for client.\n        Including iat prevents from replay attack.\n        \"\"\"\n        \n        iat = int(time.time())\n\n        return jwt.encode(\n            {\"iat\": iat, \"request_body_sha256\": self._calculate_request_body_sha256(data)},\n            key=self.private_key_jwk,\n            headers={\"kid\": self.private_key_jwk.key_id},\n            algorithm=\"RS256\"\n        )\n\n    async def send_push_notification(self, url: str, data: dict[str, Any]):\n        jwt_token = self._generate_jwt(data)\n        headers = {'Authorization': f\"Bearer {jwt_token}\"}\n        async with httpx.AsyncClient(timeout=10) as client: \n            try:\n                response = await client.post(\n                    url,\n                    json=data,\n                    headers=headers\n                )\n                response.raise_for_status()\n                logger.info(f\"Push-notification sent for URL: {url}\")                            \n            except Exception as e:\n                logger.warning(f\"Error during sending push-notification for URL {url}: {e}\")\n\nclass PushNotificationReceiverAuth(PushNotificationAuth):\n    def __init__(self):\n        self.public_keys_jwks = []\n        self.jwks_client = None\n\n    async def load_jwks(self, jwks_url: str):\n        self.jwks_client = PyJWKClient(jwks_url)\n    \n    async def verify_push_notification(self, request: Request) -> bool:\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(AUTH_HEADER_PREFIX):\n            print(\"Invalid authorization header\")\n            return False\n        \n        token = auth_header[len(AUTH_HEADER_PREFIX):]\n        signing_key = self.jwks_client.get_signing_key_from_jwt(token)\n\n        decode_token = jwt.decode(\n            token,\n            signing_key,\n            options={\"require\": [\"iat\", \"request_body_sha256\"]},\n            algorithms=[\"RS256\"],\n        )\n\n        actual_body_sha256 = self._calculate_request_body_sha256(await request.json())\n        if actual_body_sha256 != decode_token[\"request_body_sha256\"]:\n            # Payload signature does not match the digest in signed token.\n            raise ValueError(\"Invalid request body\")\n        \n        if time.time() - decode_token[\"iat\"] > 60 * 5:\n            # Do not allow push-notifications older than 5 minutes.\n            # This is to prevent replay attack.\n            raise ValueError(\"Token is expired\")\n        \n        return True\n",
      "summary": "**Summary for push_notification_auth.py**\n\n1. **Primary Purpose:**  \n   This module implements authentication and integrity verification for push notification requests. It provides classes and methods for securely signing, sending, and verifying push notifications using JSON Web Tokens (JWT) and JSON Web Keys (JWK/RSA). It also protects against replay attacks and ensures payload integrity.\n\n2. **Parameters (Brief Description):**  \n   The main parameters passed to methods include:\n   - `url` (`str`): The endpoint URL for push notification or JWK set.\n   - `data` (`dict[str, Any]`): The notification payload to be sent or verified.\n   - `request` (`starlette.requests.Request`): The HTTP request, used primarily in receiver/verifier logic.\n\n3. **Return Values (Brief Description):**  \n   - Methods like `verify_push_notification_url` and `verify_push_notification` return a `bool` indicating whether verification/sending succeeded.\n   - `generate_jwk` and `_generate_jwt` return cryptographic keys or JWT tokens.\n   - `handle_jwks_endpoint` returns a `starlette.responses.JSONResponse` containing the public JWKs.\n   - Some methods (`send_push_notification`) do not directly return a value; their purpose is side-effect (sending a request, logging).\n\n4. **Internal Function/Method Calls:**  \n   - **Class methods:**\n     - `PushNotificationAuth._calculate_request_body_sha256`\n     - `PushNotificationSenderAuth.generate_jwk`\n     - `PushNotificationSenderAuth.handle_jwks_endpoint`\n     - `PushNotificationSenderAuth._generate_jwt`\n     - `PushNotificationSenderAuth.send_push_notification`\n     - `PushNotificationReceiverAuth.load_jwks`\n     - `PushNotificationReceiverAuth.verify_push_notification`\n   - **External libraries/methods:**\n     - `uuid.uuid4`\n     - `httpx.AsyncClient`\n     - `jwt.encode`, `jwt.decode`\n     - `jwk.JWK.generate`, `PyJWK.from_json`\n     - `hashlib.sha256`\n     - `json.dumps`, `request.json()`\n     - `logging.getLogger`, `logger.info`, `logger.warning`\n     - `PyJWKClient.get_signing_key_from_jwt`\n     - `time.time()`\n\n**In Short:**  \nThe code defines sender and receiver authentication for push notifications: the sender signs the payload using JWT with an RSA key, publishes its public key, and sends notifications; the receiver loads the sender\u2019s public keys and verifies incoming notifications' authenticity, freshness, and integrity."
    },
    "75": {
      "unit_name": "in_memory_cache.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-a2a/common/utils/in_memory_cache.py",
      "code": "\"\"\"In Memory Cache utility.\"\"\"\n\nimport threading\nimport time\nfrom typing import Any, Dict, Optional\n\n\nclass InMemoryCache:\n    \"\"\"A thread-safe Singleton class to manage cache data.\n\n    Ensures only one instance of the cache exists across the application.\n    \"\"\"\n\n    _instance: Optional[\"InMemoryCache\"] = None\n    _lock: threading.Lock = threading.Lock()\n    _initialized: bool = False\n\n    def __new__(cls):\n        \"\"\"Override __new__ to control instance creation (Singleton pattern).\n\n        Uses a lock to ensure thread safety during the first instantiation.\n\n        Returns:\n            The singleton instance of InMemoryCache.\n        \"\"\"\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def __init__(self):\n        \"\"\"Initialize the cache storage.\n\n        Uses a flag (_initialized) to ensure this logic runs only on the very first\n        creation of the singleton instance.\n        \"\"\"\n        if not self._initialized:\n            with self._lock:\n                if not self._initialized:\n                    # print(\"Initializing SessionCache storage\")\n                    self._cache_data: Dict[str, Dict[str, Any]] = {}\n                    self._ttl: Dict[str, float] = {}\n                    self._data_lock: threading.Lock = threading.Lock()\n                    self._initialized = True\n\n    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n        \"\"\"Set a key-value pair.\n\n        Args:\n            key: The key for the data.\n            value: The data to store.\n            ttl: Time to live in seconds. If None, data will not expire.\n        \"\"\"\n        with self._data_lock:\n            self._cache_data[key] = value\n\n            if ttl is not None:\n                self._ttl[key] = time.time() + ttl\n            else:\n                if key in self._ttl:\n                    del self._ttl[key]\n\n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"Get the value associated with a key.\n\n        Args:\n            key: The key for the data within the session.\n            default: The value to return if the session or key is not found.\n\n        Returns:\n            The cached value, or the default value if not found.\n        \"\"\"\n        with self._data_lock:\n            if key in self._ttl and time.time() > self._ttl[key]:\n                del self._cache_data[key]\n                del self._ttl[key]\n                return default\n            return self._cache_data.get(key, default)\n\n    def delete(self, key: str) -> None:\n        \"\"\"Delete a specific key-value pair from a cache.\n\n        Args:\n            key: The key to delete.\n\n        Returns:\n            True if the key was found and deleted, False otherwise.\n        \"\"\"\n\n        with self._data_lock:\n            if key in self._cache_data:\n                del self._cache_data[key]\n                if key in self._ttl:\n                    del self._ttl[key]\n                return True\n            return False\n\n    def clear(self) -> bool:\n        \"\"\"Remove all data.\n\n        Returns:\n            True if the data was cleared, False otherwise.\n        \"\"\"\n        with self._data_lock:\n            self._cache_data.clear()\n            self._ttl.clear()\n            return True\n        return False\n",
      "summary": "**Summary for `/tmp/PocketFlow/cookbook/pocketflow-a2a/common/utils/in_memory_cache.py`:**\n\n1. **Primary Purpose:**  \n   The code implements a thread-safe singleton in-memory cache class (`InMemoryCache`). This cache allows storage and retrieval of key-value pairs with optional time-to-live (TTL) expiration, suitable for temporary data caching within a Python application.\n\n2. **Description of Parameters:**  \n   - `set(self, key: str, value: Any, ttl: Optional[int] = None)`:  \n     - `key`: String key to identify the cached value.  \n     - `value`: The data to be stored in the cache.  \n     - `ttl`: Optional integer specifying how many seconds the data should remain in the cache before expiring.\n   - `get(self, key: str, default: Any = None)`:  \n     - `key`: String key used to retrieve cached data.  \n     - `default`: Value to return if the key is not found or has expired.\n   - `delete(self, key: str)`:  \n     - `key`: String key of the cache entry to remove.\n   - `clear(self)`:  \n     - No parameters.\n\n3. **Description of Return Values:**  \n   - `set`: Returns `None`.  \n   - `get`: Returns the cached value if present and not expired; otherwise returns the provided `default` value.\n   - `delete`: Returns `True` if the key existed and was deleted; `False` otherwise.\n   - `clear`: Returns `True` after clearing all cache data.\n\n4. **Other Functions or Methods it Calls Internally:**  \n   - Standard library functions:  \n     - `time.time()` for current timestamp checks.\n   - Built-in dictionary methods:  \n     - `.clear()`, `.get()`, `del` for cache management.\n   - Uses internal attributes and threading primitives:  \n     - `threading.Lock` for thread-safety.\n   - Its own methods are not called recursively; methods operate independently."
    },
    "76": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-agent/nodes.py",
      "code": "from pocketflow import Node\nfrom utils import call_llm, search_web_duckduckgo\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web_duckduckgo(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass AnswerQuestion(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n        \n        # We're done - no need to continue the flow\n        return \"done\" \n",
      "summary": "**Summary of nodes.py**\n\n1. **Primary Purpose:**  \nThe code implements a set of agent workflow nodes for an automated research assistant, orchestrating whether to answer a question directly or search the web for additional information. It uses these nodes to decide actions, execute web searches, and generate answers via calls to a large language model (LLM).\n\n2. **Parameters:**  \nEach node operates on a shared data store (`shared`, a dictionary) and sometimes accepts or returns intermediate data between steps:\n- Methods typically accept and/or return context, question strings, or search queries (e.g., `shared[\"question\"]`, `shared[\"context\"]`).\n- The core external input is the user\u2019s question provided in the shared dict.\n\n3. **Return Value:**  \n- Each node returns action/result indicators to guide the workflow (e.g., whether to search or answer, or whether processing is \"done\").\n- Some methods also return intermediate data such as the parsed decision from the LLM or the web search results.\n\n4. **Functions/Methods Called Internally:**\n- **External:**\n  - `call_llm`: Sends a prompt to the language model and receives a response.\n  - `search_web_duckduckgo`: Executes web searches based on a query.\n  - `yaml.safe_load`: Parses YAML-formatted LLM responses.\n- **Internal (within this file):**\n  - Node methods: `prep`, `exec`, and `post` functions across the classes for handling workflow stages.\n  - Python built-ins: e.g., string manipulation, printing.\n\n**Classes define the core workflow stages:**\n- `DecideAction`: Determines next action by prompting the LLM and parsing its structured (YAML) output.\n- `SearchWeb`: Conducts web searches and appends findings to the context.\n- `AnswerQuestion`: Crafts and saves a final answer using all accumulated information.\n\nThe design enables the agent to reason, dig deeper with searches as needed, and answer questions robustly using LLM-powered decision-making and answering."
    },
    "77": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-agent/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    \"\"\"\n    Create and connect the nodes to form a complete agent flow.\n    \n    The flow works like this:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node\n    3. If answer, go to AnswerQuestion node\n    4. After SearchWeb completes, go back to DecideAction\n    \n    Returns:\n        Flow: A complete research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n    \n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n    \n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n    \n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n    \n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide) ",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary purpose:**  \n   This code defines the function `create_agent_flow`, whose primary purpose is to construct and connect the nodes of a research agent workflow. The agent repeatedly decides whether to search the web or answer a question, forming a loop based on the agent's decisions.\n\n2. **Parameters:**  \n   The `create_agent_flow` function takes **no parameters**.\n\n3. **Return value:**  \n   The function returns a `Flow` object (from `pocketflow`), representing the entire agent workflow, with its starting node set to the `DecideAction` node.\n\n4. **Internally called functions/methods:**  \n   - `DecideAction()` (constructor, from `nodes`)\n   - `SearchWeb()` (constructor, from `nodes`)\n   - `AnswerQuestion()` (constructor, from `nodes`)\n   - Operators used for node connection (likely custom overloads):  \n     - `-` (e.g., `decide - \"search\"`)\n     - `>>` (e.g., `... >> search`)\n   - `Flow(start=decide)` (constructor, from `pocketflow`)"
    },
    "78": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-agent/main.py",
      "code": "import sys\nfrom flow import create_agent_flow\n\ndef main():\n    \"\"\"Simple function to process a question.\"\"\"\n    # Default question\n    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    # Create the agent flow\n    agent_flow = create_agent_flow()\n    \n    # Process the question\n    shared = {\"question\": question}\n    print(f\"\ud83e\udd14 Processing question: {question}\")\n    agent_flow.run(shared)\n    print(\"\\n\ud83c\udfaf Final Answer:\")\n    print(shared.get(\"answer\", \"No answer found\"))\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of `main.py` from `/tmp/PocketFlow/cookbook/pocketflow-agent/`:**\n\n1. **Primary purpose:**  \n   The script acts as a command-line entry point that processes a question using an agent flow, prints the question being processed, and then prints the final answer.\n\n2. **Parameters:**  \n   The script does not define any functions with parameters for external use. However, it retrieves an optional question from the command-line arguments if provided with the `--` prefix; otherwise, it defaults to `\"Who won the Nobel Prize in Physics 2024?\"`.\n\n3. **Return value:**  \n   The script (and its `main` function) does not return any values; its output is side-effecting (printing to standard output).\n\n4. **Functions or methods called internally:**  \n   - `flow.create_agent_flow()`  \n   - `agent_flow.run(shared)`  \n   - Standard library functions: `print()`, command-line parsing via `sys.argv`."
    },
    "79": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-agent/utils.py",
      "code": "from openai import OpenAI\nimport os\nfrom duckduckgo_search import DDGS\nimport requests\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef search_web_duckduckgo(query):\n    results = DDGS().text(query, max_results=5)\n    # Convert results to a string\n    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n    return results_str\n\ndef search_web_brave(query):\n\n    url = f\"https://api.search.brave.com/res/v1/web/search?q={query}\"\n    api_key = \"your brave search api key\"\n\n    headers = {\n        \"accept\": \"application/json\",\n        \"Accept-Encoding\": \"gzip\",\n        \"x-subscription-token\": api_key\n    }\n\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n        data = response.json()\n        results = data['web']['results']\n        results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['url']}\\nDescription: {r['description']}\" for r in results])     \n    else:\n        print(f\"Request failed with status code: {response.status_code}\")\n    return results_str\n    \nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n    print(\"## Testing search_web\")\n    query = \"Who won the Nobel Prize in Physics 2024?\"\n    print(f\"## Query: {query}\")\n    results = search_web_duckduckgo(query)\n    print(f\"## Results: {results}\")",
      "summary": "**Summary of code unit: `/tmp/PocketFlow/cookbook/pocketflow-agent/utils.py`**\n\n1. **Primary Purpose:**  \n   This code provides utility functions to interact with large language models (LLMs) via OpenAI's API and to perform web searches using DuckDuckGo and Brave Search APIs. It is designed to enable an agent to generate natural language responses and retrieve search results from the web.\n\n2. **Parameters:**  \n   - `call_llm(prompt)`:  \n     - `prompt` (str): The textual prompt to send to the OpenAI LLM.\n   - `search_web_duckduckgo(query)`:  \n     - `query` (str): The search query string for DuckDuckGo search.\n   - `search_web_brave(query)`:  \n     - `query` (str): The search query string for Brave Search.\n\n3. **Return Values:**  \n   - `call_llm`: Returns a string with the LLM\u2019s generated response to the prompt.\n   - `search_web_duckduckgo`: Returns a string aggregating titles, URLs, and snippets of the top DuckDuckGo search results.\n   - `search_web_brave`: Returns a string aggregating titles, URLs, and descriptions from the Brave Search API (if successful); otherwise, returns an undefined variable (potential bug if request fails).\n\n4. **Internal Calls to Other Functions/Methods:**  \n   - Calls to external libraries:\n     - `OpenAI` (from `openai`)\n     - `DDGS().text()` (from `duckduckgo_search`)\n     - `requests.get()` (from `requests`)\n   - Uses environment variables via `os.environ.get`\n   - Standard Python functions like `print`\n\n**Additional Notes:**  \n- The script includes a test block (`if __name__ == \"__main__\":`) demonstrating basic usage of `call_llm` and `search_web_duckduckgo`.  \n- Error handling in `search_web_brave` may be incomplete, as it can attempt to return `results_str` even if the web request fails.  \n- API keys should be managed securely; currently, \"your-api-key\" and \"your brave search api key\" are placeholders."
    },
    "80": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tao/nodes.py",
      "code": "# nodes.py\n\nfrom pocketflow import Node\nimport yaml\nfrom utils import call_llm\n\nclass ThinkNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context needed for thinking\"\"\"\n        query = shared.get(\"query\", \"\")\n        observations = shared.get(\"observations\", [])\n        thoughts = shared.get(\"thoughts\", [])\n        current_thought_number = shared.get(\"current_thought_number\", 0)\n        \n        # Update thought count\n        shared[\"current_thought_number\"] = current_thought_number + 1\n        \n        # Format previous observations\n        observations_text = \"\\n\".join([f\"Observation {i+1}: {obs}\" for i, obs in enumerate(observations)])\n        if not observations_text:\n            observations_text = \"No observations yet.\"\n            \n        return {\n            \"query\": query,\n            \"observations_text\": observations_text,\n            \"thoughts\": thoughts,\n            \"current_thought_number\": current_thought_number + 1\n        }\n    \n    def exec(self, prep_res):\n        \"\"\"Execute the thinking process, decide the next action\"\"\"\n        query = prep_res[\"query\"]\n        observations_text = prep_res[\"observations_text\"]\n        current_thought_number = prep_res[\"current_thought_number\"]\n        \n        # Build the prompt\n        prompt = f\"\"\"\n        You are an AI assistant solving a problem. Based on the user's query and previous observations, think about what action to take next.\n        \n        User query: {query}\n        \n        Previous observations:\n        {observations_text}\n        \n        Please think about the next action and return your thinking process and decision in YAML format:\n        ```yaml\n        thinking: |\n            <detailed thinking process>\n        action: <action name, such as 'search' or 'answer'>\n        action_input: <input parameters for the action>\n        is_final: <set to true if this is the final answer, otherwise false>\n        ```\n        \"\"\"\n        \n        # Call LLM to get thinking result\n        response = call_llm(prompt)\n        \n        # Parse YAML response\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        thought_data = yaml.safe_load(yaml_str)\n        \n        # Add thought number\n        thought_data[\"thought_number\"] = current_thought_number\n        \n        return thought_data\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the thinking result and decide the next step in the flow\"\"\"\n        # Save thinking result\n        if \"thoughts\" not in shared:\n            shared[\"thoughts\"] = []\n        shared[\"thoughts\"].append(exec_res)\n        \n        # Save action information\n        shared[\"current_action\"] = exec_res[\"action\"]\n        shared[\"current_action_input\"] = exec_res[\"action_input\"]\n        \n        # If it's the final answer, end the flow\n        if exec_res.get(\"is_final\", False):\n            shared[\"final_answer\"] = exec_res[\"action_input\"]\n            print(f\"\ud83c\udfaf Final Answer: {exec_res['action_input']}\")\n            return \"end\"\n        \n        # Otherwise continue with the action\n        print(f\"\ud83e\udd14 Thought {exec_res['thought_number']}: Decided to execute {exec_res['action']}\")\n        return \"action\"\n\nclass ActionNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare to execute action\"\"\"\n        action = shared[\"current_action\"]\n        action_input = shared[\"current_action_input\"]\n        return action, action_input\n    \n    def exec(self, inputs):\n        \"\"\"Execute action and return result\"\"\"\n        action, action_input = inputs\n        \n        print(f\"\ud83d\ude80 Executing action: {action}, input: {action_input}\")\n        \n        # Execute different operations based on action type\n        if action == \"search\":\n            # Simulate search operation\n            result = self.search_web(action_input)\n        elif action == \"calculate\":\n            # Simulate calculation operation\n            result = self.calculate(action_input)\n        elif action == \"answer\":\n            # Direct return answer\n            result = action_input\n        else:\n            # Unknown action type\n            result = f\"Unknown action type: {action}\"\n        \n        return result\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save action result\"\"\"\n        # Save the current action result\n        shared[\"current_action_result\"] = exec_res\n        print(f\"\u2705 Action completed, result obtained\")\n        \n        # Continue to observation node\n        return \"observe\"\n    \n    # Simulated tool functions\n    def search_web(self, query):\n        # This should be actual search logic\n        return f\"Search results: Information about '{query}'...\"\n    \n    def calculate(self, expression):\n        # This should be actual calculation logic\n        try:\n            return f\"Calculation result: {eval(expression)}\"\n        except:\n            return f\"Unable to calculate expression: {expression}\"\n\nclass ObserveNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare observation data\"\"\"\n        action = shared[\"current_action\"]\n        action_input = shared[\"current_action_input\"]\n        action_result = shared[\"current_action_result\"]\n        return action, action_input, action_result\n    \n    def exec(self, inputs):\n        \"\"\"Analyze action results, generate observation\"\"\"\n        action, action_input, action_result = inputs\n        \n        # Build prompt\n        prompt = f\"\"\"\n        You are an observer, needing to analyze action results and provide objective observations.\n        \n        Action: {action}\n        Action input: {action_input}\n        Action result: {action_result}\n        \n        Please provide a concise observation of this result. Don't make decisions, just describe what you see.\n        \"\"\"\n        \n        # Call LLM to get observation result\n        observation = call_llm(prompt)\n        \n        print(f\"\ud83d\udc41\ufe0f Observation: {observation[:50]}...\")\n        return observation\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save observation result and decide next flow step\"\"\"\n        # Save observation result\n        if \"observations\" not in shared:\n            shared[\"observations\"] = []\n        shared[\"observations\"].append(exec_res)\n        \n        # Continue thinking\n        return \"think\"\n    \n\n    \nclass EndNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare end node\"\"\"\n        \n        return {}\n    def exec(self, prep_res):\n        \"\"\"Execute end operation\"\"\"\n        print(\"Flow ended, thank you for using!\")\n        return None\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"End flow\"\"\"\n        return None",
      "summary": "**Summary of /tmp/PocketFlow/cookbook/pocketflow-tao/nodes.py**\n\n1. **Primary Purpose**:  \n   This code defines a set of Node classes that implement the core logic for a multi-step, LLM-driven problem-solving workflow (\"TAO\": Think, Act, Observe) within the PocketFlow framework. Each node encapsulates a stage of the thought-action loop, managing the context, interacting with an LLM, handling actions, and producing or logging observations and results.\n\n2. **Parameters**:  \n   - Each node's methods (`prep`, `exec`, `post`) primarily take and operate on a shared state dictionary (`shared`) that tracks the current query, thoughts, actions, results, and observations as the workflow progresses.\n   - `exec` methods typically receive preparatory data (`prep_res`) or action inputs derived from `shared`.\n\n3. **Return Value**:  \n   - `prep` methods: return context or relevant data needed for execution (often as dicts or tuples).\n   - `exec` methods: return the computed result of the node's main operation (e.g., parsed YAML, action result, or observation string).\n   - `post` methods: update the shared state and return the name of the next workflow step (e.g., \"action\", \"observe\", \"think\", \"end\"). For the `EndNode`, they simply return `None`.\n\n4. **Internal Functions/Methods Called**:  \n   - `call_llm`: (from utils) Used to interact with an LLM for generating thoughts, decisions, or observations.\n   - `yaml.safe_load`: (from PyYAML) For parsing YAML-formatted reasoning/decision from the LLM output.\n   - `self.search_web`/`self.calculate`: Simulated action logic for search or calculation (within `ActionNode`).\n   - `eval`: For on-the-fly calculation (with error handling) in `calculate`.\n   - Standard Python string methods and list/dictionary operations.\n  \nAdditional Context:\n- The code assumes a larger PocketFlow orchestration where these nodes interact in a directed fashion (\"think\" -> \"act\" -> \"observe\" -> ...).\n- Print statements are used for runtime tracing of key stages and outputs."
    },
    "81": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tao/flow.py",
      "code": "# flow.py\n\nfrom pocketflow import Flow\nfrom nodes import ThinkNode, ActionNode, ObserveNode, EndNode\n\ndef create_tao_flow():\n    \"\"\"\n    Create a Thought-Action-Observation loop flow\n    \n    How the flow works:\n    1. ThinkNode decides the next action\n    2. ActionNode executes the action\n    3. ObserveNode observes the action result\n    4. Return to ThinkNode to continue thinking, or end the flow\n    \n    Returns:\n        Flow: Complete TAO loop flow\n    \"\"\"\n    # Create node instances\n    think = ThinkNode()\n    action = ActionNode()\n    observe = ObserveNode()\n    end = EndNode()\n    \n    # Connect nodes\n    # If ThinkNode returns \"action\", go to ActionNode\n    think - \"action\" >> action\n    \n    # If ThinkNode returns \"end\", end the flow\n    think - \"end\" >> end\n    \n    # After ActionNode completes, go to ObserveNode\n    action - \"observe\" >> observe\n    \n    # After ObserveNode completes, return to ThinkNode\n    observe - \"think\" >> think\n    \n    # Create and return flow, starting from ThinkNode\n    return Flow(start=think)",
      "summary": "**Summary of `flow.py` (from `/tmp/PocketFlow/cookbook/pocketflow-tao/flow.py`):**\n\n1. **Primary purpose:**  \n   This code defines the function `create_tao_flow`, which builds a reusable flow representing the Thought-Action-Observation (TAO) loop, used for iterative agent behavior or process automation. The flow connects \"thinking\", \"action\", \"observation\", and \"ending\" nodes to create a cyclical pattern.\n\n2. **Parameters:**  \n   The `create_tao_flow` function does **not** take any parameters.\n\n3. **Return value:**  \n   The function returns a `Flow` object (from the `pocketflow` package), which encapsulates the fully connected TAO loop. The flow starts at the ThinkNode.\n\n4. **Other functions or methods called internally:**\n   - `ThinkNode()`, `ActionNode()`, `ObserveNode()`, `EndNode()` \u2014 constructors for the nodes involved in the loop (from `nodes`).\n   - Node transition methods/operators (e.g., `think - \"action\" >> action`, etc.), which define the flow's state transitions.\n   - `Flow(start=think)` \u2014 creates and returns the final flow, initialized with the starting node.\n\n**Note:** No parameters are taken by the function, and the only return is the constructed `Flow` representing a TAO behavior cycle."
    },
    "82": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tao/main.py",
      "code": "# main.py\n\nfrom flow import create_tao_flow\n\ndef main():\n    \n    query = \"\"\"I need to understand the latest developments in artificial intelligence\"\"\"\n    \n    # Create shared data\n    shared = {\n        \"query\": query,\n        \"thoughts\": [],\n        \"observations\": [],\n        \"current_thought_number\": 0\n    }\n    \n    # Create and run flow\n    tao_flow = create_tao_flow()\n    tao_flow.run(shared)\n    \n    # Print final result\n    if \"final_answer\" in shared:\n        print(\"\\nFinal Answer:\")\n        print(shared[\"final_answer\"])\n    else:\n        print(\"\\nFlow did not produce a final answer\")\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of main.py**\n\n1. **Primary Purpose:**  \n   The code serves as the entry point for running a TAO (Think, Act, Observe) flow on a user-defined query about the latest developments in artificial intelligence. It sets up shared state, executes the flow, and displays the final answer if available.\n\n2. **Parameters:**  \n   This script does not define any parameters for its main function or as command-line arguments. The query is hardcoded within the script.\n\n3. **Return Value:**  \n   The `main` function (and the script overall) does not return any value. It performs its work via side effects, specifically printing to standard output.\n\n4. **Internal Calls:**  \n   - `create_tao_flow()` (imported from the `flow` module): Used to create the TAO flow object.\n   - `tao_flow.run(shared)`: Runs the TAO flow using the shared data.\n   - `print()`: Standard output to display the final answer or a message if no answer is produced."
    },
    "83": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tao/utils.py",
      "code": "# utils.py\n\nfrom openai import OpenAI\nimport os\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"Your Key Here\"),base_url=os.environ.get(\"OPENAI_API_BASE\", \"Your API Base Here\"))\n    r = client.chat.completions.create(\n        model=os.environ.get(\"OPENAI_MODEL\", \"openai/gpt-4.1-nano\"),\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tao/utils.py`:**\n\n1. **Primary purpose:**  \n   This code unit provides a utility function to send a text prompt to an OpenAI chat model using the OpenAI API, and returns the model\u2019s generated response.\n\n2. **Parameters:**  \n   - The main function `call_llm(prompt)` takes a single parameter:  \n     - `prompt` (str): The user\u2019s input text prompt to send to the language model.\n\n3. **Return value:**  \n   - `call_llm(prompt)` returns a string:  \n     - The model\u2019s textual response extracted from the first choice of the OpenAI chat completion API result.\n\n4. **Other functions/methods called internally:**  \n   - `OpenAI()` (constructor from the openai library)\n   - `os.environ.get()` (standard Python function to fetch environment variables)\n   - `client.chat.completions.create()` (OpenAI API method to generate chat completions)\n\nThe file also contains a simple test block that demonstrates calling `call_llm` if the module is run directly."
    },
    "84": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-node/flow.py",
      "code": "from pocketflow import Node, Flow\nfrom utils.call_llm import call_llm\n\nclass Summarize(Node):\n    def prep(self, shared):\n        \"\"\"Read and preprocess data from shared store.\"\"\"\n        return shared[\"data\"]\n\n    def exec(self, prep_res):\n        \"\"\"Execute the summarization using LLM.\"\"\"\n        if not prep_res:\n            return \"Empty text\"\n        prompt = f\"Summarize this text in 10 words: {prep_res}\"\n        summary = call_llm(prompt)  # might fail\n        return summary\n\n    def exec_fallback(self, shared, prep_res, exc):\n        \"\"\"Provide a simple fallback instead of crashing.\"\"\"\n        return \"There was an error processing your request.\"\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the summary in shared store.\"\"\"\n        shared[\"summary\"] = exec_res\n        # Return \"default\" by not returning\n\n# Create the flow\nsummarize_node = Summarize(max_retries=3)\nflow = Flow(start=summarize_node) ",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary purpose:**  \n   This code defines a \"Summarize\" node for use in a workflow, which reads input text, generates a brief 10-word summary using a large language model (LLM), handles errors gracefully, and stores the result. The node is then used to create a simple flow with up to three retries on failure.\n\n2. **Parameters:**  \n   - The `Summarize` class methods use:\n     - `shared`: a shared dictionary-like object used to pass and store data between stages.\n     - `prep_res`: the preprocessed input text.\n     - `exec_res`: the summarization result.\n     - `exc`: the exception encountered during execution (in `exec_fallback`).\n   - `summarize_node = Summarize(max_retries=3)`  \n     - The node is initialized with `max_retries=3`, meaning the summarization step will be retried up to three times upon failure.\n\n3. **Return value:**  \n   - `prep(self, shared)`: returns the text data to be summarized.\n   - `exec(self, prep_res)`: returns the generated summary or a message if the input is empty.\n   - `exec_fallback(self, shared, prep_res, exc)`: returns an error message if summarization fails.\n   - `post(self, shared, prep_res, exec_res)`: does not return a value; it stores the summary in the `shared` object.\n\n4. **Internal functions and methods called:**  \n   - `call_llm(prompt)` (from `utils.call_llm`): used in `exec` to generate the summary via an LLM.\n\n**In summary:**  \nThis code creates a reusable workflow node for text summarization, which interacts with an LLM, retries on failure, and safely stores the resulting summary, while depending mainly on a utility function `call_llm` to perform the summarization step."
    },
    "85": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-node/main.py",
      "code": "from flow import flow\n\ndef main():\n    # Example text to summarize\n    text = \"\"\"\n    PocketFlow is a minimalist LLM framework that models workflows as a Nested Directed Graph.\n    Nodes handle simple LLM tasks, connecting through Actions for Agents.\n    Flows orchestrate these nodes for Task Decomposition, and can be nested.\n    It also supports Batch processing and Async execution.\n    \"\"\"\n\n    # Initialize shared store\n    shared = {\"data\": text}\n    \n    # Run the flow\n    flow.run(shared)\n    \n    # Print result\n    print(\"\\nInput text:\", text)\n    print(\"\\nSummary:\", shared[\"summary\"])\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of Code Unit: `main.py`**\n\n1. **Primary Purpose:**  \n   The code serves as an entry point to run a summarization workflow using the `flow` object (imported from `flow`). It processes a sample text, running it through a predefined sequence of operations to generate a summary, and then prints both the input and the summary.\n\n2. **Parameters:**  \n   - The `main()` function does not take any parameters.  \n   - It uses a hardcoded `text` example within the function.\n\n3. **Return Value:**  \n   - The `main()` function does not return any value. Results are printed to standard output, and data is updated in the shared dictionary.\n\n4. **Other Functions or Methods Called Internally:**  \n   - `flow.run(shared)`: Runs the flow using the shared data dictionary.\n   - `print()`: Used to display the input text and generated summary.\n\n**Note:**  \nThe code expects that `flow.run(shared)` will update the `shared` dictionary with a `\"summary\"` key containing the summarized text."
    },
    "86": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-node/utils/call_llm.py",
      "code": "from openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n    \nif __name__ == \"__main__\":\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt))",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   This code unit defines a utility function to interact with the OpenAI GPT-4o language model, sending a prompt and returning the generated response.\n\n2. **Parameters:**  \n   - The function `call_llm(prompt)` takes a single parameter:\n     - `prompt` (string): The input text or question to be sent to the language model.\n\n3. **Return Value:**  \n   - The function returns the text content of the language model's response (as a string).\n\n4. **Internally Called Functions/Methods:**  \n   - `OpenAI()` constructor (from the `openai` library)\n   - `client.chat.completions.create()`\n   - Accesses `r.choices[0].message.content`\n\n**Note:**  \nThe API key `\"YOUR_API_KEY_HERE\"` is a placeholder and must be replaced with a valid key for the code to work."
    },
    "87": {
      "unit_name": "visualize.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-visualization/visualize.py",
      "code": "# %%\n\nimport json\nimport os\nimport http.server\nimport socketserver\nimport threading\nimport webbrowser\nimport time\nimport socket\nimport importlib\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Optional, Tuple, Union\n\nfrom pocketflow import Flow\n\nfrom async_flow import order_pipeline\n\n\ndef build_mermaid(start):\n    ids, visited, lines = {}, set(), [\"graph LR\"]\n    ctr = 1\n\n    def get_id(n):\n        nonlocal ctr\n        return (\n            ids[n] if n in ids else (ids.setdefault(n, f\"N{ctr}\"), (ctr := ctr + 1))[0]\n        )\n\n    def link(a, b, action=None):\n        if action:\n            lines.append(f\"    {a} -->|{action}| {b}\")\n        else:\n            lines.append(f\"    {a} --> {b}\")\n\n    def walk(node, parent=None, action=None):\n        if node in visited:\n            return parent and link(parent, get_id(node), action)\n        visited.add(node)\n        if isinstance(node, Flow):\n            node.start_node and parent and link(parent, get_id(node.start_node), action)\n            lines.append(\n                f\"\\n    subgraph sub_flow_{get_id(node)}[{type(node).__name__}]\"\n            )\n            node.start_node and walk(node.start_node)\n            for act, nxt in node.successors.items():\n                node.start_node and walk(nxt, get_id(node.start_node), act) or (\n                    parent and link(parent, get_id(nxt), action)\n                ) or walk(nxt, None, act)\n            lines.append(\"    end\\n\")\n        else:\n            lines.append(f\"    {(nid := get_id(node))}['{type(node).__name__}']\")\n            parent and link(parent, nid, action)\n            [walk(nxt, nid, act) for act, nxt in node.successors.items()]\n\n    walk(start)\n    return \"\\n\".join(lines)\n\n\ndef flow_to_json(start):\n    \"\"\"Convert a flow to JSON format suitable for D3.js visualization.\n\n    This function walks through the flow graph and builds a structure with:\n    - nodes: All non-Flow nodes with their group memberships\n    - links: Connections between nodes within the same group\n    - group_links: Connections between different groups (for inter-flow connections)\n    - flows: Flow information for group labeling\n\n    Returns:\n        dict: A JSON-serializable dictionary with 'nodes' and 'links' arrays.\n    \"\"\"\n    nodes = []\n    links = []\n    group_links = []  # For connections between groups (Flow to Flow)\n    ids = {}\n    node_types = {}\n    flow_nodes = {}  # Keep track of flow nodes\n    ctr = 1\n\n    def get_id(n):\n        nonlocal ctr\n        if n not in ids:\n            ids[n] = ctr\n            node_types[ctr] = type(n).__name__\n            if isinstance(n, Flow):\n                flow_nodes[ctr] = n  # Store flow reference\n            ctr += 1\n        return ids[n]\n\n    def walk(node, parent=None, group=None, parent_group=None, action=None):\n        \"\"\"Recursively walk the flow graph to build the visualization data.\n\n        Args:\n            node: Current node being processed\n            parent: ID of the parent node that connects to this node\n            group: Group (Flow) ID this node belongs to\n            parent_group: Group ID of the parent node\n            action: Action label on the edge from parent to this node\n        \"\"\"\n        node_id = get_id(node)\n\n        # Add node if not already in nodes list and not a Flow\n        if not any(n[\"id\"] == node_id for n in nodes) and not isinstance(node, Flow):\n            node_data = {\n                \"id\": node_id,\n                \"name\": node_types[node_id],\n                \"group\": group or 0,  # Default group\n            }\n            nodes.append(node_data)\n\n        # Add link from parent if exists\n        if parent and not isinstance(node, Flow):\n            links.append(\n                {\"source\": parent, \"target\": node_id, \"action\": action or \"default\"}\n            )\n\n        # Process different types of nodes\n        if isinstance(node, Flow):\n            # This is a Flow node - it becomes a group container\n            flow_group = node_id  # Use flow's ID as group for contained nodes\n\n            # Add a group-to-group link if this flow has a parent group\n            # This creates connections between nested flows\n            if parent_group is not None and parent_group != flow_group:\n                # Check if this link already exists\n                if not any(\n                    l[\"source\"] == parent_group and l[\"target\"] == flow_group\n                    for l in group_links\n                ):\n                    group_links.append(\n                        {\n                            \"source\": parent_group,\n                            \"target\": flow_group,\n                            \"action\": action or \"default\",\n                        }\n                    )\n\n            if node.start_node:\n                # Process the start node of this flow\n                walk(node.start_node, parent, flow_group, parent_group, action)\n\n                # Process successors of the flow's start node\n                for next_action, nxt in node.successors.items():\n                    walk(\n                        nxt,\n                        get_id(node.start_node),\n                        flow_group,\n                        parent_group,\n                        next_action,\n                    )\n        else:\n            # Process successors for regular nodes\n            for next_action, nxt in node.successors.items():\n                if isinstance(nxt, Flow):\n                    # This node connects to a flow - track the group relationship\n                    flow_group_id = get_id(nxt)\n                    walk(nxt, node_id, None, group, next_action)\n                else:\n                    # Regular node-to-node connection\n                    walk(nxt, node_id, group, parent_group, next_action)\n\n    # Start the traversal\n    walk(start)\n\n    # Post-processing: Generate group links based on node connections between different groups\n    # This ensures that when nodes in different groups are connected, we show a group-to-group\n    # link rather than a direct node-to-node link\n    node_groups = {n[\"id\"]: n[\"group\"] for n in nodes}\n    filtered_links = []\n\n    for link in links:\n        source_id = link[\"source\"]\n        target_id = link[\"target\"]\n        source_group = node_groups.get(source_id, 0)\n        target_group = node_groups.get(target_id, 0)\n\n        # If source and target are in different groups and both groups are valid\n        if source_group != target_group and source_group > 0 and target_group > 0:\n            # Add to group links if not already there\n            # This creates the dashed lines connecting group boxes\n            if not any(\n                gl[\"source\"] == source_group and gl[\"target\"] == target_group\n                for gl in group_links\n            ):\n                group_links.append(\n                    {\n                        \"source\": source_group,\n                        \"target\": target_group,\n                        \"action\": link[\"action\"],\n                    }\n                )\n            # Skip adding this link to filtered_links - we don't want direct node connections across groups\n        else:\n            # Keep links within the same group\n            filtered_links.append(link)\n\n    return {\n        \"nodes\": nodes,\n        \"links\": filtered_links,  # Use filtered links instead of all links\n        \"group_links\": group_links,\n        \"flows\": {str(k): v.__class__.__name__ for k, v in flow_nodes.items()},\n    }\n\n\ndef create_d3_visualization(\n    json_data,\n    output_dir=\"./viz\",\n    filename=\"flow_viz\",\n    html_title=\"PocketFlow Visualization\",\n):\n    \"\"\"Create a D3.js visualization from JSON data.\n\n    Args:\n        json_data: The JSON data for the visualization\n        output_dir: Directory to save the files\n        filename: Base filename (without extension)\n        html_title: Title for the HTML page\n\n    Returns:\n        str: Path to the HTML file\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Save JSON data to file\n    json_path = os.path.join(output_dir, f\"{filename}.json\")\n    with open(json_path, \"w\") as f:\n        json.dump(json_data, f, indent=2)\n\n    # Create HTML file with D3.js visualization\n    html_content = r\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <title>TITLE_PLACEHOLDER</title>\n    <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n    <style>\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 0;\n            overflow: hidden;\n        }\n        svg {\n            width: 100vw;\n            height: 100vh;\n        }\n        .links path {\n            fill: none;\n            stroke: #999;\n            stroke-opacity: 0.6;\n            stroke-width: 1.5px;\n        }\n        .group-links path {\n            fill: none;\n            stroke: #333;\n            stroke-opacity: 0.8;\n            stroke-width: 2px;\n            stroke-dasharray: 5,5;\n        }\n        .nodes circle {\n            stroke: #fff;\n            stroke-width: 1.5px;\n        }\n        .node-labels {\n            font-size: 12px;\n            pointer-events: none;\n        }\n        .link-labels {\n            font-size: 10px;\n            fill: #666;\n            pointer-events: none;\n        }\n        .group-link-labels {\n            font-size: 11px;\n            font-weight: bold;\n            fill: #333;\n            pointer-events: none;\n        }\n        .group-container {\n            stroke: #333;\n            stroke-width: 1.5px;\n            stroke-dasharray: 5,5;\n            fill: rgba(200, 200, 200, 0.1);\n            rx: 10;\n            ry: 10;\n        }\n        .group-label {\n            font-size: 14px;\n            font-weight: bold;\n            pointer-events: none;\n        }\n    </style>\n</head>\n<body>\n    <svg id=\"graph\"></svg>\n    <script>\n        // Load data from file\n        d3.json(\"FILENAME_PLACEHOLDER.json\").then(data => {\n            const svg = d3.select(\"#graph\");\n            const width = window.innerWidth;\n            const height = window.innerHeight;\n            \n            // Define arrow markers for links\n            svg.append(\"defs\").append(\"marker\")\n                .attr(\"id\", \"arrowhead\")\n                .attr(\"viewBox\", \"0 -5 10 10\")\n                .attr(\"refX\", 25) // Position the arrow away from the target node\n                .attr(\"refY\", 0)\n                .attr(\"orient\", \"auto\")\n                .attr(\"markerWidth\", 6)\n                .attr(\"markerHeight\", 6)\n                .attr(\"xoverflow\", \"visible\")\n                .append(\"path\")\n                .attr(\"d\", \"M 0,-5 L 10,0 L 0,5\")\n                .attr(\"fill\", \"#999\");\n                \n            // Define thicker arrow markers for group links\n            svg.append(\"defs\").append(\"marker\")\n                .attr(\"id\", \"group-arrowhead\")\n                .attr(\"viewBox\", \"0 -5 10 10\")\n                .attr(\"refX\", 3) // Position at the boundary of the group\n                .attr(\"refY\", 0)\n                .attr(\"orient\", \"auto\")\n                .attr(\"markerWidth\", 8)\n                .attr(\"markerHeight\", 8)\n                .attr(\"xoverflow\", \"visible\")\n                .append(\"path\")\n                .attr(\"d\", \"M 0,-5 L 10,0 L 0,5\")\n                .attr(\"fill\", \"#333\");\n            \n            // Color scale for node groups\n            const color = d3.scaleOrdinal(d3.schemeCategory10);\n            \n            // Process the data to identify groups\n            const groups = {};\n            data.nodes.forEach(node => {\n                if (node.group > 0) {\n                    if (!groups[node.group]) {\n                        // Use the flow name instead of generic \"Group X\"\n                        const flowName = data.flows && data.flows[node.group] ? data.flows[node.group] : `Flow ${node.group}`;\n                        groups[node.group] = {\n                            id: node.group,\n                            name: flowName,\n                            nodes: [],\n                            x: 0,\n                            y: 0,\n                            width: 0,\n                            height: 0\n                        };\n                    }\n                    groups[node.group].nodes.push(node);\n                }\n            });\n            \n            // Create a force simulation\n            const simulation = d3.forceSimulation(data.nodes)\n                // Controls the distance between connected nodes\n                .force(\"link\", d3.forceLink(data.links).id(d => d.id).distance(100))\n                // Controls how nodes repel each other - lower values bring nodes closer\n                .force(\"charge\", d3.forceManyBody().strength(-30))\n                // Centers the entire graph in the SVG\n                .force(\"center\", d3.forceCenter(width / 2, height / 2))\n                // Prevents nodes from overlapping - acts like a minimum distance\n                .force(\"collide\", d3.forceCollide().radius(50));\n            \n            // Group forces - create a force to keep nodes in the same group closer together\n            // This creates the effect of nodes clustering within their group boxes\n            const groupForce = alpha => {\n                for (let i = 0; i < data.nodes.length; i++) {\n                    const node = data.nodes[i];\n                    if (node.group > 0) {\n                        const group = groups[node.group];\n                        if (group && group.nodes.length > 1) {\n                            // Calculate center of group\n                            let centerX = 0, centerY = 0;\n                            group.nodes.forEach(n => {\n                                centerX += n.x || 0;\n                                centerY += n.y || 0;\n                            });\n                            centerX /= group.nodes.length;\n                            centerY /= group.nodes.length;\n                            \n                            // Move nodes toward center\n                            const k = alpha * 0.3; // Increased from 0.1 to 0.3\n                            node.vx += (centerX - node.x) * k;\n                            node.vy += (centerY - node.y) * k;\n                        }\n                    }\n                }\n            };\n            \n            // Additional force to position groups in a more organized layout (like in the image)\n            // This arranges the groups horizontally/vertically based on their connections\n            const groupLayoutForce = alpha => {\n                // Get group centers\n                const groupCenters = Object.values(groups).map(g => {\n                    return { id: g.id, cx: 0, cy: 0 };\n                });\n                \n                // Calculate current center positions\n                Object.values(groups).forEach(g => {\n                    if (g.nodes.length > 0) {\n                        let cx = 0, cy = 0;\n                        g.nodes.forEach(n => {\n                            cx += n.x || 0;\n                            cy += n.y || 0;\n                        });\n                        \n                        const groupCenter = groupCenters.find(gc => gc.id === g.id);\n                        if (groupCenter) {\n                            groupCenter.cx = cx / g.nodes.length;\n                            groupCenter.cy = cy / g.nodes.length;\n                        }\n                    }\n                });\n                \n                // Apply forces to position groups\n                const k = alpha * 0.05;\n                \n                // Try to position groups in a more structured way\n                // Adjust these values to change the overall layout\n                for (let i = 0; i < data.group_links.length; i++) {\n                    const link = data.group_links[i];\n                    const source = groupCenters.find(g => g.id === link.source);\n                    const target = groupCenters.find(g => g.id === link.target);\n                    \n                    if (source && target) {\n                        // Add a horizontal force to align groups\n                        const desiredDx = 300; // Desired horizontal distance between linked groups\n                        const dx = target.cx - source.cx;\n                        const diff = desiredDx - Math.abs(dx);\n                        \n                        // Apply forces to group nodes\n                        groups[source.id].nodes.forEach(n => {\n                            if (dx > 0) {\n                                n.vx -= diff * k;\n                            } else {\n                                n.vx += diff * k;\n                            }\n                        });\n                        \n                        groups[target.id].nodes.forEach(n => {\n                            if (dx > 0) {\n                                n.vx += diff * k;\n                            } else {\n                                n.vx -= diff * k;\n                            }\n                        });\n                    }\n                }\n            };\n            \n            simulation.force(\"group\", groupForce);\n            simulation.force(\"groupLayout\", groupLayoutForce);\n            \n            // Create links with arrow paths instead of lines\n            const link = svg.append(\"g\")\n                .attr(\"class\", \"links\")\n                .selectAll(\"path\")\n                .data(data.links)\n                .enter()\n                .append(\"path\")\n                .attr(\"stroke-width\", 2)\n                .attr(\"stroke\", \"#999\")\n                .attr(\"marker-end\", \"url(#arrowhead)\");  // Add the arrowhead marker\n            \n            // Create group containers (drawn before nodes)\n            const groupContainers = svg.append(\"g\")\n                .attr(\"class\", \"groups\")\n                .selectAll(\"rect\")\n                .data(Object.values(groups))\n                .enter()\n                .append(\"rect\")\n                .attr(\"class\", \"group-container\")\n                .attr(\"fill\", d => d3.color(color(d.id)).copy({opacity: 0.2}));\n            \n            // Create group links between flows\n            const groupLink = svg.append(\"g\")\n                .attr(\"class\", \"group-links\")\n                .selectAll(\"path\")\n                .data(data.group_links || [])\n                .enter()\n                .append(\"path\")\n                .attr(\"stroke-width\", 2)\n                .attr(\"stroke\", \"#333\")\n                .attr(\"marker-end\", \"url(#group-arrowhead)\");\n                \n            // Create group link labels\n            const groupLinkLabel = svg.append(\"g\")\n                .attr(\"class\", \"group-link-labels\")\n                .selectAll(\"text\")\n                .data(data.group_links || [])\n                .enter()\n                .append(\"text\")\n                .text(d => d.action)\n                .attr(\"font-size\", \"11px\")\n                .attr(\"font-weight\", \"bold\")\n                .attr(\"fill\", \"#333\");\n            \n            // Create group labels\n            const groupLabels = svg.append(\"g\")\n                .attr(\"class\", \"group-labels\")\n                .selectAll(\"text\")\n                .data(Object.values(groups))\n                .enter()\n                .append(\"text\")\n                .attr(\"class\", \"group-label\")\n                .text(d => d.name)  // Now using the proper flow name\n                .attr(\"fill\", d => d3.color(color(d.id)).darker());\n            \n            // Create link labels\n            const linkLabel = svg.append(\"g\")\n                .attr(\"class\", \"link-labels\")\n                .selectAll(\"text\")\n                .data(data.links)\n                .enter()\n                .append(\"text\")\n                .text(d => d.action)\n                .attr(\"font-size\", \"10px\")\n                .attr(\"fill\", \"#666\");\n            \n            // Create nodes\n            const node = svg.append(\"g\")\n                .attr(\"class\", \"nodes\")\n                .selectAll(\"circle\")\n                .data(data.nodes)\n                .enter()\n                .append(\"circle\")\n                .attr(\"r\", 15)\n                .attr(\"fill\", d => color(d.group))\n                .call(d3.drag()\n                    .on(\"start\", dragstarted)\n                    .on(\"drag\", dragged)\n                    .on(\"end\", dragended));\n            \n            // Create node labels\n            const nodeLabel = svg.append(\"g\")\n                .attr(\"class\", \"node-labels\")\n                .selectAll(\"text\")\n                .data(data.nodes)\n                .enter()\n                .append(\"text\")\n                .text(d => d.name)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"dy\", 25);\n            \n            // Add tooltip on hover\n            node.append(\"title\")\n                .text(d => d.name);\n            \n            // Update positions on each tick\n            simulation.on(\"tick\", () => {\n                // Update links with straight lines\n                link.attr(\"d\", d => {\n                    return `M${d.source.x},${d.source.y} L${d.target.x},${d.target.y}`;\n                });\n                \n                // Update nodes\n                node\n                    .attr(\"cx\", d => d.x)\n                    .attr(\"cy\", d => d.y);\n                \n                // Update node labels\n                nodeLabel\n                    .attr(\"x\", d => d.x)\n                    .attr(\"y\", d => d.y);\n                \n                // Position link labels at midpoint\n                linkLabel\n                    .attr(\"x\", d => (d.source.x + d.target.x) / 2)\n                    .attr(\"y\", d => (d.source.y + d.target.y) / 2);\n                \n                // Update group containers\n                groupContainers.each(function(d) {\n                    // If there are nodes in this group\n                    if (d.nodes.length > 0) {\n                        let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;\n                        \n                        // Find the bounding box for all nodes in the group\n                        d.nodes.forEach(n => {\n                            minX = Math.min(minX, n.x - 30);\n                            minY = Math.min(minY, n.y - 30);\n                            maxX = Math.max(maxX, n.x + 30);\n                            maxY = Math.max(maxY, n.y + 40); // Extra space for labels\n                        });\n                        \n                        // Add padding\n                        const padding = 20;\n                        minX -= padding;\n                        minY -= padding;\n                        maxX += padding;\n                        maxY += padding;\n                        \n                        // Save group dimensions\n                        d.x = minX;\n                        d.y = minY;\n                        d.width = maxX - minX;\n                        d.height = maxY - minY;\n                        d.centerX = minX + d.width / 2;\n                        d.centerY = minY + d.height / 2;\n                        \n                        // Set position and size of the group container\n                        d3.select(this)\n                            .attr(\"x\", minX)\n                            .attr(\"y\", minY)\n                            .attr(\"width\", d.width)\n                            .attr(\"height\", d.height);\n                        \n                        // Update group label position (top-left of group)\n                        groupLabels.filter(g => g.id === d.id)\n                            .attr(\"x\", minX + 10)\n                            .attr(\"y\", minY + 20);\n                    }\n                });\n                \n                // Update group links between flows\n                groupLink.attr(\"d\", d => {\n                    const sourceGroup = groups[d.source];\n                    const targetGroup = groups[d.target];\n                    \n                    if (!sourceGroup || !targetGroup) return \"\";\n                    \n                    // Find intersection points with group boundaries\n                    // This ensures links connect to the group's border rather than its center\n                    \n                    // Calculate centers of groups\n                    const sx = sourceGroup.centerX;\n                    const sy = sourceGroup.centerY;\n                    const tx = targetGroup.centerX;\n                    const ty = targetGroup.centerY;\n                    \n                    // Calculate angle between centers - used to find intersection points\n                    const angle = Math.atan2(ty - sy, tx - sx);\n                    \n                    // Calculate intersection points with source group borders\n                    // We cast a ray from center in the direction of the target\n                    let sourceX, sourceY;\n                    const cosA = Math.cos(angle);\n                    const sinA = Math.sin(angle);\n                    \n                    // Check intersection with horizontal borders (top and bottom)\n                    const ts_top = (sourceGroup.y - sy) / sinA;\n                    const ts_bottom = (sourceGroup.y + sourceGroup.height - sy) / sinA;\n                    \n                    // Check intersection with vertical borders (left and right)\n                    const ts_left = (sourceGroup.x - sx) / cosA;\n                    const ts_right = (sourceGroup.x + sourceGroup.width - sx) / cosA;\n                    \n                    // Use the closest positive intersection (first hit with the boundary)\n                    let t_source = Infinity;\n                    if (ts_top > 0) t_source = Math.min(t_source, ts_top);\n                    if (ts_bottom > 0) t_source = Math.min(t_source, ts_bottom);\n                    if (ts_left > 0) t_source = Math.min(t_source, ts_left);\n                    if (ts_right > 0) t_source = Math.min(t_source, ts_right);\n                    \n                    // Target group: Find intersection in the opposite direction\n                    // We cast a ray from target center toward the source\n                    let targetX, targetY;\n                    const oppositeAngle = angle + Math.PI;\n                    const cosOpp = Math.cos(oppositeAngle);\n                    const sinOpp = Math.sin(oppositeAngle);\n                    \n                    // Check intersections for target group\n                    const tt_top = (targetGroup.y - ty) / sinOpp;\n                    const tt_bottom = (targetGroup.y + targetGroup.height - ty) / sinOpp;\n                    const tt_left = (targetGroup.x - tx) / cosOpp;\n                    const tt_right = (targetGroup.x + targetGroup.width - tx) / cosOpp;\n                    \n                    // Use the closest positive intersection\n                    let t_target = Infinity;\n                    if (tt_top > 0) t_target = Math.min(t_target, tt_top);\n                    if (tt_bottom > 0) t_target = Math.min(t_target, tt_bottom);\n                    if (tt_left > 0) t_target = Math.min(t_target, tt_left);\n                    if (tt_right > 0) t_target = Math.min(t_target, tt_right);\n                    \n                    // Calculate actual border points using parametric equation:\n                    // point = center + t * direction\n                    if (t_source !== Infinity) {\n                        sourceX = sx + cosA * t_source;\n                        sourceY = sy + sinA * t_source;\n                    } else {\n                        sourceX = sx;\n                        sourceY = sy;\n                    }\n                    \n                    if (t_target !== Infinity) {\n                        targetX = tx + cosOpp * t_target;\n                        targetY = ty + sinOpp * t_target;\n                    } else {\n                        targetX = tx;\n                        targetY = ty;\n                    }\n                    \n                    // Create a straight line between the border points\n                    return `M${sourceX},${sourceY} L${targetX},${targetY}`;\n                });\n                \n                // Update group link labels\n                groupLinkLabel.attr(\"x\", d => {\n                    const sourceGroup = groups[d.source];\n                    const targetGroup = groups[d.target];\n                    if (!sourceGroup || !targetGroup) return 0;\n                    return (sourceGroup.centerX + targetGroup.centerX) / 2;\n                })\n                .attr(\"y\", d => {\n                    const sourceGroup = groups[d.source];\n                    const targetGroup = groups[d.target];\n                    if (!sourceGroup || !targetGroup) return 0;\n                    return (sourceGroup.centerY + targetGroup.centerY) / 2 - 10;\n                });\n            });\n            \n            // Drag functions\n            function dragstarted(event, d) {\n                if (!event.active) simulation.alphaTarget(0.3).restart();\n                d.fx = d.x;\n                d.fy = d.y;\n            }\n            \n            function dragged(event, d) {\n                d.fx = event.x;\n                d.fy = event.y;\n            }\n            \n            function dragended(event, d) {\n                if (!event.active) simulation.alphaTarget(0);\n                d.fx = null;\n                d.fy = null;\n            }\n        });\n    </script>\n</body>\n</html>\n\"\"\"\n\n    # Replace the placeholders with the actual values\n    html_content = html_content.replace(\"FILENAME_PLACEHOLDER\", filename)\n    html_content = html_content.replace(\"TITLE_PLACEHOLDER\", html_title)\n\n    # Write HTML to file\n    html_path = os.path.join(output_dir, f\"{filename}.html\")\n    with open(html_path, \"w\") as f:\n        f.write(html_content)\n\n    print(f\"Visualization created at {html_path}\")\n    return html_path\n\n\ndef find_free_port():\n    \"\"\"Find a free port on localhost.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"\", 0))\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        return s.getsockname()[1]\n\n\ndef start_http_server(directory, port=None):\n    \"\"\"Start an HTTP server in the given directory.\n\n    Args:\n        directory: Directory to serve files from\n        port: Port to use (finds a free port if None)\n\n    Returns:\n        tuple: (server_thread, port)\n    \"\"\"\n    if port is None:\n        port = find_free_port()\n\n    # Get the absolute path of the directory\n    directory = str(Path(directory).absolute())\n\n    # Change to the directory to serve files\n    os.chdir(directory)\n\n    # Create HTTP server\n    handler = http.server.SimpleHTTPRequestHandler\n    httpd = socketserver.TCPServer((\"\", port), handler)\n\n    # Start server in a separate thread\n    server_thread = threading.Thread(target=httpd.serve_forever)\n    server_thread.daemon = (\n        True  # This makes the thread exit when the main program exits\n    )\n    server_thread.start()\n\n    print(f\"Server started at http://localhost:{port}\")\n    return server_thread, port\n\n\ndef serve_and_open_visualization(html_path, auto_open=True):\n    \"\"\"Serve the HTML file and open it in a browser.\n\n    Args:\n        html_path: Path to the HTML file\n        auto_open: Whether to automatically open the browser\n\n    Returns:\n        tuple: (server_thread, url)\n    \"\"\"\n    # Get the directory and filename\n    directory = os.path.dirname(os.path.abspath(html_path))\n    filename = os.path.basename(html_path)\n\n    # Start the server\n    server_thread, port = start_http_server(directory)\n\n    # Build the URL\n    url = f\"http://localhost:{port}/{filename}\"\n\n    # Open the URL in a browser\n    if auto_open:\n        print(f\"Opening {url} in your browser...\")\n        webbrowser.open(url)\n    else:\n        print(f\"Visualization available at {url}\")\n\n    return server_thread, url\n\n\ndef visualize_flow(\n    flow: Flow,\n    flow_name: str,\n    serve: bool = True,\n    auto_open: bool = True,\n    output_dir: str = \"./viz\",\n    html_title: Optional[str] = None,\n) -> Union[str, Tuple[str, Any, str]]:\n    \"\"\"Helper function to visualize a flow with both mermaid and D3.js\n\n    Args:\n        flow: Flow object to visualize\n        flow_name: Name of the flow (used for filename and display)\n        serve: Whether to start a server for the visualization\n        auto_open: Whether to automatically open in browser\n        output_dir: Directory to save visualization files\n        html_title: Custom title for the HTML page (defaults to flow_name if None)\n\n    Returns:\n        str or tuple: Path to HTML file, or (path, server_thread, url) if serve=True\n    \"\"\"\n    print(f\"\\n--- {flow_name} Mermaid Diagram ---\")\n    print(build_mermaid(start=flow))\n\n    print(f\"\\n--- {flow_name} D3.js Visualization ---\")\n    json_data = flow_to_json(flow)\n\n    # Create the visualization\n    output_filename = f\"{flow_name.lower().replace(' ', '_')}\"\n\n    # Use flow_name as the HTML title if not specified\n    if html_title is None:\n        html_title = f\"PocketFlow: {flow_name}\"\n\n    html_path = create_d3_visualization(\n        json_data,\n        output_dir=output_dir,\n        filename=output_filename,\n        html_title=html_title,\n    )\n\n    # Serve and open if requested\n    if serve:\n        server_thread, url = serve_and_open_visualization(html_path, auto_open)\n        return html_path, server_thread, url\n\n    return html_path\n\n\ndef load_flow_from_module(module_path: str, flow_variable: str) -> Flow:\n    \"\"\"Dynamically load a flow from a module.\n\n    Args:\n        module_path: Path to the module (e.g., 'my_package.my_module')\n        flow_variable: Name of the flow variable in the module\n\n    Returns:\n        Flow: The loaded flow object\n    \"\"\"\n    try:\n        module = importlib.import_module(module_path)\n        return getattr(module, flow_variable)\n    except (ImportError, AttributeError) as e:\n        print(f\"Error loading flow: {e}\")\n        sys.exit(1)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Visualize a PocketFlow flow\")\n    parser.add_argument(\n        \"--module\", default=\"async_flow\", help=\"Module containing the flow\"\n    )\n    parser.add_argument(\n        \"--flow\", default=\"order_pipeline\", help=\"Flow variable name in the module\"\n    )\n    parser.add_argument(\n        \"--name\", default=\"Flow Visualization\", help=\"Name for the visualization\"\n    )\n    parser.add_argument(\n        \"--output-dir\", default=\"./viz\", help=\"Directory to save visualization files\"\n    )\n    parser.add_argument(\"--no-serve\", action=\"store_true\", help=\"Don't start a server\")\n    parser.add_argument(\n        \"--no-open\", action=\"store_true\", help=\"Don't open browser automatically\"\n    )\n    parser.add_argument(\"--title\", help=\"Custom HTML title\")\n\n    args = parser.parse_args()\n\n    # Load flow from the specified module\n    flow_obj = load_flow_from_module(args.module, args.flow)\n\n    # Visualize the flow\n    visualize_flow(\n        flow=flow_obj,\n        flow_name=args.name,\n        serve=not args.no_serve,\n        auto_open=not args.no_open,\n        output_dir=args.output_dir,\n        html_title=args.title,\n    )\n\n    # Keep server running if serving\n    if not args.no_serve:\n        try:\n            print(\"\\nServer is running. Press Ctrl+C to stop...\")\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            print(\"\\nShutting down...\")\n",
      "summary": "**Summary of visualize.py**\n\n1. **Primary Purpose**  \nThe primary purpose of this code unit is to visualize flow graphs defined using the PocketFlow framework. It provides both textual (Mermaid) and interactive (D3.js HTML) visualizations of a given Flow object, allowing users to view and explore flow pipelines in their browser, either by serving them locally or by generating static artifacts.\n\n2. **Brief Description of Parameters**  \nKey parameters (from main entry functions and CLI):\n\n- **flow**: The `Flow` object to visualize.\n- **flow_name**: String name for the visualization (used for file naming and display).\n- **serve**: Boolean; whether to start a server to view the visualization.\n- **auto_open**: Boolean; whether to automatically open the visualization in a web browser.\n- **output_dir**: Directory path where output files (HTML/JSON) are saved.\n- **html_title**: Custom HTML page title.\n- **module_path**: Python module path string (for loading a flow dynamically).\n- **flow_variable**: Variable name in the module holding the Flow.\n\n3. **Brief Description of Return Value**\n\n- The main function `visualize_flow` returns the HTML file path generated, and if `serve=True`, also returns the HTTP server thread and the accessible URL.\n- Command-line usage: Outputs the server URL or the HTML path for the visualization.\n\n4. **List of Internally Called Functions/Methods**\n\n- `build_mermaid(start)`: Converts a flow to a textual Mermaid diagram string.\n- `flow_to_json(start)`: Converts a flow to a JSON structure suitable for D3.js visualization.\n- `create_d3_visualization(json_data, ...)`: Writes JSON and HTML visualization files.\n- `find_free_port()`: Finds a free TCP port for hosting visualization.\n- `start_http_server(directory, port)`: Starts an HTTP server to serve visualization files.\n- `serve_and_open_visualization(html_path, auto_open)`: Launches the server and opens the HTML page in the browser.\n- `visualize_flow(...)`: High-level utility to execute the full visualization workflow.\n- `load_flow_from_module(module_path, flow_variable)`: Dynamically imports and retrieves a Flow instance.\n- Standard library functions (e.g., `os.makedirs`, `argparse.ArgumentParser`, `importlib.import_module`, etc.)\n\n**In summary:**  \nThis code provides a convenient way to turn PocketFlow \"Flow\" objects into interactive or textual visualizations that can be explored in a browser, with several command-line and programmable options for dynamic flow loading, file output, and automatic serving."
    },
    "88": {
      "unit_name": "async_flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-visualization/async_flow.py",
      "code": "from pocketflow import AsyncNode, AsyncFlow\nimport asyncio\n\n\n# Define Payment Nodes\nclass ValidatePayment(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"1.1.Validating payment...\")\n        return \"Payment validated successfully\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"payment_status\"] = exec_res\n        return \"default\"\n\n\nclass ProcessPayment(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"1.2.Processing payment...\")\n        return \"Payment processed successfully\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"payment_result\"] = exec_res\n        return \"default\"\n\n\nclass PaymentConfirmation(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"1.3.Confirming payment...\")\n        return \"Payment confirmed\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"payment_confirmation\"] = exec_res\n        return \"default\"\n\n\n# Define Inventory Nodes\nclass CheckStock(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"2.1.Checking inventory stock...\")\n        return \"Stock available\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"stock_status\"] = exec_res\n        return \"default\"\n\n\nclass ReserveItems(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"2.2.Reserving items...\")\n        return \"Items reserved\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"reservation_status\"] = exec_res\n        return \"default\"\n\n\nclass UpdateInventory(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"2.3. Updating inventory...\")\n        return \"Inventory updated\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"inventory_update\"] = exec_res\n        return \"default\"\n\n\n# Define Shipping Nodes\nclass CreateLabel(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"3.1 Creating shipping label...\")\n        return \"Shipping label created\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"shipping_label\"] = exec_res\n        return \"default\"\n\n\nclass AssignCarrier(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"3.2 Assigning carrier...\")\n        return \"Carrier assigned\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"carrier\"] = exec_res\n        return \"default\"\n\n\nclass SchedulePickup(AsyncNode):\n    async def exec_async(self, prep_res):\n        print(\"3.3 Scheduling pickup...\")\n        return \"Pickup scheduled\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        shared[\"pickup_status\"] = exec_res\n        return \"default\"\n\n\n# Create node instances\nvalidate_payment = ValidatePayment()\nprocess_payment = ProcessPayment()\npayment_confirmation = PaymentConfirmation()\n\ncheck_stock = CheckStock()\nreserve_items = ReserveItems()\nupdate_inventory = UpdateInventory()\n\ncreate_label = CreateLabel()\nassign_carrier = AssignCarrier()\nschedule_pickup = SchedulePickup()\n\n# Payment processing sub-flow\nvalidate_payment >> process_payment >> payment_confirmation\npayment_flow = AsyncFlow(start=validate_payment)\n\n# Inventory sub-flow\ncheck_stock >> reserve_items >> update_inventory\ninventory_flow = AsyncFlow(start=check_stock)\n\n# Shipping sub-flow\ncreate_label >> assign_carrier >> schedule_pickup\nshipping_flow = AsyncFlow(start=create_label)\n\n# Connect the flows into a main order pipeline\npayment_flow >> inventory_flow >> shipping_flow\n# payment_flow >> inventory_flow >> create_label\n# payment_flow >> inventory_flow >> assign_carrier\n\n\n# Create the master flow\nclass OrderFlow(AsyncFlow):\n    pass\n\n\norder_pipeline = OrderFlow(start=payment_flow)\n\n# Create shared data structure\nshared_data = {\n    \"order_id\": \"ORD-12345\",\n    \"customer\": \"John Doe\",\n    \"items\": [\n        {\"id\": \"ITEM-001\", \"name\": \"Smartphone\", \"price\": 999.99, \"quantity\": 1},\n        {\"id\": \"ITEM-002\", \"name\": \"Phone case\", \"price\": 29.99, \"quantity\": 1},\n    ],\n    \"shipping_address\": {\n        \"street\": \"123 Main St\",\n        \"city\": \"Anytown\",\n        \"state\": \"CA\",\n        \"zip\": \"12345\",\n    },\n}\n\n\n# Run the entire pipeline asynchronously\nasync def main():\n    await order_pipeline.run_async(shared_data)\n\n    # Print final status\n    print(\"\\nOrder processing completed!\")\n    print(f\"Payment: {shared_data.get('payment_confirmation')}\")\n    print(f\"Inventory: {shared_data.get('inventory_update')}\")\n    print(f\"Shipping: {shared_data.get('pickup_status')}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-visualization/async_flow.py`:**\n\n1. **Primary Purpose:**  \n   The code defines an asynchronous order processing pipeline using the PocketFlow framework, modeling the steps of payment, inventory, and shipping as sequential, asynchronously executed nodes. It demonstrates how to chain these workflow steps in a modular, extensible way and launch them with shared data.\n\n2. **Parameters:**  \n   - The main function accepts no parameters directly, but the pipeline is run over a shared dictionary (`shared_data`) that contains order details (order ID, customer, items, shipping address, etc.).\n   - Each node receives relevant arguments like `prep_res`, `shared`, and `exec_res` in its `exec_async` and `post_async` methods, as per PocketFlow\u2019s node contract.\n\n3. **Return Value:**  \n   - The `main()` function and the `OrderFlow.run_async()` call do not return a value; side effects are performed by updating `shared_data` and printing output.\n   - Each node's `exec_async` returns a string representing the result of that processing step.\n   - Each node's `post_async` returns the string `\"default\"` to indicate successful completion and flow continuation.\n\n4. **Functions / Methods Called Internally:**  \n   - `AsyncNode.exec_async` (overridden in each node class)\n   - `AsyncNode.post_async` (overridden in each node class)\n   - `AsyncFlow.run_async` (on `order_pipeline`)\n   - `print` (for console output)\n   - `asyncio.run` (to run the event loop for `main()`)\n   - Operators (`>>`) for chaining nodes and sub-flows together (provided by PocketFlow)\n\n**Other notes:**  \nNo explicit utility or helper function is defined apart from the node classes, `main()`, and the construction of flows and shared data. All orchestration is handled by the PocketFlow framework, primarily via the `AsyncNode` and `AsyncFlow` abstractions."
    },
    "89": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/nodes.py",
      "code": "from pocketflow import Node, BatchNode\nfrom tools.crawler import WebCrawler\nfrom tools.parser import analyze_site\nfrom typing import List, Dict\n\nclass CrawlWebsiteNode(Node):\n    \"\"\"Node to crawl a website and extract content\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"base_url\"), shared.get(\"max_pages\", 10)\n        \n    def exec(self, inputs):\n        base_url, max_pages = inputs\n        if not base_url:\n            return []\n            \n        crawler = WebCrawler(base_url, max_pages)\n        return crawler.crawl()\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"crawl_results\"] = exec_res\n        return \"default\"\n\nclass AnalyzeContentBatchNode(BatchNode):\n    \"\"\"Node to analyze crawled content in batches\"\"\"\n    \n    def prep(self, shared):\n        results = shared.get(\"crawl_results\", [])\n        # Process in batches of 5 pages\n        batch_size = 5\n        return [results[i:i+batch_size] for i in range(0, len(results), batch_size)]\n        \n    def exec(self, batch):\n        return analyze_site(batch)\n        \n    def post(self, shared, prep_res, exec_res_list):\n        # Flatten results from all batches\n        all_results = []\n        for batch_results in exec_res_list:\n            all_results.extend(batch_results)\n            \n        shared[\"analyzed_results\"] = all_results\n        return \"default\"\n\nclass GenerateReportNode(Node):\n    \"\"\"Node to generate a summary report of the analysis\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"analyzed_results\", [])\n        \n    def exec(self, results):\n        if not results:\n            return \"No results to report\"\n            \n        report = []\n        report.append(f\"Analysis Report\\n\")\n        report.append(f\"Total pages analyzed: {len(results)}\\n\")\n        \n        for page in results:\n            report.append(f\"\\nPage: {page['url']}\")\n            report.append(f\"Title: {page['title']}\")\n            \n            analysis = page.get(\"analysis\", {})\n            report.append(f\"Summary: {analysis.get('summary', 'N/A')}\")\n            report.append(f\"Topics: {', '.join(analysis.get('topics', []))}\")\n            report.append(f\"Content Type: {analysis.get('content_type', 'unknown')}\")\n            report.append(\"-\" * 80)\n            \n        return \"\\n\".join(report)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"report\"] = exec_res\n        print(\"\\nReport generated:\")\n        print(exec_res)\n        return \"default\"\n",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary purpose:**  \n   This code unit defines a set of reusable workflow nodes for a website analysis pipeline. Specifically, it provides nodes to crawl a website, batch-analyze crawled content, and generate a summary report of the analysis. These nodes are structured to fit into a larger data processing or workflow system (likely based on PocketFlow).\n\n2. **Parameters:**  \n   - The nodes rely on a shared context dictionary (`shared`) for input and output, rather than direct parameters.  \n   - Key `shared` dictionary parameters used include:\n     - `\"base_url\"`: The initial URL to start the web crawl.\n     - `\"max_pages\"`: (optional, default 10) Maximum number of pages to crawl.\n     - `\"crawl_results\"`: List of results from the crawl.\n     - `\"analyzed_results\"`: Output of the analysis process.\n   - Each class method typically receives either data from `shared` or results from the previous pipeline step.\n\n3. **Return value:**  \n   - Each node\u2019s `exec` method returns the result of its operation, e.g.:\n     - `CrawlWebsiteNode`: Returns a list of crawled pages.\n     - `AnalyzeContentBatchNode`: Returns the batch analysis results.\n     - `GenerateReportNode`: Returns a formatted string report or an error message.\n   - The `post` method generally updates the `shared` context with the node\u2019s output and returns a status string (always `\"default\"`).\n\n4. **Internal function/method calls:**  \n   - `WebCrawler(base_url, max_pages).crawl()` \u2014 to perform the website crawl.\n   - `analyze_site(batch)` \u2014 to analyze a batch of crawled pages.\n   - Built-in dictionary and list operations (e.g., `get`, `extend`).\n   - String formatting and `print()` in the report generation step.\n\n**In summary,** the code structures a three-stage, shareable workflow for web content crawling, batch analysis, and reporting, by leveraging custom nodes that manage their data via a shared context dictionary and orchestrate their actions using both internal and imported helper methods."
    },
    "90": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import CrawlWebsiteNode, AnalyzeContentBatchNode, GenerateReportNode\n\ndef create_flow() -> Flow:\n    \"\"\"Create and configure the crawling flow\n    \n    Returns:\n        Flow: Configured flow ready to run\n    \"\"\"\n    # Create nodes\n    crawl = CrawlWebsiteNode()\n    analyze = AnalyzeContentBatchNode()\n    report = GenerateReportNode()\n    \n    # Connect nodes\n    crawl >> analyze >> report\n    \n    # Create flow starting with crawl\n    return Flow(start=crawl)\n",
      "summary": "**Summary of flow.py**\n\n1. **Primary Purpose:**  \n   This code defines a function to create and configure a simple data flow for crawling websites, analyzing their content, and generating a report using predefined node classes.\n\n2. **Parameters:**  \n   The function `create_flow()` does not take any parameters.\n\n3. **Return Value:**  \n   The function returns a `Flow` object that is configured to start with the web crawling node and continue through content analysis to report generation.\n\n4. **Internally Called Functions/Methods:**  \n   - `CrawlWebsiteNode()` (constructor)\n   - `AnalyzeContentBatchNode()` (constructor)\n   - `GenerateReportNode()` (constructor)\n   - The chaining operator `>>` (likely overloads for connecting nodes)\n   - `Flow(start=...)` (constructor to initialize the flow)"
    },
    "91": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/main.py",
      "code": "import os\nfrom flow import create_flow\n\ndef main():\n    \"\"\"Run the web crawler flow\"\"\"\n    \n    # Get website URL from user\n    url = input(\"Enter website URL to crawl (e.g., https://example.com): \")\n    if not url:\n        print(\"Error: URL is required\")\n        return\n        \n    # Initialize shared data\n    shared = {\n        \"base_url\": url,\n        \"max_pages\": 1\n    }\n    \n    # Create and run flow\n    flow = create_flow()\n    flow.run(shared)\n    \n    # Results are in shared[\"report\"]\n    \nif __name__ == \"__main__\":\n    main()\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/main.py`:**\n\n1. **Primary purpose:**  \n   This code serves as the entry point for a web crawler tool. It prompts the user for a website URL, sets up relevant configuration, and initiates a web crawling flow by invoking a pipeline defined elsewhere.\n\n2. **Parameters:**  \n   The code does not define any explicit function parameters; instead, it interacts with the user through standard input within the `main()` function.\n\n3. **Return value:**  \n   The `main()` function does not return a value; it operates primarily through side effects (user interaction, flow execution, and, possibly, updating the `shared` dictionary with results).\n\n4. **Internally called functions/methods:**  \n   - `input()` (built-in Python function for user input)  \n   - `print()` (built-in Python function to output errors)  \n   - `create_flow()` (imported from the local `flow` module)  \n   - `flow.run()` (method of the object returned by `create_flow()`)"
    },
    "92": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/utils/__init__.py",
      "code": "",
      "summary": "It appears that you have asked for an analysis of the code unit __init__.py from /tmp/PocketFlow/cookbook/pocketflow-tool-crawler/utils/, but you have not provided any code content for this file.\n\nWithout any code to review, I can only offer this general summary:\n\n1. The primary purpose of a __init__.py file is typically to indicate that the directory is a Python package, and it can also be used to initialize package-level variables or import submodules.\n\n2. No parameters can be described, as no functions or classes are present without code content.\n\n3. No return values are produced or can be described for the same reason.\n\n4. No other functions or methods are called internally, unless there is relevant code provided.\n\nIf you provide the actual code within the file, I will be able to offer a more detailed and specific summary based on the content."
    },
    "93": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/utils/call_llm.py",
      "code": "from openai import OpenAI\nimport os\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef call_llm(prompt: str) -> str:\n    \"\"\"Call OpenAI API to analyze text\n    \n    Args:\n        prompt (str): Input prompt for the model\n        \n    Returns:\n        str: Model response\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.choices[0].message.content\n        \n    except Exception as e:\n        print(f\"Error calling LLM API: {str(e)}\")\n        return \"\"\n\nif __name__ == \"__main__\":\n    # Test LLM call\n    response = call_llm(\"What is web crawling?\")\n    print(\"Response:\", response)\n",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   The code unit provides a utility function to interact with OpenAI's GPT-4 model via the OpenAI API, sending user prompts and retrieving responses. It is designed to serve as a simple wrapper for making LLM (Large Language Model) calls.\n\n2. **Parameters:**  \n   - The main function `call_llm` takes a single parameter:\n     - `prompt` (str): The text prompt to be sent to the GPT-4 model for processing.\n\n3. **Return Value:**  \n   - The function returns a string containing the model's response to the input prompt. If an error occurs (e.g., API call failure), it returns an empty string.\n\n4. **Internally Called Functions/Methods:**\n   - `os.getenv`: To retrieve the `OPENAI_API_KEY` from environment variables.\n   - `OpenAI` (class initialization): To initialize the OpenAI client with the API key.\n   - `client.chat.completions.create`: To send the chat prompt to the GPT-4 model and receive a generated response.\n   - `print`: Used for error messages and example output.\n   - `str(e)`: To format exception messages."
    },
    "94": {
      "unit_name": "parser.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/tools/parser.py",
      "code": "from typing import Dict, List\nfrom utils.call_llm import call_llm\n\ndef analyze_content(content: Dict) -> Dict:\n    \"\"\"Analyze webpage content using LLM\n    \n    Args:\n        content (Dict): Webpage content with url, title and text\n        \n    Returns:\n        Dict: Analysis results including summary and topics\n    \"\"\"\n    prompt = f\"\"\"\nAnalyze this webpage content:\n\nTitle: {content['title']}\nURL: {content['url']}\nContent: {content['text'][:2000]}  # Limit content length\n\nPlease provide:\n1. A brief summary (2-3 sentences)\n2. Main topics/keywords (up to 5)\n3. Content type (article, product page, etc)\n\nOutput in YAML format:\n```yaml\nsummary: >\n    brief summary here\ntopics:\n    - topic 1\n    - topic 2\ncontent_type: type here\n```\n\"\"\"\n    \n    try:\n        response = call_llm(prompt)\n        # Extract YAML between code fences\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        \n        import yaml\n        analysis = yaml.safe_load(yaml_str)\n        \n        # Validate required fields\n        assert \"summary\" in analysis\n        assert \"topics\" in analysis\n        assert \"content_type\" in analysis\n        assert isinstance(analysis[\"topics\"], list)\n        \n        return analysis\n        \n    except Exception as e:\n        print(f\"Error analyzing content: {str(e)}\")\n        return {\n            \"summary\": \"Error analyzing content\",\n            \"topics\": [],\n            \"content_type\": \"unknown\"\n        }\n\ndef analyze_site(crawl_results: List[Dict]) -> List[Dict]:\n    \"\"\"Analyze all crawled pages\n    \n    Args:\n        crawl_results (List[Dict]): List of crawled page contents\n        \n    Returns:\n        List[Dict]: Original content with added analysis\n    \"\"\"\n    analyzed_results = []\n    \n    for content in crawl_results:\n        if content and content.get(\"text\"):\n            analysis = analyze_content(content)\n            content[\"analysis\"] = analysis\n            analyzed_results.append(content)\n            \n    return analyzed_results\n",
      "summary": "**Summary of `parser.py`:**\n\n1. **Primary Purpose:**  \n   This code unit is designed to analyze the content of crawled web pages using a language model (LLM). It generates concise summaries, identifies main topics, and determines the content type for each web page, returning structured analysis for further use.\n\n2. **Parameters:**  \n   - `analyze_content(content: Dict)`: Takes a dictionary representing a single web page, expected to contain `'url'`, `'title'`, and `'text'` fields.\n   - `analyze_site(crawl_results: List[Dict])`: Takes a list of dictionaries, each representing the content of a crawled web page.\n\n3. **Return Values:**  \n   - `analyze_content` returns a dictionary with keys: `'summary'`, `'topics'`, and `'content_type'`, or an error response if analysis fails.\n   - `analyze_site` returns a list of the original content dictionaries, each augmented with an `'analysis'` field containing the result from `analyze_content`.\n\n4. **Functions/Methods Called Internally:**  \n   - `call_llm`: Used to interact with the language model for content analysis.\n   - `yaml.safe_load`: Parses the YAML output produced by the LLM.\n   - Standard assertions and exception handling for response validation.\n   - Built-in string manipulation methods (`split`, `strip`)."
    },
    "95": {
      "unit_name": "crawler.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-crawler/tools/crawler.py",
      "code": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nfrom typing import Dict, List, Set\n\nclass WebCrawler:\n    \"\"\"Simple web crawler that extracts content and follows links\"\"\"\n    \n    def __init__(self, base_url: str, max_pages: int = 10):\n        self.base_url = base_url\n        self.max_pages = max_pages\n        self.visited: Set[str] = set()\n        \n    def is_valid_url(self, url: str) -> bool:\n        \"\"\"Check if URL belongs to the same domain\"\"\"\n        base_domain = urlparse(self.base_url).netloc\n        url_domain = urlparse(url).netloc\n        return base_domain == url_domain\n        \n    def extract_page_content(self, url: str) -> Dict:\n        \"\"\"Extract content from a single page\"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.text, \"html.parser\")\n            \n            # Extract main content\n            content = {\n                \"url\": url,\n                \"title\": soup.title.string if soup.title else \"\",\n                \"text\": soup.get_text(separator=\"\\n\", strip=True),\n                \"links\": []\n            }\n            \n            # Extract links\n            for link in soup.find_all(\"a\"):\n                href = link.get(\"href\")\n                if href:\n                    absolute_url = urljoin(url, href)\n                    if self.is_valid_url(absolute_url):\n                        content[\"links\"].append(absolute_url)\n            \n            return content\n            \n        except Exception as e:\n            print(f\"Error crawling {url}: {str(e)}\")\n            return None\n    \n    def crawl(self) -> List[Dict]:\n        \"\"\"Crawl website starting from base_url\"\"\"\n        to_visit = [self.base_url]\n        results = []\n        \n        while to_visit and len(self.visited) < self.max_pages:\n            url = to_visit.pop(0)\n            \n            if url in self.visited:\n                continue\n                \n            print(f\"Crawling: {url}\")\n            content = self.extract_page_content(url)\n            \n            if content:\n                self.visited.add(url)\n                results.append(content)\n                \n                # Add new URLs to visit\n                new_urls = [url for url in content[\"links\"] \n                          if url not in self.visited \n                          and url not in to_visit]\n                to_visit.extend(new_urls)\n        \n        return results\n",
      "summary": "**Summary of `crawler.py` (Unit: WebCrawler)**\n\n1. **Primary purpose:**  \n   The code defines a simple web crawler class (`WebCrawler`) that fetches web pages starting from a base URL, extracts their content (title, text, and internal links), and recursively follows links within the same domain, up to a specified number of pages.\n\n2. **Parameters:**  \n   - **`base_url` (str):** The starting URL for the crawler. Determines which domain links are considered valid and initiates the crawling process.  \n   - **`max_pages` (int, default=10):** The maximum number of pages to crawl before stopping.\n\n3. **Return value:**  \n   - The main method `crawl()` returns a list of dictionaries. Each dictionary contains information about a crawled page: its URL, title, plain text content, and internal links found on that page.\n\n4. **Internally called functions/methods:**  \n   - **requests.get**: Fetches web page content over HTTP(S).\n   - **response.raise_for_status**: Raises an error if the HTTP request failed.\n   - **BeautifulSoup**: Parses HTML content of each page.\n   - **urljoin, urlparse** (from urllib.parse): Construct absolute URLs and extract domain information.\n   - **WebCrawler.is_valid_url**: Checks if a link belongs to the original domain.\n   - **WebCrawler.extract_page_content**: Handles the fetching and parsing of a single page.\n   - Methods that make use of standard Python set and list operations (for tracking visited and pending URLs).\n   - **print**: Logs crawling status and errors."
    },
    "96": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-cli-hitl/nodes.py",
      "code": "from pocketflow import Node\nfrom utils.call_llm import call_llm\n\nclass GetTopicNode(Node):\n    \"\"\"Prompts the user to enter the topic for the joke.\"\"\"\n    def exec(self, _shared):\n        return input(\"What topic would you like a joke about? \")\n\n    def post(self, shared, _prep_res, exec_res):\n        shared[\"topic\"] = exec_res\n\nclass GenerateJokeNode(Node):\n    \"\"\"Generates a joke based on the topic and any previous feedback.\"\"\"\n    def prep(self, shared):\n        topic = shared.get(\"topic\", \"anything\")\n        disliked_jokes = shared.get(\"disliked_jokes\", [])\n        \n        prompt = f\"Please generate an one-liner joke about: {topic}. Make it short and funny.\"\n        if disliked_jokes:\n            disliked_str = \"; \".join(disliked_jokes)\n            prompt = f\"The user did not like the following jokes: [{disliked_str}]. Please generate a new, different joke about {topic}.\"\n        return prompt\n\n    def exec(self, prep_res):\n        return call_llm(prep_res)\n\n    def post(self, shared, _prep_res, exec_res):\n        shared[\"current_joke\"] = exec_res\n        print(f\"\\nJoke: {exec_res}\")\n\nclass GetFeedbackNode(Node):\n    \"\"\"Presents the joke to the user and asks for approval.\"\"\"\n    def exec(self, _prep_res):\n        while True:\n            feedback = input(\"Did you like this joke? (yes/no): \").strip().lower()\n            if feedback in [\"yes\", \"y\", \"no\", \"n\"]:\n                return feedback\n            print(\"Invalid input. Please type 'yes' or 'no'.\")\n\n    def post(self, shared, _prep_res, exec_res):\n        if exec_res in [\"yes\", \"y\"]:\n            shared[\"user_feedback\"] = \"approve\"\n            print(\"Great! Glad you liked it.\")\n            return \"Approve\"\n        else:\n            shared[\"user_feedback\"] = \"disapprove\"\n            current_joke = shared.get(\"current_joke\")\n            if current_joke:\n                if \"disliked_jokes\" not in shared:\n                    shared[\"disliked_jokes\"] = []\n                shared[\"disliked_jokes\"].append(current_joke)\n            print(\"Okay, let me try another one.\")\n            return \"Disapprove\" ",
      "summary": "**Summary of nodes.py:**\n\n1. **Primary purpose:**  \n   The code defines three nodes/classes (GetTopicNode, GenerateJokeNode, GetFeedbackNode) that together form a human-in-the-loop pipeline for generating and rating AI-generated one-liner jokes based on user input and feedback. The workflow gathers a joke topic from the user, generates a joke using an LLM, presents the joke, and collects user feedback to refine further generations.\n\n2. **Parameters:**  \n   - The main methods (`exec`, `prep`, `post`) in each node receive state dictionaries (`shared`, `_shared`, `prep_res`, etc.), which carry and share state among nodes.\n   - `GetTopicNode.exec` receives `_shared` (unused).\n   - `GenerateJokeNode.prep` and `post` use the shared dictionary to access and store context.\n   - `GenerateJokeNode.exec` takes in the prompt generated by `prep_res`.\n   - `GetFeedbackNode.exec` receives `_prep_res` (unused).\n   - `GetFeedbackNode.post` receives `shared`, `_prep_res`, and the feedback result.\n\n3. **Return values:**  \n   - `GetTopicNode.exec` returns the user-entered topic as a string.\n   - `GenerateJokeNode.prep` returns a prompt string for the LLM.\n   - `GenerateJokeNode.exec` returns the generated joke from the LLM.\n   - `GetFeedbackNode.exec` returns the normalized user feedback (\"yes\"/\"no\").\n   - `GetFeedbackNode.post` returns a status string (\"Approve\" or \"Disapprove\") based on the user feedback.  \n   - `post` methods may also update the shared state dictionary.\n\n4. **Other functions/methods called internally:**  \n   - `input()` (reads user input in GetTopicNode and GetFeedbackNode)\n   - `print()` (outputs text to user in GenerateJokeNode and GetFeedbackNode)\n   - `call_llm()` (in GenerateJokeNode, to generate the joke using an LLM)\n   - Basic dictionary methods (`get`, item access, append) for state management in shared dict.\n   - Standard string methods (`strip()`, `lower()`, `join()`).\n\n**In short:**  \nThese classes orchestrate a simple, interactive joke generation and feedback pipeline, using user input, shared state, and an external LLM function, with straightforward user I/O and repeated improvement based on feedback."
    },
    "97": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-cli-hitl/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import GetTopicNode, GenerateJokeNode, GetFeedbackNode\n\ndef create_joke_flow() -> Flow:\n    \"\"\"Creates and returns the joke generation flow.\"\"\"\n    get_topic_node = GetTopicNode()\n    generate_joke_node = GenerateJokeNode()\n    get_feedback_node = GetFeedbackNode()\n\n    get_topic_node >> generate_joke_node\n    generate_joke_node >> get_feedback_node\n    get_feedback_node - \"Disapprove\" >> generate_joke_node\n\n    joke_flow = Flow(start=get_topic_node)\n    return joke_flow ",
      "summary": "**Summary of `flow.py` code unit:**\n\n1. **Primary Purpose:**  \n   The code defines a function to construct a conversational flow for generating jokes, collecting user feedback, and iterating based on feedback using PocketFlow's structures.\n\n2. **Parameters:**  \n   The `create_joke_flow` function does not accept any parameters.\n\n3. **Return Value:**  \n   The function returns a `Flow` object representing the joke generation and feedback process, with nodes connected to define the flow's steps, including a cycle for disapproved jokes.\n\n4. **Internally Called Functions/Methods:**  \n   - `GetTopicNode()` (constructor)\n   - `GenerateJokeNode()` (constructor)\n   - `GetFeedbackNode()` (constructor)\n   - `Flow()` (constructor, from `pocketflow`)\n   - Operator overloading for chaining nodes:\n     - `>>` for connecting nodes sequentially\n     - `- \"Disapprove\" >>` for conditional branching based on feedback\n\n**Overall, it sets up a simple interactive process: get topic \u2192 generate joke \u2192 get feedback \u2192 repeat joke generation if feedback is 'Disapprove'.**"
    },
    "98": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-cli-hitl/main.py",
      "code": "from flow import create_joke_flow\n\ndef main():\n    \"\"\"Main function to run the joke generator application.\"\"\"\n    print(\"Welcome to the Command-Line Joke Generator!\")\n\n    shared = {\n        \"topic\": None,\n        \"current_joke\": None,\n        \"disliked_jokes\": [],\n        \"user_feedback\": None\n    }\n\n    joke_flow = create_joke_flow()\n    joke_flow.run(shared)\n\n    print(\"\\nThanks for using the Joke Generator!\")\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of Code Unit: `/tmp/PocketFlow/cookbook/pocketflow-cli-hitl/main.py`**\n\n1. **Primary Purpose:**  \n   The code serves as the entry point for a command-line joke generator application. It initializes the application state, runs the main joke-generation flow, and manages basic interaction with the user.\n\n2. **Parameters:**  \n   The `main()` function does not take any parameters.\n\n3. **Return Value:**  \n   The `main()` function does not return any value (it returns `None`).\n\n4. **Internal Function/Method Calls:**  \n   - `create_joke_flow()` from the `flow` module: Used to obtain the main joke-generation flow object.\n   - `.run(shared)` method of the joke flow object: Executes the joke-generation workflow using the `shared` dictionary.\n   - Built-in `print()` function: Used to print welcome and exit messages.\n\n**Note:**  \nThe script is designed to be run directly as a Python program. When executed, it starts the joke generator, facilitates the interaction flow, and provides a closing message to the user."
    },
    "99": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-cli-hitl/utils/__init__.py",
      "code": "",
      "summary": "It appears that you have not provided any code from the file /tmp/PocketFlow/cookbook/pocketflow-cli-hitl/utils/__init__.py. Without code to analyze, I cannot fulfill your request.\n\nIf you provide the contents of the code unit, I can summarize it according to your guidelines. Please paste the code or clarify the information you would like analyzed."
    },
    "100": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-cli-hitl/utils/call_llm.py",
      "code": "from anthropic import Anthropic\nimport os\n\ndef call_llm(prompt: str) -> str:\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-anthropic-api-key\")) # Default if key not found\n    response = client.messages.create(\n        model=\"claude-3-haiku-20240307\", # Using a smaller model for jokes\n        max_tokens=150, # Jokes don't need to be very long\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[0].text\n\nif __name__ == \"__main__\":\n    print(\"Testing Anthropic LLM call for jokes:\")\n    joke_prompt = \"Tell me a one-liner joke about a cat.\"\n    print(f\"Prompt: {joke_prompt}\")\n    try:\n        response = call_llm(joke_prompt)\n        print(f\"Response: {response}\")\n    except Exception as e:\n        print(f\"Error calling LLM: {e}\")\n        print(\"Please ensure your ANTHROPIC_API_KEY environment variable is set correctly.\")",
      "summary": "**Summary for `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides a utility function to interact with the Anthropic Claude language model API, enabling users to send prompt strings and receive generated responses\u2014specifically tailored for generating short outputs like jokes.\n\n2. **Parameters:**  \n   - The main function `call_llm` accepts a single parameter:  \n     - `prompt` (str): The input text prompt that the user wishes the language model to respond to.\n\n3. **Return Value:**  \n   - The `call_llm` function returns a string containing the generated text from the Claude language model, extracted from the API response.\n\n4. **Functions/Methods Called Internally:**  \n   - `Anthropic()` class constructor (from the anthropic package) to set up the API client.\n   - `client.messages.create()` to send the prompt to the language model and get a response.\n   - Environment variable access via `os.environ.get()`.\n   - Standard Python built-ins like `print()`.\n   - Error handling via Python's `try/except`.\n   - `response.content[0].text` to retrieve the generated output text from the API response."
    },
    "101": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/nodes.py",
      "code": "from pocketflow import Node\nfrom tools.embeddings import get_embedding\n\nclass EmbeddingNode(Node):\n    \"\"\"Node for getting embeddings from OpenAI API\"\"\"\n    \n    def prep(self, shared):\n        # Get text from shared store\n        return shared.get(\"text\", \"\")\n        \n    def exec(self, text):\n        # Get embedding using tool function\n        return get_embedding(text)\n        \n    def post(self, shared, prep_res, exec_res):\n        # Store embedding in shared store\n        shared[\"embedding\"] = exec_res\n        return \"default\" ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/nodes.py` (Unit: nodes.py):**\n\n1. **Primary purpose:**  \n   This code defines an `EmbeddingNode` class that integrates with the PocketFlow framework to retrieve embeddings for a given text using an external embedding tool (likely OpenAI's API). It is structured for use as a modular node in a workflow, handling text extraction, embedding computation, and storage of results.\n\n2. **Parameters:**  \n   - The `prep` method takes a `shared` dictionary (shared data store) and extracts the `\"text\"` value.  \n   - The `exec` method receives a `text` string to be embedded.  \n   - The `post` method takes `shared`, `prep_res` (the text), and `exec_res` (the embedding).\n\n3. **Return value:**  \n   - `prep` returns the extracted text (string; defaults to empty string if not present).\n   - `exec` returns the embedding obtained from `get_embedding` (format depends on `get_embedding` implementation).\n   - `post` updates the shared dictionary with the embedding and returns the string `\"default\"`.\n\n4. **Internal function/method calls:**  \n   - `shared.get(\"text\", \"\")` (dictionary method)\n   - `get_embedding(text)` (from `tools.embeddings`)\n   - `shared[\"embedding\"] = exec_res` (dictionary assignment)"
    },
    "102": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import EmbeddingNode\n\ndef create_embedding_flow():\n    \"\"\"Create a flow for text embedding\"\"\"\n    # Create embedding node\n    embedding = EmbeddingNode()\n    \n    # Create and return flow\n    return Flow(start=embedding) ",
      "summary": "**Summary of `flow.py` code unit:**\n\n1. **Primary Purpose:**  \n   The code defines a function to construct and return a \"flow\" for generating text embeddings using the `PocketFlow` framework.\n\n2. **Parameters:**  \n   The function `create_embedding_flow()` takes no parameters.\n\n3. **Return Value:**  \n   It returns a `Flow` object that starts from an `EmbeddingNode`, representing a computational pipeline for text embedding.\n\n4. **Internally Called Functions/Methods:**  \n   - `EmbeddingNode()` constructor (from `nodes`)\n   - `Flow()` constructor (from `pocketflow`)\n\n**In summary:**  \nThe code provides a utility function to easily create a text embedding flow by instantiating necessary node and flow objects."
    },
    "103": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/main.py",
      "code": "from flow import create_embedding_flow\n\ndef main():\n    # Create the flow\n    flow = create_embedding_flow()\n    \n    # Example text\n    text = \"What's the meaning of life?\"\n    \n    # Prepare shared data\n    shared = {\"text\": text}\n    \n    # Run the flow\n    flow.run(shared)\n    \n    # Print results\n    print(\"Text:\", text)\n    print(\"Embedding dimension:\", len(shared[\"embedding\"]))\n    print(\"First 5 values:\", shared[\"embedding\"][:5])\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of `main.py`:**\n\n1. **Primary purpose:**  \n   The main purpose of this code unit is to create and execute an embedding flow that processes a sample text and outputs the results, including the embedding vector's dimension and its first few values. It acts as a demonstration or test of the embedding pipeline.\n\n2. **Parameters:**  \n   This script does not take any externally supplied parameters. The function `main()` and the script as a whole use only internally defined variables.\n\n3. **Return value:**  \n   The code does not return a value. Instead, it prints results to the standard output.\n\n4. **Functions/methods called internally:**\n   - `create_embedding_flow()` (imported from `flow`)\n   - `flow.run(shared)` (method on the flow object)\n   - `print()` (built-in Python function)"
    },
    "104": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/utils/__init__.py",
      "code": "",
      "summary": "Since you provided an empty code snippet from /tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/utils/__init__.py (there is no code between \"Code:\" and your instructions), here's the analysis:\n\n1. Primary Purpose  \nThe code unit appears to be an __init__.py file for the utils package of the pocketflow-tool-embeddings Python module. By convention, such files are used to mark directories on disk as Python package directories, and sometimes to perform package-level initialization or expose specific modules.\n\n2. Parameters  \nThere are no parameters, as there is no code defined.\n\n3. Return Value  \nThere is no return value.\n\n4. Internal Function or Method Calls  \nThere are no functions or methods called internally, as the file is empty.\n\nSummary:  \nThis is an empty __init__.py file, present only to indicate that the directory is a Python package. It defines no functions, parameters, or return values, and makes no internal calls."
    },
    "105": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/utils/call_llm.py",
      "code": "import os\nfrom openai import OpenAI\n\n# No need for dotenv if using system environment variables\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef call_llm(prompt):    \n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n    \nif __name__ == \"__main__\":\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt)) ",
      "summary": "**Summary of call_llm.py:**\n\n1. **Primary Purpose:**  \n   This code unit provides a simple interface for sending a text prompt to OpenAI's GPT-4o language model and retrieving its response, using the OpenAI Python SDK.\n\n2. **Parameters:**  \n   The main function `call_llm` takes a single parameter:\n   - `prompt` (str): The text prompt to send to the language model.\n\n3. **Return Value:**  \n   The `call_llm` function returns the content (str) of the first message generated by the language model's response to the prompt.\n\n4. **Internal Functions/Methods Called:**  \n   - `os.getenv(\"OPENAI_API_KEY\")` (to retrieve the API key from environment variables)\n   - `OpenAI(api_key=...)` (to instantiate the OpenAI client)\n   - `client.chat.completions.create(...)` (to call the GPT-4o model with the prompt)\n   - Accesses attributes of the response: `r.choices[0].message.content`\n   - In the script block: built-in `print()` function."
    },
    "106": {
      "unit_name": "embeddings.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tool-embeddings/tools/embeddings.py",
      "code": "from utils.call_llm import client\n\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding",
      "summary": "**Summary of `embeddings.py`:**\n\n1. **Primary Purpose:**  \n   The code defines a utility function to generate a vector embedding for a given text string using the \"text-embedding-ada-002\" model via a client API.\n\n2. **Parameters:**  \n   - `text`: A string containing the input text for which the embedding is to be generated.\n\n3. **Return Value:**  \n   - Returns a vector (typically a list of floats) representing the embedding of the input text.\n\n4. **Internal Function/Method Calls:**  \n   - `client.embeddings.create(...)` from the imported `client` object in `utils.call_llm`.  \n   - Accesses `response.data[0].embedding` to extract the embedding from the API response."
    },
    "107": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-rag/nodes.py",
      "code": "from pocketflow import Node, Flow, BatchNode\nimport numpy as np\nimport faiss\nfrom utils import call_llm, get_embedding, fixed_size_chunk\n\n# Nodes for the offline flow\nclass ChunkDocumentsNode(BatchNode):\n    def prep(self, shared):\n        \"\"\"Read texts from shared store\"\"\"\n        return shared[\"texts\"]\n    \n    def exec(self, text):\n        \"\"\"Chunk a single text into smaller pieces\"\"\"\n        return fixed_size_chunk(text)\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Store chunked texts in the shared store\"\"\"\n        # Flatten the list of lists into a single list of chunks\n        all_chunks = []\n        for chunks in exec_res_list:\n            all_chunks.extend(chunks)\n        \n        # Replace the original texts with the flat list of chunks\n        shared[\"texts\"] = all_chunks\n        \n        print(f\"\u2705 Created {len(all_chunks)} chunks from {len(prep_res)} documents\")\n        return \"default\"\n    \nclass EmbedDocumentsNode(BatchNode):\n    def prep(self, shared):\n        \"\"\"Read texts from shared store and return as an iterable\"\"\"\n        return shared[\"texts\"]\n    \n    def exec(self, text):\n        \"\"\"Embed a single text\"\"\"\n        return get_embedding(text)\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Store embeddings in the shared store\"\"\"\n        embeddings = np.array(exec_res_list, dtype=np.float32)\n        shared[\"embeddings\"] = embeddings\n        print(f\"\u2705 Created {len(embeddings)} document embeddings\")\n        return \"default\"\n\nclass CreateIndexNode(Node):\n    def prep(self, shared):\n        \"\"\"Get embeddings from shared store\"\"\"\n        return shared[\"embeddings\"]\n    \n    def exec(self, embeddings):\n        \"\"\"Create FAISS index and add embeddings\"\"\"\n        print(\"\ud83d\udd0d Creating search index...\")\n        dimension = embeddings.shape[1]\n        \n        # Create a flat L2 index\n        index = faiss.IndexFlatL2(dimension)\n        \n        # Add the embeddings to the index\n        index.add(embeddings)\n        \n        return index\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the index in shared store\"\"\"\n        shared[\"index\"] = exec_res\n        print(f\"\u2705 Index created with {exec_res.ntotal} vectors\")\n        return \"default\"\n\n# Nodes for the online flow\nclass EmbedQueryNode(Node):\n    def prep(self, shared):\n        \"\"\"Get query from shared store\"\"\"\n        return shared[\"query\"]\n    \n    def exec(self, query):\n        \"\"\"Embed the query\"\"\"\n        print(f\"\ud83d\udd0d Embedding query: {query}\")\n        query_embedding = get_embedding(query)\n        return np.array([query_embedding], dtype=np.float32)\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store query embedding in shared store\"\"\"\n        shared[\"query_embedding\"] = exec_res\n        return \"default\"\n\nclass RetrieveDocumentNode(Node):\n    def prep(self, shared):\n        \"\"\"Get query embedding, index, and texts from shared store\"\"\"\n        return shared[\"query_embedding\"], shared[\"index\"], shared[\"texts\"]\n    \n    def exec(self, inputs):\n        \"\"\"Search the index for similar documents\"\"\"\n        print(\"\ud83d\udd0e Searching for relevant documents...\")\n        query_embedding, index, texts = inputs\n        \n        # Search for the most similar document\n        distances, indices = index.search(query_embedding, k=1)\n        \n        # Get the index of the most similar document\n        best_idx = indices[0][0]\n        distance = distances[0][0]\n        \n        # Get the corresponding text\n        most_relevant_text = texts[best_idx]\n        \n        return {\n            \"text\": most_relevant_text,\n            \"index\": best_idx,\n            \"distance\": distance\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store retrieved document in shared store\"\"\"\n        shared[\"retrieved_document\"] = exec_res\n        print(f\"\ud83d\udcc4 Retrieved document (index: {exec_res['index']}, distance: {exec_res['distance']:.4f})\")\n        print(f\"\ud83d\udcc4 Most relevant text: \\\"{exec_res['text']}\\\"\")\n        return \"default\"\n    \nclass GenerateAnswerNode(Node):\n    def prep(self, shared):\n        \"\"\"Get query, retrieved document, and any other context needed\"\"\"\n        return shared[\"query\"], shared[\"retrieved_document\"]\n    \n    def exec(self, inputs):\n        \"\"\"Generate an answer using the LLM\"\"\"\n        query, retrieved_doc = inputs\n        \n        prompt = f\"\"\"\nBriefly answer the following question based on the context provided:\nQuestion: {query}\nContext: {retrieved_doc['text']}\nAnswer:\n\"\"\"\n        \n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store generated answer in shared store\"\"\"\n        shared[\"generated_answer\"] = exec_res\n        print(\"\\n\ud83e\udd16 Generated Answer:\")\n        print(exec_res)\n        return \"default\"\n",
      "summary": "**Summary of the Code Unit: `nodes.py`**\n\n1. **Primary Purpose:**  \n   This code defines a series of modular \"Nodes\" for a Retrieval-Augmented Generation (RAG) pipeline using the PocketFlow framework. The pipeline is designed to process a set of input texts into vector (embedding) representations, build a FAISS search index for similarity matching, handle user queries by embedding and retrieving relevant context, and finally generate answers using a language model. The nodes are structured for both offline batch processing (document chunking, embedding, and indexing) and online query-time operations (query embedding, retrieval, and answer generation).\n\n2. **Brief Description of Parameters:**  \n   - Most node methods (`prep`, `exec`, `post`) work with a shared dictionary named `shared`, which holds persistent state across the pipeline (such as \"texts\", \"embeddings\", \"index\", \"query\", \"retrieved_document\", etc.).\n   - Batch nodes operate over lists of texts (for chunking and embedding).  \n   - Query-time nodes accept a single query or small batch, and referenced context from the shared state.\n\n3. **Brief Description of Return Values:**  \n   - Each node's `post` method returns a simple status string ('default').\n   - Nodes store their intermediate and final outputs (chunks, embeddings, indices, predictions) into `shared`, rather than returning them directly.\n\n4. **Other Functions or Methods Called Internally:**  \n   - `fixed_size_chunk` \u2013 Used to split large texts into smaller, uniform chunk sizes.\n   - `get_embedding` \u2013 Converts text strings into fixed-size vector embeddings.\n   - FAISS library (`faiss.IndexFlatL2`) \u2013 Used to create and query a fast similarity search index.\n   - `call_llm` \u2013 Calls a language model to generate answers from prompt+context pairs.\n   - Standard Python utilities like `numpy` (for arrays) and print statements for logging.\n\n**In Short:**  \n`nodes.py` modularizes each stage of a RAG pipeline\u2014chunking documents, embedding, indexing, retrieving, and answering\u2014by defining reusable node classes that communicate via a shared state dictionary, while calling helper utilities and external libraries for vector operations and language modeling."
    },
    "108": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-rag/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import EmbedDocumentsNode, CreateIndexNode, EmbedQueryNode, RetrieveDocumentNode, ChunkDocumentsNode, GenerateAnswerNode\n\ndef get_offline_flow():\n    # Create offline flow for document indexing\n    chunk_docs_node = ChunkDocumentsNode()\n    embed_docs_node = EmbedDocumentsNode()\n    create_index_node = CreateIndexNode()\n    \n    # Connect the nodes\n    chunk_docs_node >> embed_docs_node >> create_index_node\n    \n    offline_flow = Flow(start=chunk_docs_node)\n    return offline_flow\n\ndef get_online_flow():\n    # Create online flow for document retrieval and answer generation\n    embed_query_node = EmbedQueryNode()\n    retrieve_doc_node = RetrieveDocumentNode()\n    generate_answer_node = GenerateAnswerNode()\n    \n    # Connect the nodes\n    embed_query_node >> retrieve_doc_node >> generate_answer_node\n    \n    online_flow = Flow(start=embed_query_node)\n    return online_flow\n\n# Initialize flows\noffline_flow = get_offline_flow()\nonline_flow = get_online_flow()",
      "summary": "**Summary of flow.py**\n\n1. **Primary Purpose:**  \n   The code defines two processing flows\u2014one for offline document indexing and one for online document retrieval and question answering\u2014by chaining together modular components (nodes) using the PocketFlow framework. It prepares reusable pipelines for a retrieval-augmented generation (RAG) application.\n\n2. **Parameters:**  \n   - The module-level functions `get_offline_flow()` and `get_online_flow()` do not take any parameters.\n\n3. **Return Value:**  \n   - Both `get_offline_flow()` and `get_online_flow()` return an instance of a `Flow` object representing a fully constructed node pipeline (offline or online, respectively).\n\n4. **Functions/Methods Called Internally:**  \n   - `ChunkDocumentsNode()` (constructor)\n   - `EmbedDocumentsNode()` (constructor)\n   - `CreateIndexNode()` (constructor)\n   - `EmbedQueryNode()` (constructor)\n   - `RetrieveDocumentNode()` (constructor)\n   - `GenerateAnswerNode()` (constructor)\n   - `Flow(start=...)` (constructor)\n   - The `>>` operator to connect nodes (assumed to be overloaded to define the flow's wiring)\n\nTwo flow objects\u2014`offline_flow` and `online_flow`\u2014are initialized at the module level for later use."
    },
    "109": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-rag/main.py",
      "code": "import sys\nfrom flow import offline_flow, online_flow\n\ndef run_rag_demo():\n    \"\"\"\n    Run a demonstration of the RAG system.\n    \n    This function:\n    1. Indexes a set of sample documents (offline flow)\n    2. Takes a query from the command line\n    3. Retrieves the most relevant document (online flow)\n    4. Generates an answer using an LLM\n    \"\"\"\n\n    # Sample texts - specialized/fictional content that benefits from RAG\n    texts = [\n        # PocketFlow framework\n        \"\"\"Pocket Flow is a 100-line minimalist LLM framework\n        Lightweight: Just 100 lines. Zero bloat, zero dependencies, zero vendor lock-in.\n        Expressive: Everything you love\u2014(Multi-)Agents, Workflow, RAG, and more.\n        Agentic Coding: Let AI Agents (e.g., Cursor AI) build Agents\u201410x productivity boost!\n        To install, pip install pocketflow or just copy the source code (only 100 lines).\"\"\",\n        \n        # Fictional medical device\n        \"\"\"NeurAlign M7 is a revolutionary non-invasive neural alignment device.\n        Targeted magnetic resonance technology increases neuroplasticity in specific brain regions.\n        Clinical trials showed 72% improvement in PTSD treatment outcomes.\n        Developed by Cortex Medical in 2024 as an adjunct to standard cognitive therapy.\n        Portable design allows for in-home use with remote practitioner monitoring.\"\"\",\n        \n        # Made-up historical event\n        \"\"\"The Velvet Revolution of Caldonia (1967-1968) ended Generalissimo Verak's 40-year rule.\n        Led by poet Eliza Markovian through underground literary societies.\n        Culminated in the Great Silence Protest with 300,000 silent protesters.\n        First democratic elections held in March 1968 with 94% voter turnout.\n        Became a model for non-violent political transitions in neighboring regions.\"\"\",\n        \n        # Fictional technology \n        \"\"\"Q-Mesh is QuantumLeap Technologies' instantaneous data synchronization protocol.\n        Utilizes directed acyclic graph consensus for 500,000 transactions per second.\n        Consumes 95% less energy than traditional blockchain systems.\n        Adopted by three central banks for secure financial data transfer.\n        Released in February 2024 after five years of development in stealth mode.\"\"\",\n        \n        # Made-up scientific research\n        \"\"\"Harlow Institute's Mycelium Strain HI-271 removes 99.7% of PFAS from contaminated soil.\n        Engineered fungi create symbiotic relationships with native soil bacteria.\n        Breaks down \"forever chemicals\" into non-toxic compounds within 60 days.\n        Field tests successfully remediated previously permanently contaminated industrial sites.\n        Deployment costs 80% less than traditional chemical extraction methods.\"\"\"\n    ]\n    \n    print(\"=\" * 50)\n    print(\"PocketFlow RAG Document Retrieval\")\n    print(\"=\" * 50)\n    \n    # Default query about the fictional technology\n    default_query = \"How to install PocketFlow?\"\n    \n    # Get query from command line if provided with --\n    query = default_query\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            query = arg[2:]\n            break\n    \n    # Single shared store for both flows\n    shared = {\n        \"texts\": texts,\n        \"embeddings\": None,\n        \"index\": None,\n        \"query\": query,\n        \"query_embedding\": None,\n        \"retrieved_document\": None,\n        \"generated_answer\": None\n    }\n    \n    # Initialize and run the offline flow (document indexing)\n    offline_flow.run(shared)\n    \n    # Run the online flow to retrieve the most relevant document and generate an answer\n    online_flow.run(shared)\n\n\nif __name__ == \"__main__\":\n    run_rag_demo()",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-rag/main.py`**\n\n1. **Primary Purpose**  \n   The code demonstrates a Retrieval-Augmented Generation (RAG) workflow using the PocketFlow framework. It showcases document indexing, retrieval, and language model answer generation on a set of sample (mostly fictional) texts.\n\n2. **Parameters**  \n   The main function (`run_rag_demo`) does not take any parameters. However, the script can accept a command-line argument prefixed by `--` to specify a custom query string.\n\n3. **Return Value**  \n   Neither the script nor its primary function returns any values; outputs are printed to the console as part of the demonstration.\n\n4. **Functions/Methods Called Internally**  \n   - `offline_flow.run(shared)`: Indexes the sample documents\u2014part of the offline RAG process.\n   - `online_flow.run(shared)`: Processes the query by retrieving the most relevant indexed document and generating an answer\u2014part of the online RAG process.\n\n**Additional notes:**  \n- The script is intended to be run as a standalone module (see the `if __name__ == \"__main__\":` block).\n- All state (texts, indices, query, etc.) is passed via a shared dictionary (`shared`)."
    },
    "110": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-rag/utils.py",
      "code": "import os\nimport numpy as np\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef get_embedding(text):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    \n    # Extract the embedding vector from the response\n    embedding = response.data[0].embedding\n    \n    # Convert to numpy array for consistency with other embedding functions\n    return np.array(embedding, dtype=np.float32)\n\ndef fixed_size_chunk(text, chunk_size=2000):\n    chunks = []\n    for i in range(0, len(text), chunk_size):\n        chunks.append(text[i : i + chunk_size])\n    return chunks\n\nif __name__ == \"__main__\":\n    print(\"=== Testing call_llm ===\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"Response: {response}\")\n\n    print(\"=== Testing embedding function ===\")\n    \n    text1 = \"The quick brown fox jumps over the lazy dog.\"\n    text2 = \"Python is a popular programming language for data science.\"\n    \n    oai_emb1 = get_embedding(text1)\n    oai_emb2 = get_embedding(text2)\n    print(f\"OpenAI Embedding 1 shape: {oai_emb1.shape}\")\n    oai_similarity = np.dot(oai_emb1, oai_emb2)\n    print(f\"OpenAI similarity between texts: {oai_similarity:.4f}\")",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-rag/utils.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides utility functions for interacting with OpenAI's GPT models and embeddings, as well as text chunking, to support workflows involving language model responses and vector-based text processing.\n\n2. **Parameters:**\n   - `call_llm(prompt)`:  \n     - **prompt** (str): The user prompt or message to be sent to the OpenAI language model.\n   - `get_embedding(text)`:  \n     - **text** (str): The input text to be converted into an embedding vector using OpenAI's embedding model.\n   - `fixed_size_chunk(text, chunk_size=2000)`:  \n     - **text** (str): The input text to be split into chunks.  \n     - **chunk_size** (int, optional): The size of each text chunk (default is 2000 characters).\n\n3. **Return Values:**\n   - `call_llm(prompt)`:  \n     - Returns the generated response (str) from the model based on the prompt.\n   - `get_embedding(text)`:  \n     - Returns a numpy array (`np.ndarray` of type float32) representing the text embedding.\n   - `fixed_size_chunk(text, chunk_size=2000)`:  \n     - Returns a list of text chunks (list of strings).\n\n4. **Other Functions/Methods Called Internally:**\n   - `OpenAI` class from the `openai` library (used to create API clients).\n   - `client.chat.completions.create` (for chat completions).\n   - `client.embeddings.create` (for generating embeddings).\n   - `os.environ.get` (for getting the OpenAI API key).\n   - `np.array`, `np.dot` (numpy functions for working with embeddings).\n\n**Note:**  \nThe script also includes example usage/tests in its `__main__` section (for direct execution and verification of utility functions)."
    },
    "111": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-multi-agent/main.py",
      "code": "import asyncio\nfrom pocketflow import AsyncNode, AsyncFlow\nfrom utils import call_llm\n\nclass AsyncHinter(AsyncNode):\n    async def prep_async(self, shared):\n        # Wait for message from guesser (or empty string at start)\n        guess = await shared[\"hinter_queue\"].get()\n        if guess == \"GAME_OVER\":\n            return None\n        return shared[\"target_word\"], shared[\"forbidden_words\"], shared.get(\"past_guesses\", [])\n\n    async def exec_async(self, inputs):\n        if inputs is None:\n            return None\n        target, forbidden, past_guesses = inputs\n        prompt = f\"Generate hint for '{target}'\\nForbidden words: {forbidden}\"\n        if past_guesses:\n            prompt += f\"\\nPrevious wrong guesses: {past_guesses}\\nMake hint more specific.\"\n        prompt += \"\\nUse at most 5 words.\"\n        \n        hint = call_llm(prompt)\n        print(f\"\\nHinter: Here's your hint - {hint}\")\n        return hint\n\n    async def post_async(self, shared, prep_res, exec_res):\n        if exec_res is None:\n            return \"end\"\n        # Send hint to guesser\n        await shared[\"guesser_queue\"].put(exec_res)\n        return \"continue\"\n\nclass AsyncGuesser(AsyncNode):\n    async def prep_async(self, shared):\n        # Wait for hint from hinter\n        hint = await shared[\"guesser_queue\"].get()\n        return hint, shared.get(\"past_guesses\", [])\n\n    async def exec_async(self, inputs):\n        hint, past_guesses = inputs\n        prompt = f\"Given hint: {hint}, past wrong guesses: {past_guesses}, make a new guess. Directly reply a single word:\"\n        guess = call_llm(prompt)\n        print(f\"Guesser: I guess it's - {guess}\")\n        return guess\n\n    async def post_async(self, shared, prep_res, exec_res):\n        # Check if guess is correct\n        if exec_res.lower() == shared[\"target_word\"].lower():\n            print(\"Game Over - Correct guess!\")\n            await shared[\"hinter_queue\"].put(\"GAME_OVER\")\n            return \"end\"\n            \n        # Store the guess in shared state\n        if \"past_guesses\" not in shared:\n            shared[\"past_guesses\"] = []\n        shared[\"past_guesses\"].append(exec_res)\n        \n        # Send guess to hinter\n        await shared[\"hinter_queue\"].put(exec_res)\n        return \"continue\"\n\nasync def main():\n    # Set up game\n    shared = {\n        \"target_word\": \"nostalgic\",\n        \"forbidden_words\": [\"memory\", \"past\", \"remember\", \"feeling\", \"longing\"],\n        \"hinter_queue\": asyncio.Queue(),\n        \"guesser_queue\": asyncio.Queue()\n    }\n    \n    print(\"=========== Taboo Game Starting! ===========\")\n    print(f\"Target word: {shared['target_word']}\")\n    print(f\"Forbidden words: {shared['forbidden_words']}\")\n    print(\"============================================\")\n\n    # Initialize by sending empty guess to hinter\n    await shared[\"hinter_queue\"].put(\"\")\n\n    # Create nodes and flows\n    hinter = AsyncHinter()\n    guesser = AsyncGuesser()\n\n    # Set up flows\n    hinter_flow = AsyncFlow(start=hinter)\n    guesser_flow = AsyncFlow(start=guesser)\n\n    # Connect nodes to themselves for looping\n    hinter - \"continue\" >> hinter\n    guesser - \"continue\" >> guesser\n\n    # Run both agents concurrently\n    await asyncio.gather(\n        hinter_flow.run_async(shared),\n        guesser_flow.run_async(shared)\n    )\n    \n    print(\"=========== Game Complete! ===========\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "summary": "**Summary for `main.py` from `/tmp/PocketFlow/cookbook/pocketflow-multi-agent/`:**\n\n1. **Primary Purpose:**  \n   This code implements an asynchronous, two-agent \"Taboo\" style guessing game using the PocketFlow framework. One agent (\"Hinter\") generates hints for a target word while avoiding a set of forbidden words; the other agent (\"Guesser\") tries to guess the word from the hint. Agents communicate via asynchronous queues, and both use a Language Model (via `call_llm`) for generating hints and guesses.\n\n2. **Parameters:**  \n   - The main execution (`main()`) does not take external parameters; it initializes all required configuration internally.  \n   - Internal async methods in agent classes receive either the shared state dictionary or inputs passed between flow steps.\n\n3. **Return Value:**  \n   - There is no return value from the main function; it runs the game to completion, printing game progress and results to the console.\n   - Step and agent methods may return data used for internal game flow management (e.g., hints, guesses, or signals).\n\n4. **Other Functions/Methods Called Internally:**  \n   - `call_llm` from `utils`: Used to query the language model for hints (Hinter) and guesses (Guesser).\n   - Methods from PocketFlow (`AsyncNode`, `AsyncFlow`) for structuring asynchronous workflows.\n   - Standard library: `asyncio.Queue`, `asyncio.gather`, and `asyncio.run` for asynchronous game flow and concurrency.\n   - Internal class methods:  \n     - `prep_async`, `exec_async`, and `post_async` in both `AsyncHinter` and `AsyncGuesser`.  \n     - The special operator overloads (`-` and `>>`) provided by PocketFlow are used to wire node transitions in the game loop.\n\n**In brief:** The code creates an automated, concurrent word-guessing game where both the hint and the guess are generated by a language model, with agents interacting asynchronously in a PocketFlow framework."
    },
    "112": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-multi-agent/utils.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-multi-agent/utils.py`:**\n\n1. **Primary purpose:**  \n   This code unit provides a utility function to send a prompt to the OpenAI GPT-4o-mini language model via the OpenAI API and return its generated response. It is designed to facilitate easy interaction with an LLM (large language model) in Python scripts.\n\n2. **Parameters:**  \n   - The main function, `call_llm`, takes a single parameter:  \n     - `prompt` (string): The textual input that will be sent as a message to the language model.\n\n3. **Return value:**  \n   - The function returns the language model's response (string) corresponding to the prompt, extracted from the API's completion result.\n\n4. **Functions/methods called internally:**  \n   - `os.environ.get()`: Retrieves the OpenAI API key from environment variables.\n   - `OpenAI(api_key=...)`: Instantiates an OpenAI API client with the specified API key.\n   - `client.chat.completions.create(...)`: Sends the prompt to the specified GPT model and gets a completion.\n   - Accesses `r.choices[0].message.content` to extract the response content.  \n   \nThe script also includes an example usage block that runs `call_llm(\"Tell me a short joke\")` when executed directly."
    },
    "113": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch-node/nodes.py",
      "code": "import pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    \"\"\"BatchNode that processes a large CSV file in chunks.\"\"\"\n    \n    def __init__(self, chunk_size=1000):\n        \"\"\"Initialize with chunk size.\"\"\"\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        \"\"\"Split CSV file into chunks.\n        \n        Returns an iterator of DataFrames, each containing chunk_size rows.\n        \"\"\"\n        # Read CSV in chunks\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        \"\"\"Process a single chunk of the CSV.\n        \n        Args:\n            chunk: pandas DataFrame containing chunk_size rows\n            \n        Returns:\n            dict: Statistics for this chunk\n        \"\"\"\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Combine results from all chunks.\n        \n        Args:\n            prep_res: Original chunks iterator\n            exec_res_list: List of results from each chunk\n            \n        Returns:\n            str: Action to take next\n        \"\"\"\n        # Combine statistics from all chunks\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        total_transactions = sum(res[\"num_transactions\"] for res in exec_res_list)\n        total_amount = sum(res[\"total_amount\"] for res in exec_res_list)\n        \n        # Calculate final statistics\n        shared[\"statistics\"] = {\n            \"total_sales\": total_sales,\n            \"average_sale\": total_amount / total_transactions,\n            \"total_transactions\": total_transactions\n        }\n        \n        return \"show_stats\" ",
      "summary": "**Summary of nodes.py (Unit: CSVProcessor)**\n\n1. **Primary Purpose**  \n   The `CSVProcessor` class is designed to process large CSV files in manageable chunks within a batch processing framework, extracting summary statistics (such as total sales and average sale) from the data.\n\n2. **Parameters**  \n   - `chunk_size` (default: 1000): The number of rows per chunk when splitting the CSV file for batch processing.\n   - `prep(self, shared)`: Receives `shared`, a dictionary containing, at minimum, an `\"input_file\"` key specifying the path to the CSV file.\n   - `exec(self, chunk)`: Receives a `chunk`, which is a pandas DataFrame containing up to `chunk_size` rows of CSV data.\n   - `post(self, shared, prep_res, exec_res_list)`: Receives the `shared` dict, the iterator of CSV chunks (`prep_res`), and a list of dictionaries (`exec_res_list`) produced by `exec` for each chunk.\n\n3. **Return Values**  \n   - `prep(...)`: Returns an iterator over pandas DataFrames (the CSV chunks).\n   - `exec(...)`: Returns a dictionary with statistics for the current chunk: total sales, number of transactions, and total amount.\n   - `post(...)`: Aggregates results from all chunks and stores combined statistics in `shared[\"statistics\"]`; returns the string `\"show_stats\"` to indicate a subsequent action.\n\n4. **Other Functions/Methods Called Internally**\n   - `pd.read_csv` (from pandas): Used with `chunksize` to read the CSV file in pieces.\n   - `super().__init__()`: Initializes the parent `BatchNode` class.\n   - DataFrame methods: `sum()` (on columns) and `len()` (Python built-in for the DataFrame length)."
    },
    "114": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch-node/flow.py",
      "code": "from pocketflow import Flow, Node\nfrom nodes import CSVProcessor\n\nclass ShowStats(Node):\n    \"\"\"Node to display the final statistics.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get statistics from shared store.\"\"\"\n        return shared[\"statistics\"]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Display the statistics.\"\"\"\n        stats = prep_res\n        print(\"\\nFinal Statistics:\")\n        print(f\"- Total Sales: ${stats['total_sales']:,.2f}\")\n        print(f\"- Average Sale: ${stats['average_sale']:,.2f}\")\n        print(f\"- Total Transactions: {stats['total_transactions']:,}\\n\")\n        return \"end\"\n\ndef create_flow():\n    \"\"\"Create and return the processing flow.\"\"\"\n    # Create nodes\n    processor = CSVProcessor(chunk_size=1000)\n    show_stats = ShowStats()\n    \n    # Connect nodes\n    processor - \"show_stats\" >> show_stats\n    \n    # Create and return flow\n    return Flow(start=processor) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-batch-node/flow.py`:**\n\n1. **Primary Purpose:**  \n   The code defines and configures a simple data processing workflow using the PocketFlow framework. It specifies a flow that processes CSV data in chunks and then displays summary statistics about the sales data.\n\n2. **Parameters:**  \n   - The `CSVProcessor` node in the `create_flow` function is instantiated with the parameter `chunk_size=1000`. No parameters are accepted by the top-level code unit or the `create_flow` function.\n\n3. **Return Value:**  \n   - The `create_flow` function returns an instance of `Flow` representing the constructed processing pipeline, with `processor` as the starting node.\n\n4. **Internally Called Functions/Methods:**  \n   - `shared.__getitem__` (from `prep` in `ShowStats`\u2014accesses `shared[\"statistics\"]`)\n   - `print` (used in `ShowStats.post` to display statistics)\n   - The class constructors `CSVProcessor()` and `ShowStats()`\n   - Flow composition operators: `-` and `>>` (used to connect nodes\u2014PocketFlow internal API)\n   - `Flow()` constructor (from `pocketflow`)\n   \nAdditionally, the `ShowStats` node defines `prep` and `post` methods to extract and print summary statistics."
    },
    "115": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch-node/main.py",
      "code": "import os\nfrom flow import create_flow\n\ndef main():\n    \"\"\"Run the batch processing example.\"\"\"\n    # Create data directory if it doesn't exist\n    os.makedirs(\"data\", exist_ok=True)\n    \n    # Create sample CSV if it doesn't exist\n    if not os.path.exists(\"data/sales.csv\"):\n        print(\"Creating sample sales.csv...\")\n        import pandas as pd\n        import numpy as np\n        \n        # Generate sample data\n        np.random.seed(42)\n        n_rows = 10000\n        df = pd.DataFrame({\n            \"date\": pd.date_range(\"2024-01-01\", periods=n_rows),\n            \"amount\": np.random.normal(100, 30, n_rows).round(2),\n            \"product\": np.random.choice([\"A\", \"B\", \"C\"], n_rows)\n        })\n        df.to_csv(\"data/sales.csv\", index=False)\n    \n    # Initialize shared store\n    shared = {\n        \"input_file\": \"data/sales.csv\"\n    }\n    \n    # Create and run flow\n    print(f\"Processing sales.csv in chunks...\")\n    flow = create_flow()\n    flow.run(shared)\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of main.py**\n\n1. **Primary purpose**  \n   The primary purpose of this code unit is to set up and execute a batch data processing workflow on a sample 'sales.csv' dataset. If the data file does not already exist, it generates a synthetic one. It then initializes necessary parameters and runs a data processing \"flow\" using an external function.\n\n2. **Parameters**  \n   This code does not take any parameters; the main function is parameterless and the script is intended to be executed directly.\n\n3. **Return value**  \n   There is no return value. The main function and the script as a whole produce side effects (creating files, printing output, running a flow), but do not return results.\n\n4. **Functions and methods called internally**  \n   - `os.makedirs()` \u2013 To ensure the data directory exists.\n   - `os.path.exists()` \u2013 To check if the sample CSV file exists.\n   - `print()` \u2013 To provide user feedback.\n   - `pandas.DataFrame()` and associated pandas methods \u2013 To construct and save synthetic data.\n   - `numpy` functions \u2013 To generate random data.\n   - `create_flow()` (from `flow`) \u2013 To create a data processing \"flow\" object.\n   - `flow.run()` \u2013 To execute the flow with the shared parameters."
    },
    "116": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-structured-output/main.py",
      "code": "import yaml\nimport os  # Needed for the utils import below\nfrom pocketflow import Node, Flow\nfrom utils import call_llm # Assumes utils.py with call_llm exists\n\nclass ResumeParserNode(Node):\n    def prep(self, shared):\n        \"\"\"Return resume text and target skills from shared state.\"\"\"\n        return {\n            \"resume_text\": shared[\"resume_text\"],\n            \"target_skills\": shared.get(\"target_skills\", [])\n        }\n\n    def exec(self, prep_res):\n        \"\"\"Extract structured data from resume using prompt engineering.\n        Requests YAML output with comments and skill indexes as a list.\n        \"\"\"\n        resume_text = prep_res[\"resume_text\"]\n        target_skills = prep_res[\"target_skills\"]\n\n        # Format skills with indexes for the prompt\n        skill_list_for_prompt = \"\\n\".join([f\"{i}: {skill}\" for i, skill in enumerate(target_skills)])\n\n        # Simplified Prompt focusing on key instructions and format\n        prompt = f\"\"\"\nAnalyze the resume below. Output ONLY the requested information in YAML format.\n\n**Resume:**\n```\n{resume_text}\n```\n\n**Target Skills (use these indexes):**\n```\n{skill_list_for_prompt}\n```\n\n**YAML Output Requirements:**\n- Extract `name` (string).\n- Extract `email` (string).\n- Extract `experience` (list of objects with `title` and `company`).\n- Extract `skill_indexes` (list of integers found from the Target Skills list).\n- **Add a YAML comment (`#`) explaining the source BEFORE `name`, `email`, `experience`, each item in `experience`, and `skill_indexes`.**\n\n**Example Format:**\n```yaml\n# Found name at top\nname: Jane Doe\n# Found email in contact info\nemail: jane@example.com\n# Experience section analysis\nexperience:\n  # First job listed\n  - title: Manager\n    company: Corp A\n  # Second job listed\n  - title: Assistant\n    company: Corp B\n# Skills identified from the target list based on resume content\nskill_indexes:\n  # Found 0 at top  \n  - 0\n  # Found 2 in experience\n  - 2\n```\n\nGenerate the YAML output now:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Minimal YAML Extraction ---\n        # Assumes LLM correctly uses ```yaml blocks\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        structured_result = yaml.safe_load(yaml_str)\n        # --- End Minimal Extraction ---\n\n        # --- Basic Validation ---\n        assert structured_result is not None, \"Validation Failed: Parsed YAML is None\"\n        assert \"name\" in structured_result, \"Validation Failed: Missing 'name'\"\n        assert \"email\" in structured_result, \"Validation Failed: Missing 'email'\"\n        assert \"experience\" in structured_result, \"Validation Failed: Missing 'experience'\"\n        assert isinstance(structured_result.get(\"experience\"), list), \"Validation Failed: 'experience' is not a list\"\n        assert \"skill_indexes\" in structured_result, \"Validation Failed: Missing 'skill_indexes'\"\n        skill_indexes_val = structured_result.get(\"skill_indexes\")\n        assert skill_indexes_val is None or isinstance(skill_indexes_val, list), \"Validation Failed: 'skill_indexes' is not a list or None\"\n        if isinstance(skill_indexes_val, list):\n             for index in skill_indexes_val:\n                 assert isinstance(index, int), f\"Validation Failed: Skill index '{index}' is not an integer\"\n        # --- End Basic Validation ---\n\n        return structured_result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store structured data and print it.\"\"\"\n        shared[\"structured_data\"] = exec_res\n\n        print(\"\\n=== STRUCTURED RESUME DATA (Comments & Skill Index List) ===\\n\")\n        # Dump YAML ensuring block style for readability\n        print(yaml.dump(exec_res, sort_keys=False, allow_unicode=True, default_flow_style=None))\n        print(\"\\n============================================================\\n\")\n        print(\"\u2705 Extracted resume information.\")\n\n\n# === Main Execution Logic ===\nif __name__ == \"__main__\":\n    print(\"=== Resume Parser - Structured Output with Indexes & Comments ===\\n\")\n\n    # --- Configuration ---\n    target_skills_to_find = [\n        \"Team leadership & management\", # 0\n        \"CRM software\",                 # 1\n        \"Project management\",           # 2\n        \"Public speaking\",              # 3\n        \"Microsoft Office\",             # 4\n        \"Python\",                       # 5\n        \"Data Analysis\"                 # 6\n    ]\n    resume_file = 'data.txt' # Assumes data.txt contains the resume\n\n    # --- Prepare Shared State ---\n    shared = {}\n    try:\n        with open(resume_file, 'r') as file:\n            shared[\"resume_text\"] = file.read()\n    except FileNotFoundError:\n        print(f\"Error: Resume file '{resume_file}' not found.\")\n        exit(1) # Exit if resume file is missing\n\n    shared[\"target_skills\"] = target_skills_to_find\n\n    # --- Define and Run Flow ---\n    parser_node = ResumeParserNode(max_retries=3, wait=10)\n    flow = Flow(start=parser_node)\n    flow.run(shared) # Execute the parsing node\n\n    # --- Display Found Skills ---\n    if \"structured_data\" in shared and \"skill_indexes\" in shared[\"structured_data\"]:\n         print(\"\\n--- Found Target Skills (from Indexes) ---\")\n         found_indexes = shared[\"structured_data\"][\"skill_indexes\"]\n         if found_indexes: # Check if the list is not empty or None\n             for index in found_indexes:\n                 if 0 <= index < len(target_skills_to_find):\n                     print(f\"- {target_skills_to_find[index]} (Index: {index})\")\n                 else:\n                     print(f\"- Warning: Found invalid skill index {index}\")\n         else:\n             print(\"No target skills identified from the list.\")\n         print(\"----------------------------------------\\n\")",
      "summary": "**Summary of main.py**\n\n1. **Primary Purpose:**  \n   This script extracts structured information (name, email, job experience, and target skill indexes) from a resume using an LLM (Large Language Model) prompt. The LLM is instructed to return YAML with explanatory comments, highlighting where the information was found in the resume and which target skills matched.\n\n2. **Parameters:**  \n   - **Command-line/Config Inputs:**  \n     - The filename of the resume (`data.txt`, hardcoded in the script).\n     - A list of `target_skills_to_find`, also defined in the script.\n   - **Class Parameters:**  \n     - `ResumeParserNode` is instantiated with `max_retries=3` and `wait=10` (used by its parent `Node` class, not shown here).\n   - **Function/Method Parameters:**  \n     - `prep(self, shared)`: Receives a shared dictionary with `resume_text` and (optionally) `target_skills`.\n     - `exec(self, prep_res)`: Receives result of `prep`, containing `resume_text` and `target_skills`.\n     - `post(self, shared, prep_res, exec_res)`: Receives shared state, prep result, and execution result.\n\n3. **Return Value:**  \n   - `exec(self, prep_res)` returns a Python `dict` containing the parsed resume information (`name`, `email`, `experience` list, `skill_indexes` list).\n   - The main script prints the structured results but does not return a value.\n\n4. **Internal Functions/Methods Called:**  \n   - `call_llm(prompt)` _(from utils.py)_: Sends a crafted prompt to a Large Language Model and gets the response.\n   - `yaml.safe_load(yaml_str)`: Parses the returned YAML string from the LLM.\n   - `yaml.dump(exec_res, ...)`: Outputs structured YAML for readability.\n   - Methods of the base classes/Flow framework:\n     - `Node.prep`, `Node.exec`, `Node.post`\n     - `Flow.run(shared)`\n   - **Standard Functions**:  \n     - `open(resume_file)` (to load the resume)  \n     - `print` and `exit`\n   - **Other**:  \n     - A series of `assert` statements for validation.\n\n**In short:**  \nThe script defines a flow node that uses a prompt-engineered LLM call to extract (with YAML-format comments) structured information from an input resume, validates the output, displays it, and highlights which target skills (by index) were found."
    },
    "117": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-structured-output/utils.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-structured-output/utils.py`:**\n\n1. **Primary purpose:**  \n   The code provides a utility function to send a prompt to OpenAI's GPT-4o model and return the generated response. It is intended as a helper for interacting with the OpenAI API in a conversational manner.\n\n2. **Parameters:**  \n   - The main function, `call_llm`, has a single parameter:\n     - `prompt`: A string containing the input prompt to send to the language model.\n\n3. **Return value:**  \n   - `call_llm` returns a string containing the content of the language model's reply to the provided prompt.\n\n4. **List of other functions or methods called internally:**  \n   - `os.environ.get()` : Retrieves the OpenAI API key from environment variables.\n   - `OpenAI()` : Instantiates the OpenAI API client.\n   - `client.chat.completions.create()` : Sends the prompt to the GPT-4o model and gets a response.\n   - Standard Python `print()` (only in the example usage block)."
    },
    "118": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-memory/nodes.py",
      "code": "from pocketflow import Node\nfrom utils.vector_index import create_index, add_vector, search_vectors\nfrom utils.call_llm import call_llm\nfrom utils.get_embedding import get_embedding\n\nclass GetUserQuestionNode(Node):\n    def prep(self, shared):\n        \"\"\"Initialize messages if first run\"\"\"\n        if \"messages\" not in shared:\n            shared[\"messages\"] = []\n            print(\"Welcome to the interactive chat! Type 'exit' to end the conversation.\")\n        \n        return None\n    \n    def exec(self, _):\n        \"\"\"Get user input interactively\"\"\"\n        # Get interactive input from user\n        user_input = input(\"\\nYou: \")\n            \n        # Check if user wants to exit\n        if user_input.lower() == 'exit':\n            return None\n            \n        return user_input\n    \n    def post(self, shared, prep_res, exec_res):\n        # If exec_res is None, the user wants to exit\n        if exec_res is None:\n            print(\"\\nGoodbye!\")\n            return None  # End the conversation\n            \n        # Add user message to current messages\n        shared[\"messages\"].append({\"role\": \"user\", \"content\": exec_res})\n        \n        return \"retrieve\"\n\nclass AnswerNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare context for the LLM\"\"\"\n        if not shared.get(\"messages\"):\n            return None\n            \n        # 1. Get the last 3 conversation pairs (or fewer if not available)\n        recent_messages = shared[\"messages\"][-6:] if len(shared[\"messages\"]) > 6 else shared[\"messages\"]\n        \n        # 2. Add the retrieved relevant conversation if available\n        context = []\n        if shared.get(\"retrieved_conversation\"):\n            # Add a system message to indicate this is a relevant past conversation\n            context.append({\n                \"role\": \"system\", \n                \"content\": \"The following is a relevant past conversation that may help with the current query:\"\n            })\n            context.extend(shared[\"retrieved_conversation\"])\n            context.append({\n                \"role\": \"system\", \n                \"content\": \"Now continue the current conversation:\"\n            })\n        \n        # 3. Add the recent messages\n        context.extend(recent_messages)\n        \n        return context\n    \n    def exec(self, messages):\n        \"\"\"Generate a response using the LLM\"\"\"\n        if messages is None:\n            return None\n        \n        # Call LLM with the context\n        response = call_llm(messages)\n        return response\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Process the LLM response\"\"\"\n        if prep_res is None or exec_res is None:\n            return None  # End the conversation\n        \n        # Print the assistant's response\n        print(f\"\\nAssistant: {exec_res}\")\n        \n        # Add assistant message to history\n        shared[\"messages\"].append({\"role\": \"assistant\", \"content\": exec_res})\n        \n        # If we have more than 6 messages (3 conversation pairs), archive the oldest pair\n        if len(shared[\"messages\"]) > 6:\n            return \"embed\"\n        \n        # We only end if the user explicitly typed 'exit'\n        # Even if last_question is set, we continue in interactive mode\n        return \"question\"\n\nclass EmbedNode(Node):\n    def prep(self, shared):\n        \"\"\"Extract the oldest conversation pair for embedding\"\"\"\n        if len(shared[\"messages\"]) <= 6:\n            return None\n            \n        # Extract the oldest user-assistant pair\n        oldest_pair = shared[\"messages\"][:2]\n        # Remove them from current messages\n        shared[\"messages\"] = shared[\"messages\"][2:]\n        \n        return oldest_pair\n    \n    def exec(self, conversation):\n        \"\"\"Embed a conversation\"\"\"\n        if not conversation:\n            return None\n            \n        # Combine user and assistant messages into a single text for embedding\n        user_msg = next((msg for msg in conversation if msg[\"role\"] == \"user\"), {\"content\": \"\"})\n        assistant_msg = next((msg for msg in conversation if msg[\"role\"] == \"assistant\"), {\"content\": \"\"})\n        combined = f\"User: {user_msg['content']} Assistant: {assistant_msg['content']}\"\n        \n        # Generate embedding\n        embedding = get_embedding(combined)\n        \n        return {\n            \"conversation\": conversation,\n            \"embedding\": embedding\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the embedding and add to index\"\"\"\n        if not exec_res:\n            # If there's nothing to embed, just continue with the next question\n            return \"question\"\n            \n        # Initialize vector index if not exist\n        if \"vector_index\" not in shared:\n            shared[\"vector_index\"] = create_index()\n            shared[\"vector_items\"] = []  # Track items separately\n            \n        # Add the embedding to the index and store the conversation\n        position = add_vector(shared[\"vector_index\"], exec_res[\"embedding\"])\n        shared[\"vector_items\"].append(exec_res[\"conversation\"])\n        \n        print(f\"\u2705 Added conversation to index at position {position}\")\n        print(f\"\u2705 Index now contains {len(shared['vector_items'])} conversations\")\n        \n        # Continue with the next question\n        return \"question\"\n\nclass RetrieveNode(Node):\n    def prep(self, shared):\n        \"\"\"Get the current query for retrieval\"\"\"\n        if not shared.get(\"messages\"):\n            return None\n            \n        # Get the latest user message for searching\n        latest_user_msg = next((msg for msg in reversed(shared[\"messages\"]) \n                                if msg[\"role\"] == \"user\"), {\"content\": \"\"})\n        \n        # Check if we have a vector index with items\n        if (\"vector_index\" not in shared or \n            \"vector_items\" not in shared or \n            len(shared[\"vector_items\"]) == 0):\n            return None\n            \n        return {\n            \"query\": latest_user_msg[\"content\"],\n            \"vector_index\": shared[\"vector_index\"],\n            \"vector_items\": shared[\"vector_items\"]\n        }\n    \n    def exec(self, inputs):\n        \"\"\"Find the most relevant past conversation\"\"\"\n        if not inputs:\n            return None\n            \n        query = inputs[\"query\"]\n        vector_index = inputs[\"vector_index\"]\n        vector_items = inputs[\"vector_items\"]\n        \n        print(f\"\ud83d\udd0d Finding relevant conversation for: {query[:30]}...\")\n        \n        # Create embedding for the query\n        query_embedding = get_embedding(query)\n        \n        # Search for the most similar conversation\n        indices, distances = search_vectors(vector_index, query_embedding, k=1)\n        \n        if not indices:\n            return None\n            \n        # Get the corresponding conversation\n        conversation = vector_items[indices[0]]\n        \n        return {\n            \"conversation\": conversation,\n            \"distance\": distances[0]\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the retrieved conversation\"\"\"\n        if exec_res is not None:\n            shared[\"retrieved_conversation\"] = exec_res[\"conversation\"]\n            print(f\"\ud83d\udcc4 Retrieved conversation (distance: {exec_res['distance']:.4f})\")\n        else:\n            shared[\"retrieved_conversation\"] = None\n        \n        return \"answer\"",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary purpose:**  \nThis code unit defines the main node classes for an interactive chat application with long-term memory. Nodes manage the conversation flow, interactively obtain user questions, generate AI responses, store conversation pairs as vector embeddings for retrieval, and retrieve the most relevant past conversation to provide context for responses. The node system allows for a looped flow: questioning, answering, archiving, and recalling conversations.\n\n2. **Parameters:**  \nEach node class defines methods (`prep`, `exec`, `post`) that interact using:\n\n- `shared`: a shared dictionary containing conversation state, message history, vector indexes, and retrieved context.  \n- Various intermediate results passed between `prep`, `exec`, and `post` methods (e.g., user input, message lists, embedding results).\n\n3. **Return values:**  \nEach node's `prep`, `exec`, and `post` methods return:\n\n- Typically, processed data for the next stage or action (\"question\", \"retrieve\", \"embed\", etc.), or `None` to indicate an exit or no-op.\n- The execution methods might return user input strings, LLM responses, embedding information, or past conversation context, depending on the node.\n\n4. **Functions/methods called internally:**\n\n- **From `utils/vector_index`:**\n  - `create_index`\n  - `add_vector`\n  - `search_vectors`\n\n- **From `utils/call_llm`:**\n  - `call_llm`\n\n- **From `utils/get_embedding`:**\n  - `get_embedding`\n\n- **Standard Python:**\n  - `input`\n  - `print`\n  - List slicing and comprehensions\n\nThe node classes defined and orchestrated in this file are: `GetUserQuestionNode`, `AnswerNode`, `EmbedNode`, and `RetrieveNode`. Each handles a segment of the overall chat/memory functionality by calling the above functions and managing conversation state through the `shared` dictionary."
    },
    "119": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-memory/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import GetUserQuestionNode, RetrieveNode, AnswerNode, EmbedNode\n\ndef create_chat_flow():\n    # Create the nodes\n    question_node = GetUserQuestionNode()\n    retrieve_node = RetrieveNode()\n    answer_node = AnswerNode()\n    embed_node = EmbedNode()\n    \n    # Connect the flow:\n    # 1. Start with getting a question\n    # 2. Retrieve relevant conversations\n    # 3. Generate an answer\n    # 4. Optionally embed old conversations\n    # 5. Loop back to get the next question\n\n    # Main flow path\n    question_node - \"retrieve\" >> retrieve_node\n    retrieve_node - \"answer\" >> answer_node\n    \n    # When we need to embed old conversations\n    answer_node - \"embed\" >> embed_node\n    \n    # Loop back for next question\n    answer_node - \"question\" >> question_node\n    embed_node - \"question\" >> question_node\n    \n    # Create the flow starting with question node\n    return Flow(start=question_node)\n\n# Initialize the flow\nchat_flow = create_chat_flow() ",
      "summary": "**Summary of flow.py:**\n\n1. **Primary Purpose:**  \n   This code defines and initializes a chat conversation flow for a chatbot, using modular nodes to manage chatbot interactions such as receiving user questions, retrieving relevant information, generating answers, and embedding conversation history.\n\n2. **Parameters:**  \n   The main function, `create_chat_flow()`, does **not** take any parameters.\n\n3. **Return Value:**  \n   `create_chat_flow()` returns a `Flow` object (from the `pocketflow` module) that represents the configured sequence and logic of the chat conversation.\n\n4. **Other Functions or Methods Called Internally:**\n   - `GetUserQuestionNode()` constructor (from `nodes`)\n   - `RetrieveNode()` constructor (from `nodes`)\n   - `AnswerNode()` constructor (from `nodes`)\n   - `EmbedNode()` constructor (from `nodes`)\n   - Flow connection methods using the custom operators (`- \"event\" >> node`)\n   - `Flow(start=question_node)` (constructor from `pocketflow`)"
    },
    "120": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-memory/main.py",
      "code": "from flow import chat_flow\n\ndef run_chat_memory_demo():\n    \"\"\"\n    Run an interactive chat interface with memory retrieval.\n    \n    Features:\n    1. Maintains a window of the 3 most recent conversation pairs\n    2. Archives older conversations with embeddings\n    3. Retrieves 1 relevant past conversation when needed\n    4. Total context to LLM: 3 recent pairs + 1 retrieved pair\n    \"\"\"\n    \n    print(\"=\" * 50)\n    print(\"PocketFlow Chat with Memory\")\n    print(\"=\" * 50)\n    print(\"This chat keeps your 3 most recent conversations\")\n    print(\"and brings back relevant past conversations when helpful\")\n    print(\"Type 'exit' to end the conversation\")\n    print(\"=\" * 50)\n    \n    # Run the chat flow\n    chat_flow.run({})\n\nif __name__ == \"__main__\":\n    run_chat_memory_demo()",
      "summary": "**Summary of main.py**\n\n1. **Primary purpose:**  \n   This code serves as the entry point for launching an interactive chat demo that showcases conversational memory. It maintains recent conversation context, archives older exchanges with embeddings, and retrieves relevant past conversations as needed.\n\n2. **Parameters:**  \n   - The main function (`run_chat_memory_demo`) does not take any parameters.\n\n3. **Return value:**  \n   - The function and script do not return any value.\n\n4. **Internal function/method calls:**  \n   - Calls `chat_flow.run({})` from the imported `flow` module (specifically, the `chat_flow` object).\n   - Uses Python's built-in `print` function to display instructions and interface information.\n\n**Overall:**  \nWhen executed, this script starts a memory-augmented chat interface. The core chat logic is handled externally by `chat_flow.run({})`. The script itself presents user instructions and manages execution flow; it does not process input/output directly or use/return function parameters or values."
    },
    "121": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-memory/utils/__init__.py",
      "code": "\n",
      "summary": "The code unit you provided from /tmp/PocketFlow/cookbook/pocketflow-chat-memory/utils/__init__.py does not contain any code (it is empty).\n\nAnswers to your specific questions:\n\n1. The primary purpose of the code unit:\n   - As an empty __init__.py file, its primary purpose is to indicate to Python that the utils directory is a package. This allows modules within this directory to be imported.\n\n2. A brief description of its parameters (if any):\n   - There are no parameters, as the file does not define any functions, classes, or executable statements.\n\n3. A brief description of its return value (if any):\n   - It has no return value.\n\n4. A list of other functions or methods it calls internally:\n   - It calls no functions or methods internally.\n\nIn summary: This empty __init__.py file is used only to mark its directory as a Python package and does not contain any executable code or definitions."
    },
    "122": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-memory/utils/call_llm.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef call_llm(messages):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test the LLM call\n    messages = [{\"role\": \"user\", \"content\": \"In a few words, what's the meaning of life?\"}]\n    response = call_llm(messages)\n    print(f\"Prompt: {messages[0]['content']}\")\n    print(f\"Response: {response}\") ",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   This code provides a utility function to interact with OpenAI's GPT-4o language model API, sending a list of chat messages and retrieving the model's response. It allows simple programmatic access to the chatbot for integration into other Python applications.\n\n2. **Parameters:**  \n   - The main function, `call_llm`, accepts a single parameter:  \n     - `messages`: a list of dictionaries representing the conversation history, with each dictionary containing a \"role\" (such as \"user\" or \"assistant\") and \"content\" (the message text).\n\n3. **Return Value:**  \n   - The function returns the generated text response from the language model, specifically the content of the first completion's message.\n\n4. **Internally Called Functions/Methods:**  \n   - `os.environ.get()` (to retrieve the OpenAI API key from environment variables)\n   - `OpenAI()` constructor (to create an API client)\n   - `client.chat.completions.create()` (to send the chat messages to the model and get a completion)\n   - Standard Python `print()` (only in the test block under `if __name__ == \"__main__\":`)"
    },
    "123": {
      "unit_name": "get_embedding.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-memory/utils/get_embedding.py",
      "code": "import os\nimport numpy as np\nfrom openai import OpenAI\n\ndef get_embedding(text):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"YOUR_API_KEY\"))\n    \n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    \n    # Extract the embedding vector from the response\n    embedding = response.data[0].embedding\n    \n    # Convert to numpy array for consistency with other embedding functions\n    return np.array(embedding, dtype=np.float32)\n\n\nif __name__ == \"__main__\":\n    # Test the embedding function\n    text1 = \"The quick brown fox jumps over the lazy dog.\"\n    text2 = \"Python is a popular programming language for data science.\"\n    \n    emb1 = get_embedding(text1)\n    emb2 = get_embedding(text2)\n    \n    print(f\"Embedding 1 shape: {emb1.shape}\")\n    print(f\"Embedding 2 shape: {emb2.shape}\")\n    \n    # Calculate similarity (dot product)\n    similarity = np.dot(emb1, emb2)\n    print(f\"Similarity between texts: {similarity:.4f}\") ",
      "summary": "**Summary for `/tmp/PocketFlow/cookbook/pocketflow-chat-memory/utils/get_embedding.py`:**\n\n1. **Primary Purpose**  \n   This code provides a utility function to obtain a numerical embedding vector for a given text string using OpenAI's \"text-embedding-ada-002\" model. Embeddings can be used for tasks such as calculating semantic similarity between texts.\n\n2. **Parameters**  \n   The main function, `get_embedding`, takes one parameter:\n   - `text`: A string consisting of the text to be embedded.\n\n3. **Return Value**  \n   The `get_embedding` function returns a NumPy array (`np.array`) representing the embedding vector (of type `float32`) for the input text.\n\n4. **Functions/Methods Called Internally**  \n   - `OpenAI()`: Initializes the OpenAI client with an API key.\n   - `client.embeddings.create(model, input)`: Calls the OpenAI API to create an embedding for the text.\n   - `os.environ.get()`: Reads the API key from environment variables.\n   - `np.array()`: Converts the embedding list to a NumPy array.\n   - `np.dot()`: Computes the dot product (used in the example/test block).\n   - Standard Python list/dictionary accesses (e.g., `response.data[0].embedding`)."
    },
    "124": {
      "unit_name": "vector_index.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-chat-memory/utils/vector_index.py",
      "code": "import numpy as np\nimport faiss\n\ndef create_index(dimension=1536):\n    return faiss.IndexFlatL2(dimension)\n\ndef add_vector(index, vector):\n    # Make sure the vector is a numpy array with the right shape for FAISS\n    vector = np.array(vector).reshape(1, -1).astype(np.float32)\n    \n    # Add the vector to the index\n    index.add(vector)\n    \n    # Return the position (index.ntotal is the total number of vectors in the index)\n    return index.ntotal - 1\n\ndef search_vectors(index, query_vector, k=1):\n    \"\"\"Search for the k most similar vectors to the query vector\n    \n    Args:\n        index: The FAISS index\n        query_vector: The query vector (numpy array or list)\n        k: Number of results to return (default: 1)\n        \n    Returns:\n        tuple: (indices, distances) where:\n            - indices is a list of positions in the index\n            - distances is a list of the corresponding distances\n    \"\"\"\n    # Make sure we don't try to retrieve more vectors than exist in the index\n    k = min(k, index.ntotal)\n    if k == 0:\n        return [], []\n        \n    # Make sure the query is a numpy array with the right shape for FAISS\n    query_vector = np.array(query_vector).reshape(1, -1).astype(np.float32)\n    \n    # Search the index\n    distances, indices = index.search(query_vector, k)\n    \n    return indices[0].tolist(), distances[0].tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a new index\n    index = create_index(dimension=3)\n    \n    # Add some random vectors and track them separately\n    items = []\n    for i in range(5):\n        vector = np.random.random(3)\n        position = add_vector(index, vector)\n        items.append(f\"Item {i}\")\n        print(f\"Added vector at position {position}\")\n        \n    print(f\"Index contains {index.ntotal} vectors\")\n    \n    # Search for a similar vector\n    query = np.random.random(3)\n    indices, distances = search_vectors(index, query, k=2)\n    \n    print(\"Query:\", query)\n    print(\"Found indices:\", indices)\n    print(\"Distances:\", distances)\n    print(\"Retrieved items:\", [items[idx] for idx in indices]) ",
      "summary": "**Summary of `vector_index.py`**\n\n1. **Primary purpose:**  \n   The code provides utility functions for creating and using a FAISS-based vector index. It enables storage, retrieval, and similarity search of high-dimensional vectors, suitable for applications like semantic search or embedding-based memory.\n\n2. **Brief parameter description:**  \n   - `create_index(dimension=1536)`:  \n     - `dimension` (int): The dimensionality of vectors the index will store (defaults to 1536).\n   - `add_vector(index, vector)`:  \n     - `index`: The FAISS index object.\n     - `vector`: The vector to add (list or numpy array).\n   - `search_vectors(index, query_vector, k=1)`:  \n     - `index`: The FAISS index object.\n     - `query_vector`: The vector to query (list or numpy array).\n     - `k` (int): Number of nearest vectors to retrieve (defaults to 1).\n\n3. **Brief description of return values:**  \n   - `create_index`: Returns a new FAISS L2 index.\n   - `add_vector`: Returns the position (int) at which the vector was added to the index.\n   - `search_vectors`: Returns a tuple `(indices, distances)` where:\n     - `indices`: List of index positions for the most similar vectors.\n     - `distances`: List of L2 distances for the corresponding vectors.\n\n4. **Other functions or methods called internally:**\n   - `np.array`, `np.float32`, `np.random.random`, `np.reshape` (from `numpy`)\n   - `faiss.IndexFlatL2`, `index.add()`, `index.search()`, `index.ntotal` (from `faiss`)\n   - Standard Python list operations and print functions (in the example usage block)"
    },
    "125": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/nodes.py",
      "code": "import numpy as np\nimport scipy.io.wavfile\nimport io\nimport soundfile # For converting MP3 bytes to NumPy array\n\nfrom pocketflow import Node\nfrom utils.audio_utils import record_audio, play_audio_data\nfrom utils.speech_to_text import speech_to_text_api\nfrom utils.call_llm import call_llm\nfrom utils.text_to_speech import text_to_speech_api\n\nclass CaptureAudioNode(Node):\n    \"\"\"Records audio input from the user using VAD.\"\"\"\n    def exec(self, _): # prep_res is not used as per design\n        print(\"\\nListening for your query...\")\n        audio_data, sample_rate = record_audio()\n        if audio_data is None:\n            return None, None\n        return audio_data, sample_rate\n\n    def post(self, shared, prep_res, exec_res):\n        audio_numpy_array, sample_rate = exec_res\n        if audio_numpy_array is None:\n            shared[\"user_audio_data\"] = None\n            shared[\"user_audio_sample_rate\"] = None\n            print(\"CaptureAudioNode: Failed to capture audio.\")\n            return \"end_conversation\" \n\n        shared[\"user_audio_data\"] = audio_numpy_array\n        shared[\"user_audio_sample_rate\"] = sample_rate\n        print(f\"Audio captured ({len(audio_numpy_array)/sample_rate:.2f}s), proceeding to STT.\")\n\nclass SpeechToTextNode(Node):\n    \"\"\"Converts the recorded in-memory audio to text.\"\"\"\n    def prep(self, shared):\n        user_audio_data = shared.get(\"user_audio_data\")\n        user_audio_sample_rate = shared.get(\"user_audio_sample_rate\")\n        if user_audio_data is None or user_audio_sample_rate is None:\n            print(\"SpeechToTextNode: No audio data to process.\")\n            return None # Signal to skip exec\n        return user_audio_data, user_audio_sample_rate\n\n    def exec(self, prep_res):\n        if prep_res is None:\n            return None # Skip if no audio data\n\n        audio_numpy_array, sample_rate = prep_res\n        \n        # Convert NumPy array to WAV bytes for the API\n        byte_io = io.BytesIO()\n        scipy.io.wavfile.write(byte_io, sample_rate, audio_numpy_array)\n        wav_bytes = byte_io.getvalue()\n        \n        print(\"Converting speech to text...\")\n        transcribed_text = speech_to_text_api(audio_data=wav_bytes, sample_rate=sample_rate)\n        return transcribed_text\n\n    def post(self, shared, prep_res, exec_res):\n        if exec_res is None:\n            print(\"SpeechToTextNode: STT API returned no text.\")\n            return \"end_conversation\" \n\n        transcribed_text = exec_res\n        print(f\"User: {transcribed_text}\")\n        \n        if \"chat_history\" not in shared:\n            shared[\"chat_history\"] = []\n        shared[\"chat_history\"].append({\"role\": \"user\", \"content\": transcribed_text})\n        \n        shared[\"user_audio_data\"] = None\n        shared[\"user_audio_sample_rate\"] = None\n        return \"default\"\n\nclass QueryLLMNode(Node):\n    \"\"\"Gets a response from the LLM.\"\"\"\n    def prep(self, shared):\n        chat_history = shared.get(\"chat_history\", [])\n        \n        if not chat_history:\n            print(\"QueryLLMNode: Chat history is empty. Skipping LLM call.\")\n            return None \n        \n        return chat_history\n\n    def exec(self, prep_res):\n        if prep_res is None: \n            return None \n\n        chat_history = prep_res\n        print(\"Sending query to LLM...\")\n        llm_response_text = call_llm(messages=chat_history)\n        return llm_response_text\n\n    def post(self, shared, prep_res, exec_res):\n        if exec_res is None:\n            print(\"QueryLLMNode: LLM API returned no response.\")\n            return \"end_conversation\" \n\n        llm_response_text = exec_res\n        print(f\"LLM: {llm_response_text}\")\n        \n        shared[\"chat_history\"].append({\"role\": \"assistant\", \"content\": llm_response_text})\n        return \"default\"\n\nclass TextToSpeechNode(Node):\n    \"\"\"Converts the LLM's text response into speech and plays it.\"\"\"\n    def prep(self, shared):\n        chat_history = shared.get(\"chat_history\", [])\n        if not chat_history:\n            print(\"TextToSpeechNode: Chat history is empty. No LLM response to synthesize.\")\n            return None\n        \n        last_message = chat_history[-1]\n        if last_message.get(\"role\") == \"assistant\" and last_message.get(\"content\"):\n            return last_message.get(\"content\")\n        else:\n            print(\"TextToSpeechNode: Last message not from assistant or no content. Skipping TTS.\")\n            return None\n\n    def exec(self, prep_res):\n        if prep_res is None:\n            return None, None\n            \n        llm_text_response = prep_res\n        print(\"Converting LLM response to speech...\")\n        llm_audio_bytes, llm_sample_rate = text_to_speech_api(llm_text_response)\n        return llm_audio_bytes, llm_sample_rate\n\n    def post(self, shared, prep_res, exec_res):\n        if exec_res is None or exec_res[0] is None:\n            print(\"TextToSpeechNode: TTS failed or was skipped.\")\n            return \"next_turn\" \n\n        llm_audio_bytes, llm_sample_rate = exec_res\n        \n        print(\"Playing LLM response...\")\n        try:\n            audio_segment, sr_from_file = soundfile.read(io.BytesIO(llm_audio_bytes))\n            play_audio_data(audio_segment, sr_from_file)\n        except Exception as e:\n            print(f\"Error playing TTS audio: {e}\")\n            return \"next_turn\" \n\n        if shared.get(\"continue_conversation\", True):\n            return \"next_turn\"\n        else:\n            print(\"Conversation ended by user flag.\")\n            return \"end_conversation\" ",
      "summary": "**Summary of `nodes.py` in `/tmp/PocketFlow/cookbook/pocketflow-voice-chat/`:**\n\n---\n\n### 1. Primary Purpose\nThe code defines a set of modular nodes for a voice chat pipeline. Each node represents a distinct stage in a conversational AI flow\u2014capturing user audio, transcribing speech to text, querying a language model, converting the model\u2019s response to speech, and playing it back to the user. These classes collectively enable a seamless, multi-turn voice conversation with an LLM-powered assistant.\n\n---\n\n### 2. Parameters (Inputs)\n- Each node class inherits from a base `Node` class and implements three methods (`prep`, `exec`, and `post`) with specific parameter conventions:\n  - `prep(self, shared)`: Accepts a shared state dictionary used to pass data between nodes.\n  - `exec(self, prep_res)` or `exec(self, _)`: Executes the main logic using the output from `prep` (if any).\n  - `post(self, shared, prep_res, exec_res)`: Updates shared state based on results from `exec`, handles flow control, and may signal the next state.\n\n- The primary input across nodes is the `shared` dictionary, which stores audio data, transcription text, chat history, and control flags.\n\n---\n\n### 3. Return Values (Outputs)\n- Most `prep` and `exec` methods return data intended for internal processing; for example:\n  - Audio data and sample rate (NumPy array, int)\n  - Transcribed user query (string)\n  - LLM-generated text response (string)\n  - TTS-generated audio bytes and sample rate (bytes, int)\n- `post` methods typically return a string instruction for the pipeline controller (e.g., `\"default\"`, `\"next_turn\"`, `\"end_conversation\"`), signaling what the flow should do next.\n\n---\n\n### 4. Internal Functions and Methods Called\nThe code invokes several external and utility functions inside the node methods:\n\n- **Audio Handling:**\n  - `record_audio()` \u2014 Records audio via microphone.\n  - `play_audio_data(audio_segment, sample_rate)` \u2014 Plays back an audio segment.\n\n- **File and Array Operations:**\n  - `scipy.io.wavfile.write()` \u2014 Serializes audio data to WAV format (for STT API).\n  - `soundfile.read()` \u2014 Reads audio data from bytes (for TTS playback).\n\n- **AI/ML APIs:**\n  - `speech_to_text_api(audio_data, sample_rate)` \u2014 Transcribes speech to text.\n  - `call_llm(messages)` \u2014 Queries a large language model for a response.\n  - `text_to_speech_api(text)` \u2014 Synthesizes speech from text.\n\n- **Standard Libraries:**\n  - `io.BytesIO` \u2014 Used for in-memory byte manipulation.\n\n---\n\n**In summary:**  \nThis module orchestrates end-to-end voice chat by defining node classes, each responsible for a step in the pipeline (audio capture \u2192 transcription \u2192 LLM query \u2192 speech synthesis \u2192 playback). Data and control flow through a shared dictionary and predetermined node output signals, relying on various internal utility and external API functions for audio and language processing."
    },
    "126": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import CaptureAudioNode, SpeechToTextNode, QueryLLMNode, TextToSpeechNode\n\ndef create_voice_chat_flow() -> Flow:\n    \"\"\"Creates and returns the voice chat flow.\"\"\"\n    # Create nodes\n    capture_audio = CaptureAudioNode()\n    speech_to_text = SpeechToTextNode()\n    query_llm = QueryLLMNode()\n    text_to_speech = TextToSpeechNode()\n\n    # Define transitions\n    capture_audio >> speech_to_text\n    speech_to_text >> query_llm\n    query_llm >> text_to_speech\n\n    # Loop back for next turn or end\n    text_to_speech - \"next_turn\" >> capture_audio\n    # \"end_conversation\" action from any node will terminate the flow naturally\n    # if no transition is defined for it from the current node.\n    # Alternatively, one could explicitly transition to an EndNode if desired.\n\n    # Create flow starting with the capture audio node\n    voice_chat_flow = Flow(start=capture_audio)\n    return voice_chat_flow ",
      "summary": "Certainly! Here is a concise summary of the provided code unit:\n\n1. **Primary Purpose:**  \n   The code defines a function to construct and return a flow (using the PocketFlow library) that models the turn-based logic of a voice chat system\u2014converting captured audio to text, querying a language model, synthesizing speech, and looping for each conversation turn.\n\n2. **Parameters:**  \n   The main function, `create_voice_chat_flow`, does not take any parameters.\n\n3. **Return Value:**  \n   The function returns a `Flow` object representing the voice chat logic, where nodes correspond to stages such as capturing audio, speech-to-text conversion, querying a language model, and converting text back to speech.\n\n4. **Other Functions or Methods Called Internally:**  \n   - `CaptureAudioNode()` (constructor)\n   - `SpeechToTextNode()` (constructor)\n   - `QueryLLMNode()` (constructor)\n   - `TextToSpeechNode()` (constructor)\n   - The transition operator (`>>`) defined by PocketFlow, to connect nodes\n   - The labeled transition operator (`- \"next_turn\" >>`)\n   - `Flow(start=capture_audio)` (constructor to create the flow instance)\n\nNo other external functions are called. The code relies on class constructors and operator overloads for flow definition."
    },
    "127": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/main.py",
      "code": "from flow import create_voice_chat_flow\n\ndef main():\n    \"\"\"Runs the PocketFlow Voice Chat application.\"\"\"\n    print(\"Starting PocketFlow Voice Chat...\")\n    print(\"Speak your query after 'Listening for your query...' appears.\")\n    print(\"The conversation will continue until an error occurs or the loop is intentionally stopped.\")\n    print(\"To attempt to stop, you might need to cause an error (e.g., silence during capture if not handled by VAD to end gracefully) or modify shared[\\\"continue_conversation\\\"] if a mechanism is added.\")\n\n    shared = {\n        \"user_audio_data\": None,\n        \"user_audio_sample_rate\": None,\n        \"chat_history\": [],\n        \"continue_conversation\": True # Flag to control the main conversation loop\n    }\n\n    # Create the flow\n    voice_chat_flow = create_voice_chat_flow()\n\n    # Run the flow\n    # The flow will loop based on the \"next_turn\" action from TextToSpeechNode\n    # and the continue_conversation flag checked within nodes or if an error action is returned.\n    voice_chat_flow.run(shared)\n\nif __name__ == \"__main__\":\n    main() \n",
      "summary": "**Concise Summary of `/tmp/PocketFlow/cookbook/pocketflow-voice-chat/main.py`:**\n\n1. **Primary Purpose:**  \n   The code serves as the main entry point for the PocketFlow Voice Chat application. It sets up a shared state for the conversation, initializes the voice chat flow, and starts the main conversational loop.\n\n2. **Parameters:**  \n   - The `main()` function takes no parameters.\n   - The script itself does not directly accept parameters via function calls; any configuration occurs within the `shared` dictionary.\n\n3. **Return Value:**  \n   - The `main()` function does not return any value; its purpose is side-effect-driven (running the application).\n\n4. **Functions/Methods Called Internally:**  \n   - `create_voice_chat_flow()` from the `flow` module (used to obtain a voice chat flow instance).\n   - `voice_chat_flow.run(shared)` (starts the voice chat conversation loop using the shared state).\n   - Built-in `print()` (used several times for user-facing instructions).\n\n**Summary:**  \nThis script starts the PocketFlow Voice Chat app by printing instructions, preparing a shared state dictionary, creating the voice chat processing flow, and running the conversation loop until an error or manual interruption stops it."
    },
    "128": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/utils/__init__.py",
      "code": "",
      "summary": "There is no code provided in your submission for /tmp/PocketFlow/cookbook/pocketflow-voice-chat/utils/__init__.py\u2014it's empty.\n\nSummary:\n1. **Primary purpose:**  \n   This code unit (__init__.py) currently has no content. In Python packages, an __init__.py file generally designates a directory as a Python package and can be used to initialize package-level code.\n\n2. **Parameters:**  \n   None, since there is no function or class defined.\n\n3. **Return value:**  \n   None, for the same reason.\n\n4. **Internal function/method calls:**  \n   None.\n\nIf you provide specific code within the file, I can analyze it in detail as requested."
    },
    "129": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/utils/call_llm.py",
      "code": "from openai import OpenAI\nimport os\n\ndef call_llm(messages):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test the LLM call\n    messages = [{\"role\": \"user\", \"content\": \"In a few words, what's the meaning of life?\"}]\n    response = call_llm(messages)\n    print(f\"Prompt: {messages[0]['content']}\")\n    print(f\"Response: {response}\")",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides a utility function to interact with OpenAI's chat models (e.g., GPT-4o). It sends user-defined message prompts to the model and returns the generated response, acting as a simple wrapper around the OpenAI API for chat completions.\n\n2. **Parameters:**  \n   - The main function `call_llm(messages)` takes a single parameter:\n     - `messages`: A list of dictionaries representing the conversation history, where each dictionary has keys like \"role\" (e.g., \"user\", \"system\") and \"content\" (the message text).\n\n3. **Return Value:**  \n   - The `call_llm` function returns a string containing the content of the model's first response message.\n\n4. **Other Functions/Methods Called Internally:**  \n   - `OpenAI()`: Instantiates the OpenAI client with the API key.\n   - `client.chat.completions.create()`: Sends the chat request to the model and returns a response containing the generated messages.\n   - System calls: `os.environ.get()` retrieves the OpenAI API key from environment variables.\n   - Standard output: `print()` (only in the test block at the end)."
    },
    "130": {
      "unit_name": "audio_utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/utils/audio_utils.py",
      "code": "import sounddevice as sd\nimport numpy as np\n\nDEFAULT_SAMPLE_RATE = 44100\nDEFAULT_CHANNELS = 1\nDEFAULT_CHUNK_SIZE_MS = 50  # Process audio in 50ms chunks for VAD\nDEFAULT_SILENCE_THRESHOLD_RMS = 0.01 # RMS value, needs tuning\nDEFAULT_MIN_SILENCE_DURATION_MS = 1000 # 1 second of silence to stop\nDEFAULT_MAX_RECORDING_DURATION_S = 15 # Safety cap for recording\nDEFAULT_PRE_ROLL_CHUNKS = 3 # Number of chunks to keep before speech starts\n\ndef record_audio(sample_rate = DEFAULT_SAMPLE_RATE,\n                 channels = DEFAULT_CHANNELS,\n                 chunk_size_ms = DEFAULT_CHUNK_SIZE_MS,\n                 silence_threshold_rms = DEFAULT_SILENCE_THRESHOLD_RMS,\n                 min_silence_duration_ms = DEFAULT_MIN_SILENCE_DURATION_MS,\n                 max_recording_duration_s = DEFAULT_MAX_RECORDING_DURATION_S,\n                 pre_roll_chunks_count = DEFAULT_PRE_ROLL_CHUNKS):\n    \"\"\"\n    Records audio from the microphone with silence-based VAD.\n    Returns in-memory audio data (NumPy array of float32) and sample rate.\n    Returns (None, sample_rate) if recording fails or max duration is met without speech.\n    \"\"\"\n    chunk_size_frames = int(sample_rate * chunk_size_ms / 1000)\n    min_silence_chunks = int(min_silence_duration_ms / chunk_size_ms)\n    max_chunks = int(max_recording_duration_s * 1000 / chunk_size_ms)\n\n    print(f\"Listening... (max {max_recording_duration_s}s). Speak when ready.\")\n    print(f\"(Silence threshold RMS: {silence_threshold_rms}, Min silence duration: {min_silence_duration_ms}ms)\")\n\n    recorded_frames = []\n    pre_roll_frames = []\n    is_recording = False\n    silence_counter = 0\n    chunks_recorded = 0\n\n    with sd.InputStream(samplerate=sample_rate, channels=channels, dtype='float32') as stream:\n\n        for i in range(max_chunks):\n            audio_chunk, overflowed = stream.read(chunk_size_frames)\n            if overflowed:\n                print(\"Warning: Audio buffer overflowed!\")\n            \n            rms = np.sqrt(np.mean(audio_chunk**2))\n\n            if is_recording:\n                recorded_frames.append(audio_chunk)\n                chunks_recorded += 1\n                if rms < silence_threshold_rms:\n                    silence_counter += 1\n                    if silence_counter >= min_silence_chunks:\n                        print(\"Silence detected, stopping recording.\")\n                        break\n                else:\n                    silence_counter = 0 # Reset silence counter on sound\n            else:\n                pre_roll_frames.append(audio_chunk)\n                if len(pre_roll_frames) > pre_roll_chunks_count:\n                    pre_roll_frames.pop(0)\n                \n                if rms > silence_threshold_rms:\n                    print(\"Speech detected, starting recording.\")\n                    is_recording = True\n                    for frame_to_add in pre_roll_frames:\n                        recorded_frames.append(frame_to_add)\n                    chunks_recorded = len(recorded_frames)\n                    pre_roll_frames.clear()\n            \n            if i == max_chunks - 1 and not is_recording:\n                print(\"No speech detected within the maximum recording duration.\")\n                return None, sample_rate\n\n        if not recorded_frames and is_recording:\n            print(\"Recording started but captured no frames before stopping. This might be due to immediate silence.\")\n\n    if not recorded_frames:\n        print(\"No audio was recorded.\")\n        return None, sample_rate\n\n    audio_data = np.concatenate(recorded_frames)\n    print(f\"Recording finished. Total duration: {len(audio_data)/sample_rate:.2f}s\")\n    return audio_data, sample_rate\n\ndef play_audio_data(audio_data, sample_rate):\n    \"\"\"Plays in-memory audio data (NumPy array).\"\"\"\n    try:\n        print(f\"Playing in-memory audio data (Sample rate: {sample_rate} Hz, Duration: {len(audio_data)/sample_rate:.2f}s)\")\n        sd.play(audio_data, sample_rate)\n        sd.wait()\n        print(\"Playback from memory finished.\")\n    except Exception as e:\n        print(f\"Error playing in-memory audio: {e}\")\n\n\nif __name__ == \"__main__\":\n    print(\"--- Testing audio_utils.py ---\")\n\n    # Test 1: record_audio() and play_audio_data() (in-memory)\n    print(\"\\n--- Test: Record and Play In-Memory Audio ---\")\n    print(\"Please speak into the microphone. Recording will start on sound and stop on silence.\")\n    recorded_audio, rec_sr = record_audio(\n        sample_rate=DEFAULT_SAMPLE_RATE,\n        silence_threshold_rms=0.02, \n        min_silence_duration_ms=1500,\n        max_recording_duration_s=10\n    )\n\n    if recorded_audio is not None and rec_sr is not None:\n        print(f\"Recorded audio data shape: {recorded_audio.shape}, Sample rate: {rec_sr} Hz\")\n        play_audio_data(recorded_audio, rec_sr)\n    else:\n        print(\"No audio recorded or recording failed.\")\n\n    print(\"\\n--- audio_utils.py tests finished. ---\") ",
      "summary": "**Summary of `audio_utils.py`:**\n\n1. **Primary Purpose:**\n   - This code unit provides utility functions for recording audio with silence-based voice activity detection (VAD) and for playing back audio data in-memory. Its main use is to enable interactive voice capture and playback, particularly useful for voice chat or voice-activated applications.\n\n2. **Brief Description of Parameters:**\n   - The main function, `record_audio`, accepts several parameters to control audio capture:\n     - `sample_rate`: The sampling rate for audio capture (Hz).\n     - `channels`: Number of audio channels (default mono).\n     - `chunk_size_ms`: Size of each audio chunk in milliseconds.\n     - `silence_threshold_rms`: Threshold for distinguishing between silence and speech (RMS value).\n     - `min_silence_duration_ms`: Minimum duration of silence (ms) before stopping recording.\n     - `max_recording_duration_s`: Safety cap, maximum possible duration of a recording (seconds).\n     - `pre_roll_chunks_count`: Number of audio chunks to buffer before speech detection (\"pre-roll\").\n\n3. **Brief Description of Return Value:**\n   - `record_audio` returns a tuple:\n     - The in-memory audio data (as a NumPy array of float32), or `None` if no audio is recorded.\n     - The sample rate used for recording.\n   - `play_audio_data` has no explicit return value; it plays the given audio data out loud.\n\n4. **Other Functions or Methods Called Internally:**\n   - **Within `record_audio` and `play_audio_data`:**\n     - `sounddevice.InputStream` and associated `stream.read()`\n     - `sounddevice.play()`, `sounddevice.wait()`\n     - NumPy functions: `np.sqrt`, `np.mean`, `np.concatenate`\n   - **Within Testing Block:**\n     - Calls its own functions (`record_audio`, `play_audio_data`)\n     - Standard print statements for status and errors\n\n**In summary:**  \nThis code is a practical audio utility module that records voice from a microphone (with silence detection so it only records when you talk) into memory, and can then play it back, using the SoundDevice and NumPy libraries. It allows tuning of how 'speech' is detected and for how long to keep recording after silence is sensed."
    },
    "131": {
      "unit_name": "text_to_speech.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/utils/text_to_speech.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef text_to_speech_api(text_to_synthesize: str):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n    response = client.audio.speech.create(\n        model=\"gpt-4o-mini-tts\",\n        voice=\"alloy\", # Other voices: echo, fable, onyx, nova, shimmer\n        input=text_to_synthesize,\n        response_format=\"mp3\" # Other formats: opus, aac, flac. MP3 is widely supported.\n                              # OpenAI default sample rate for tts-1 is 24kHz.\n    )\n    # The response.content is already bytes (the audio data)\n    # Alternatively, for streaming and saving to file: response.stream_to_file(\"output.mp3\")\n    audio_data_bytes = response.content\n    sample_rate = 24000 # OpenAI TTS model tts-1 outputs 24kHz\n    return audio_data_bytes, sample_rate\n\nif __name__ == \"__main__\":\n    print(\"Testing Text-to-Speech API...\")\n    # The OpenAI client will raise an error if API key is not found or invalid.\n    # No explicit check here to keep it minimal.\n    text = \"Hello from PocketFlow! This is a test of the text-to-speech functionality.\"\n    audio_bytes, rate = text_to_speech_api(text)\n    if audio_bytes and rate:\n        print(f\"Successfully converted text to speech. Audio data length: {len(audio_bytes)} bytes, Sample rate: {rate} Hz.\")\n        with open('tts_output.mp3', 'wb') as f:\n            f.write(audio_bytes)\n        print(\"Saved TTS output to tts_output.mp3\")\n    else: \n        print(\"Failed to convert text to speech (API returned empty data).\")",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-voice-chat/utils/text_to_speech.py`:**\n\n1. **Primary purpose:**  \n   This code unit provides a function to convert text to speech audio using OpenAI's TTS API and includes basic testing of that functionality. It enables the generation and saving of synthesized speech as an MP3 file from input text.\n\n2. **Parameters:**  \n   - The main function, `text_to_speech_api`, takes one parameter:  \n     - `text_to_synthesize` (str): The text string that will be converted into speech.\n\n3. **Return value:**  \n   - The function returns a tuple:  \n     - `audio_data_bytes`: The synthesized speech audio in MP3 format, as bytes.  \n     - `sample_rate`: The sample rate of the audio, hardcoded as 24000 Hz (24kHz).\n\n4. **Internal functions/methods called:**  \n   - `os.environ.get(\"OPENAI_API_KEY\")` to retrieve the OpenAI API key from environment variables.\n   - `OpenAI(api_key=...)` to instantiate the OpenAI API client.\n   - `client.audio.speech.create(...)` to synthesize speech from the input text.\n   - `response.content` to access the audio bytes from the API response.\n   - File handling methods (`open`, `f.write`) for saving the output audio in the test block.\n   - Standard Python built-ins (`print`, `len`) for output and testing.\n\n**Note:** The script expects the environment variable `OPENAI_API_KEY` to be set for API access."
    },
    "132": {
      "unit_name": "speech_to_text.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-voice-chat/utils/speech_to_text.py",
      "code": "import os\nfrom openai import OpenAI\nimport io\n\ndef speech_to_text_api(audio_data: bytes, sample_rate: int):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n    # The API expects a file-like object. We can use io.BytesIO for in-memory bytes.\n    # We also need to give it a name, as if it were a file upload.\n    audio_file = io.BytesIO(audio_data)\n    audio_file.name = \"audio.wav\"  # Corrected to WAV format\n\n    transcript = client.audio.transcriptions.create(\n        model=\"gpt-4o-transcribe\",\n        file=audio_file\n        # language=\"en\" # Optional: specify language ISO-639-1 code\n        # prompt=\"PocketFlow, LLM\" # Optional: provide a prompt to guide the model\n    )\n    return transcript.text\n\nif __name__ == \"__main__\":\n    print(\"Testing Speech-to-Text API...\")\n    # The OpenAI client will raise an error if API key is not found or invalid.\n    # No explicit check here to keep it minimal.\n    test_audio_path = \"tts_output.mp3\"\n    if os.path.exists(test_audio_path):\n        print(f\"Found {test_audio_path}, using it for STT test.\")\n        with open(test_audio_path, \"rb\") as f:\n            audio_bytes_for_stt = f.read()\n        \n        # Sample rate for tts_output.mp3 from our TTS script is 24000\n        # but Whisper should ideally infer or handle common formats well.\n        stt_sample_rate = 24000 \n\n        transcribed_text = speech_to_text_api(audio_bytes_for_stt, stt_sample_rate)\n\n        if transcribed_text:\n            print(f\"Transcribed text: {transcribed_text}\")\n        else:\n            print(\"Failed to transcribe audio (API returned empty data).\")\n    else:\n        print(f\"Test audio file '{test_audio_path}' not found.\")\n        print(\"Please run the text_to_speech.py test first to generate it, or place your own audio file\")\n        print(\" (e.g., named 'test_audio.mp3') in the same directory as this script and modify the path.\")\n        print(\"Make sure it's a common audio format like MP3, WAV, M4A etc.\") ",
      "summary": "**Summary of `speech_to_text.py`:**\n\n1. **Primary Purpose:**  \n   This code provides a utility function to convert speech audio (as raw bytes) into text using OpenAI's speech-to-text API (specifically the \"gpt-4o-transcribe\" model). The script can also be run as a standalone test to transcribe a sample audio file.\n\n2. **Parameters (for `speech_to_text_api`):**\n   - `audio_data` (`bytes`): The raw audio data to be transcribed.\n   - `sample_rate` (`int`): The sample rate (Hz) of the audio data. (Note: It's passed to the function but not currently used inside.)\n\n3. **Return Value:**  \n   - Returns the transcribed text as a string (`transcript.text`) representing the recognized speech from the provided audio.\n\n4. **Other Functions/Methods Called Internally:**\n   - `OpenAI` class from the `openai` library to create an API client.\n   - `os.environ.get` to fetch the OpenAI API key from environment variables.\n   - `io.BytesIO` to create an in-memory file-like object from the audio bytes.\n   - `client.audio.transcriptions.create` to send the audio for transcription using OpenAI's API.\n   - Various file and OS methods in the `__main__` test block: `os.path.exists`, file `open`, and standard print statements."
    },
    "133": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/nodes.py",
      "code": "from datetime import datetime\nfrom queue import Queue\n\nimport yaml\nfrom pocketflow import Node\n\nfrom utils.call_llm import call_llm\nfrom utils.call_mock_api import call_book_hotel_api, call_check_weather_api\nfrom utils.conversation import load_conversation, save_conversation\nfrom utils.format_chat_history import format_chat_history\n\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        conversation_id = shared[\"conversation_id\"]\n        session = load_conversation(conversation_id)\n        return session, shared[\"history\"], shared[\"query\"]\n\n    def exec(self, prep_res):\n        session, history, query = prep_res\n        prompt = f\"\"\"\n### INSTRUCTIONS\nYou are a lifestyle assistant capable of helping users book hotels and check weather conditions.\nYou need to decide the next action based on your last action, action execution result, chat history, and current user question.\n\n### CHAT HISTORY\n{format_chat_history(history)}\n\n### CURRENT USER QUESTION\nuser: {query}\n\n### CONTEXT\nLast Action: {session.get(\"last_action\", None)}\nLast Action Result: {session.get(\"action_result\", None)}\nCurrent Date: {datetime.now().date()} \n\n### ACTION SPACE\n[1] check-weather\nDescription: When the user asks about the weather, use this tool.\nParameters:\n    - name: city\n        description: The city to check the weather\n        required: true\n        example: Beijing\n    - name: date\n        description: The date to check the weather, if not provided, use the current date\n        required: false\n        example: 2025-05-28\n\n[2] book-hotel\nDescription: When the user wants to book a hotel, use this tool.\nParameters:\n    - name: hotel\n        description: The name of the hotel to be booked\n        required: true\n        example: ShanghaiHilton Hotel\n    - name: checkin_date\n        description: The check-in date\n        required: true\n        example: 2025-05-28\n    - name: checkout_date\n        description: The check-out date\n        required: true\n        example: 2025-05-29\n\n[3] follow-up\nDescription: 1. When the user's question is out of the scope of booking hotels and checking weather, use this tool to guide the user; 2. When the current information cannot meet the parameter requirements of the corresponding tool, use this tool to ask the user.\nParameters:\n    - name: question\n        description: Your guidance or follow-up to the user, maintain an enthusiastic and lively language style, and use the same language as the user's question.\n        required: true\n        example: Which hotel would you like to book?\ud83d\ude0a\n\n[4] result-notification\nDescription: When the booking of a hotel or checking the weather is completed, use this tool to notify the user of the result and ask if they need any other help. If you find that the user's question is not completed in the history conversation, you can guide the user to complete the intention in the last step.\nParameters:\n    - name: result\n        description: Notify the user of the result based on the Last Action Result. Maintain an enthusiastic and lively language style, and use the same language as the user's question.\n        required: true\n        example: The hotel has been successfully booked for you. \ud83d\ude09\\n\\nThe check-in date is XX, and the check-out date is XX. Thank you for using it. Would you like any other help?\ud83d\ude00\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: check-weather OR book-hotel OR follow-up OR result-notification\nreason: <why you chose this action>\nquestion: <if action is follow-up>\ncity: <if action is check-weather> \nhotel: <if action is book-hotel>\ncheckin_date: <if action is book-hotel>\ncheckout_date: <if action is book-hotel>\nresult: <if action is result-notification>\n```\n\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n\n        response = call_llm(prompt.strip())\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        print(f\"\ud83e\udd16 Agent response: \\n{yaml_str}\")\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        session[\"last_action\"] = exec_res[\"action\"]\n        flow_log: Queue = shared[\"flow_queue\"]\n\n        for line in exec_res[\"thinking\"].split(\"\\n\"):\n            line = line.replace(\"-\", \"\").strip()\n            if line:\n                flow_log.put(f\"\ud83e\udd14 {line}\")\n\n        if exec_res[\"action\"] == \"check-weather\":\n            session[\"check_weather_params\"] = {\n                \"city\": exec_res[\"city\"],\n                \"date\": exec_res.get(\"date\", None),\n            }\n            flow_log.put(f\"\u27a1\ufe0f Agent decided to check weather for: {exec_res['city']}\")\n        elif exec_res[\"action\"] == \"book-hotel\":\n            session[\"book_hotel_params\"] = {\n                \"hotel\": exec_res[\"hotel\"],\n                \"checkin_date\": exec_res[\"checkin_date\"],\n                \"checkout_date\": exec_res[\"checkout_date\"],\n            }\n            flow_log.put(f\"\u27a1\ufe0f Agent decided to book hotel: {exec_res['hotel']}\")\n        elif exec_res[\"action\"] == \"follow-up\":\n            session[\"follow_up_params\"] = {\"question\": exec_res[\"question\"]}\n            flow_log.put(f\"\u27a1\ufe0f Agent decided to follow up: {exec_res['question']}\")\n        elif exec_res[\"action\"] == \"result-notification\":\n            session[\"result_notification_params\"] = {\"result\": exec_res[\"result\"]}\n            flow_log.put(f\"\u27a1\ufe0f Agent decided to notify the result: {exec_res['result']}\")\n        save_conversation(conversation_id, session)\n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\n\nclass CheckWeather(Node):\n    def prep(self, shared):\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        city = session[\"check_weather_params\"][\"city\"]\n        date = session[\"check_weather_params\"].get(\"date\", None)\n        return city, date\n\n    def exec(self, prep_res):\n        city, date = prep_res\n        return call_check_weather_api(city, date)\n\n    def post(self, shared, prep_res, exec_res):\n        flow_log: Queue = shared[\"flow_queue\"]\n        flow_log.put(f\"\u2b05\ufe0f Check weather result: {exec_res}\")\n\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        session[\"action_result\"] = exec_res\n        save_conversation(conversation_id, session)\n        return \"default\"\n\n\nclass BookHotel(Node):\n    def prep(self, shared):\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n\n        hotel = session[\"book_hotel_params\"][\"hotel\"]\n        checkin_date = session[\"book_hotel_params\"][\"checkin_date\"]\n        checkout_date = session[\"book_hotel_params\"][\"checkout_date\"]\n        return hotel, checkin_date, checkout_date\n\n    def exec(self, prep_res):\n        hotel, checkin_date, checkout_date = prep_res\n        return call_book_hotel_api(hotel, checkin_date, checkout_date)\n\n    def post(self, shared, prep_res, exec_res):\n        flow_log: Queue = shared[\"flow_queue\"]\n        flow_log.put(f\"\u2b05\ufe0f Book hotel result: {exec_res}\")\n\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        session[\"action_result\"] = exec_res\n        save_conversation(conversation_id, session)\n        return \"default\"\n\n\nclass FollowUp(Node):\n    def prep(self, shared):\n        flow_log: Queue = shared[\"flow_queue\"]\n        flow_log.put(None)\n\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        question = session[\"follow_up_params\"][\"question\"]\n        return question, shared[\"queue\"]\n\n    def exec(self, prep_res):\n        question, queue = prep_res\n        queue.put(question)\n        queue.put(None)\n        return question\n\n    def post(self, shared, prep_res, exec_res):\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        session[\"action_result\"] = exec_res\n        return \"done\"\n\n\nclass ResultNotification(Node):\n    def prep(self, shared):\n        flow_log: Queue = shared[\"flow_queue\"]\n        flow_log.put(None)\n\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        result = session[\"result_notification_params\"][\"result\"]\n        return result, shared[\"queue\"]\n\n    def exec(self, prep_res):\n        result, queue = prep_res\n        queue.put(result)\n        queue.put(None)\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        conversation_id = shared[\"conversation_id\"]\n        session: dict = load_conversation(conversation_id)\n        session[\"action_result\"] = None\n        session[\"last_action\"] = None\n        save_conversation(conversation_id, session)\n        return \"done\"\n",
      "summary": "**Summary of nodes.py**\n\n1. **Primary purpose:**  \nThis code defines a set of node classes for a conversational assistant workflow that helps users book hotels and check weather conditions. Each node represents a step in the dialogue management process, using a large language model (LLM) to decide what to do next and calling APIs to execute user requests. The nodes handle different actions: deciding the next action, checking the weather, booking a hotel, following up with the user for more information, and sending result notifications.\n\n2. **Description of parameters:**  \nEach node\u2019s methods usually accept parameters as follows:\n- `shared`: A dictionary containing shared context between nodes, including the current conversation ID, conversation history, user query, a queue for logging the flow, and possibly queues for communicating with the user interface.\n- `prep_res`: The result of the `prep` method, typically containing extracted and relevant parameters for the current action (e.g., city name, hotel, question text).\n- `exec_res`: The result of the `exec` method, typically an API call response or generated text.\n\n3. **Description of return value:**  \nEach node\u2019s methods return the following:\n- `prep`: Returns action parameters extracted from shared state or session, bundled as tuples.\n- `exec`: Returns either the API call result (string or dict), a question or a result notification string, or a dictionary with a decision (in the DecideAction node).\n- `post`: Updates the conversation state and/or flow logs as needed. Returns a string indicating the next node to execute (e.g., the action name like \"done\" or \"default\", or the next high-level action).\n\n4. **Other functions or methods called internally:**\n- `call_llm` (from utils.call_llm): Calls the language model to decide the next action.\n- `call_check_weather_api` (from utils.call_mock_api): Simulates the checking of weather.\n- `call_book_hotel_api` (from utils.call_mock_api): Simulates hotel booking.\n- `load_conversation`, `save_conversation` (from utils.conversation): Load and save the session/conversation data.\n- `format_chat_history` (from utils.format_chat_history): Format the conversation history into a string for prompt construction.\n- Standard Python utilities: `yaml.safe_load`, `datetime.now`.\n- Usage of `Queue` (from queue): For communicating actions, questions, and results between nodes and/or the user interface.\n- Various base methods from `pocketflow.Node` (by subclassing).\n\n**Overall, this code organizes the workflow for a hotel- and weather-booking conversational agent, handling decision logic, API simulations, user follow-ups, and conversational state management via a node-based pattern.**"
    },
    "134": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/flow.py",
      "code": "from pocketflow import Flow\n\nfrom nodes import (\n    DecideAction,\n    CheckWeather,\n    BookHotel,\n    FollowUp,\n    ResultNotification,\n)\n\n\ndef create_flow():\n    \"\"\"\n    Create and connect the nodes to form a complete agent flow.\n    \"\"\"\n    decide_action = DecideAction()\n    check_weather = CheckWeather()\n    book_hotel = BookHotel()\n    follow_up = FollowUp()\n    result_notification = ResultNotification()\n\n    decide_action - \"check-weather\" >> check_weather\n    check_weather >> decide_action\n    decide_action - \"book-hotel\" >> book_hotel\n    book_hotel >> decide_action\n    decide_action - \"follow-up\" >> follow_up\n    decide_action - \"result-notification\" >> result_notification\n\n    return Flow(start=decide_action)\n",
      "summary": "**Summary of flow.py:**\n\n1. **Primary purpose:**  \n   This code unit defines a function to construct and return an agent flow by connecting various workflow nodes (such as deciding actions, checking weather, booking hotels, or sending notifications) using the PocketFlow library.\n\n2. **Parameters:**  \n   The `create_flow()` function does **not** take any parameters.\n\n3. **Return value:**  \n   The function returns a `Flow` object that represents the constructed workflow, beginning at the `DecideAction` node.\n\n4. **Functions/methods called internally:**  \n   - Constructors for the following classes (presumably from the `nodes` module):  \n     - `DecideAction()`\n     - `CheckWeather()`\n     - `BookHotel()`\n     - `FollowUp()`\n     - `ResultNotification()`\n   - Operator overloads for connecting nodes (e.g., `-`, `>>`) are used to define flow logic between the nodes.\n   - The `Flow` class constructor from the `pocketflow` library.\n\n**In summary:**  \nThe code sets up and returns a connected workflow involving action decision, weather check, hotel booking, follow-up actions, and result notification, with no function parameters and with several node constructors and flow-connecting methods used internally."
    },
    "135": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/main.py",
      "code": "import time\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\nfrom queue import Queue\n\nimport gradio as gr\nfrom gradio import ChatMessage\n\nfrom flow import create_flow\n\n# create global thread pool\nchatflow_thread_pool = ThreadPoolExecutor(\n    max_workers=5,\n    thread_name_prefix=\"chatflow_worker\",\n)\n\n\ndef chat_fn(message, history, uuid):\n    \"\"\"\n    Main chat function that handles the conversation flow and message processing.\n    \n    Args:\n        message (str): The current user message\n        history (list): Previous conversation history\n        uuid (UUID): Unique identifier for the conversation\n    \n    Yields:\n        ChatMessage: Streams of thought process and chat responses\n    \"\"\"\n    # Log conversation details\n    print(f\"Conversation ID: {str(uuid)}\\nHistory: {history}\\nQuery: {message}\\n---\")\n    \n    # Initialize queues for chat messages and flow thoughts\n    chat_queue = Queue()\n    flow_queue = Queue()\n    \n    # Create shared context for the flow\n    shared = {\n        \"conversation_id\": str(uuid),\n        \"query\": message,\n        \"history\": history,\n        \"queue\": chat_queue,\n        \"flow_queue\": flow_queue,\n    }\n    \n    # Create and run the chat flow in a separate thread\n    chat_flow = create_flow()\n    chatflow_thread_pool.submit(chat_flow.run, shared)\n\n    # Initialize thought response tracking\n    start_time = time.time()\n    thought_response = ChatMessage(\n        content=\"\", metadata={\"title\": \"Flow Log\", \"id\": 0, \"status\": \"pending\"}\n    )\n    yield thought_response\n\n    # Process and accumulate thoughts from the flow queue\n    accumulated_thoughts = \"\"\n    while True:\n        thought = flow_queue.get()\n        if thought is None:\n            break\n        accumulated_thoughts += f\"- {thought}\\n\\n\"\n        thought_response.content = accumulated_thoughts.strip()\n        yield thought_response\n        flow_queue.task_done()\n\n    # Mark thought processing as complete and record duration\n    thought_response.metadata[\"status\"] = \"done\"\n    thought_response.metadata[\"duration\"] = time.time() - start_time\n    yield thought_response\n\n    # Process and yield chat messages from the chat queue\n    while True:\n        msg = chat_queue.get()\n        if msg is None:\n            break\n        chat_response = [thought_response, ChatMessage(content=msg)]\n        yield chat_response\n        chat_queue.task_done()\n\n\ndef clear_fn():\n    print(\"Clearing conversation\")\n    return uuid.uuid4()\n\n\nwith gr.Blocks(fill_height=True, theme=\"ocean\") as demo:\n    uuid_state = gr.State(uuid.uuid4())\n    demo.load(clear_fn, outputs=[uuid_state])\n\n    chatbot = gr.Chatbot(type=\"messages\", scale=1)\n    chatbot.clear(clear_fn, outputs=[uuid_state])\n\n    gr.ChatInterface(\n        fn=chat_fn,\n        type=\"messages\",\n        additional_inputs=[uuid_state],\n        chatbot=chatbot,\n        title=\"PocketFlow Gradio Demo\",\n    )\n\n\ndemo.launch()\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/main.py`:**\n\n1. **Primary Purpose:**  \n   This code defines and launches a Gradio-based web application for conversational AI with human-in-the-loop (HITL) capabilities. It manages chat conversations, processes messages through a custom flow, and displays streaming responses and system thoughts in real time.\n\n2. **Parameters:**  \n   - The main `chat_fn` function accepts:\n     - `message` (`str`): The latest user input/message.\n     - `history` (`list`): The conversation history.\n     - `uuid` (`UUID`): A unique ID for the current conversation/session.\n\n3. **Return Value:**  \n   - The `chat_fn` function is a Python generator that yields one or more `ChatMessage` objects, streaming both thought logs and chat responses to Gradio as they're produced.\n\n4. **Internally Called Functions/Methods:**  \n   - `create_flow()` (from `flow`): Instantiates the main chat processing flow.\n   - `chat_flow.run(shared)`: Executes the flow logic on a separate thread, passing conversation context in `shared`.\n   - `Queue().get()`, `Queue().task_done()`: Standard Python queue operations for message/thought retrieval and completion signaling.\n   - `time.time()`: Records processing duration.\n   - `uuid.uuid4()`: Generates new unique identifiers for conversations.\n   - Gradio components:\n     - `gr.Blocks()`\n     - `gr.State()`\n     - `gr.Chatbot()`\n     - `gr.ChatInterface()`\n     - `.load()`, `.clear()`, `.launch()`: Methods for app setup, event hooks, and launch.\n\n**In short:**  \nThe file sets up a threaded conversational AI web demo, streaming system thought logs and bot replies to the user. It orchestrates message handling across threads with Python queues, utilizes a custom flow for reasoning, and exposes the interaction via Gradio's chat interface."
    },
    "136": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/utils/call_llm.py",
      "code": "import os\n\nfrom openai import OpenAI\nfrom openai.types.chat.chat_completion import ChatCompletion\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nbase_url = \"https://api.openai.com/v1\"\nmodel = \"gpt-4o\"\n\n\ndef call_llm(message: str):\n    print(f\"Calling LLM with message: \\n{message}\")\n    client = OpenAI(api_key=api_key, base_url=base_url)\n    response: ChatCompletion = client.chat.completions.create(\n        model=model, messages=[{\"role\": \"user\", \"content\": message}]\n    )\n    return response.choices[0].message.content\n\n\nif __name__ == \"__main__\":\n    print(call_llm(\"Hello, how are you?\"))\n",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   This code provides a utility function to interact with OpenAI's GPT-4o language model: it sends a message to the model using OpenAI's API and returns the generated response.\n\n2. **Parameters:**  \n   - The main function, `call_llm`, has a single parameter:  \n     - `message` (str): The text prompt or message you want to send to the language model.\n\n3. **Return Value:**  \n   - The `call_llm` function returns a string containing the model's generated response to the input `message`.\n\n4. **Internal Functions/Methods Called:**  \n   - `os.getenv` (to retrieve the OpenAI API key)\n   - `OpenAI` constructor from the `openai` library (to instantiate the API client)\n   - `client.chat.completions.create` (to send the message and receive a response from the OpenAI API)\n   - Accessing the `response.choices[0].message.content` attribute (to extract the content of the model's reply)"
    },
    "137": {
      "unit_name": "call_mock_api.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/utils/call_mock_api.py",
      "code": "import random\nfrom datetime import date, datetime\n\n\ndef call_check_weather_api(city: str, date: date | None):\n    if date is None:\n        date = datetime.now().date()\n\n    current_date = datetime.now().date()\n\n    # calculate date difference\n    date_diff = (date - current_date).days\n\n    # check if the date is within the allowed range\n    if abs(date_diff) > 7:\n        return f\"Failed to check weather: Date {date} is more than 7 days away from current date.\"\n\n    return f\"The weather in {city} on {date} is {random.choice(['sunny', 'cloudy', 'rainy', 'snowy'])}, and the temperature is {random.randint(10, 30)}\u00b0C.\"\n\n\ndef call_book_hotel_api(hotel: str, checkin_date: date, checkout_date: date):\n    current_date = datetime.now().date()\n\n    # check if the checkin date is after the current date\n    if checkin_date <= current_date:\n        return (\n            f\"Failed to book hotel {hotel}: Check-in date must be after current date.\"\n        )\n\n    # check if the checkin date is before the checkout date\n    if checkin_date >= checkout_date:\n        return f\"Failed to book hotel {hotel}, because the checkin date is after the checkout date.\"\n\n    # check if the date difference is more than 7 days\n    date_diff = (checkout_date - checkin_date).days\n    if date_diff > 7:\n        return f\"Failed to book hotel {hotel}: Stay duration cannot exceed 7 days.\"\n\n    return f\"Booked hotel {hotel} from {checkin_date.strftime('%Y-%m-%d')} to {checkout_date.strftime('%Y-%m-%d')} successfully.\"\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/utils/call_mock_api.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides mock implementations of API calls for checking the weather in a city and booking a hotel, primarily for testing or demonstration purposes. It generates simulated responses based on simple logic and randomization.\n\n2. **Description of Parameters:**  \n   - `call_check_weather_api(city: str, date: date | None)`:  \n     - `city` (str): Name of the city to check the weather for.  \n     - `date` (date or None): The date for which to check the weather; if `None`, defaults to today.\n   - `call_book_hotel_api(hotel: str, checkin_date: date, checkout_date: date)`:  \n     - `hotel` (str): Name of the hotel to book.  \n     - `checkin_date` (date): Check-in date.  \n     - `checkout_date` (date): Check-out date.\n\n3. **Description of Return Values:**  \n   - Both functions return a formatted string.  \n   - On success, a string confirming the outcome (weather status or booking success) is returned.  \n   - On failure, a string describing the reason for failure (e.g., invalid date range, date out of allowed bounds).\n\n4. **Internally Called Functions or Methods:**  \n   - `datetime.now()` and `datetime.date()` (from `datetime` module)  \n   - `random.choice()` (for random weather status)  \n   - `random.randint()` (for random temperature)  \n   - `date.strftime()` method (for formatting dates)"
    },
    "138": {
      "unit_name": "conversation.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/utils/conversation.py",
      "code": "conversation_cache = {}\n\n\ndef load_conversation(conversation_id: str):\n    print(f\"Loading conversation {conversation_id}\")\n    return conversation_cache.get(conversation_id, {})\n\n\ndef save_conversation(conversation_id: str, session: dict):\n    print(f\"Saving conversation {session}\")\n    conversation_cache[conversation_id] = session\n",
      "summary": "**Summary of `conversation.py`:**\n\n1. **Primary Purpose:**  \n   This code unit manages an in-memory cache of conversations, providing simple load and save operations by conversation ID.\n\n2. **Parameters:**  \n   - `load_conversation(conversation_id: str)`:  \n     - `conversation_id`: The unique identifier (string) for the conversation to load.\n   - `save_conversation(conversation_id: str, session: dict)`:  \n     - `conversation_id`: The unique identifier (string) for the conversation session to save.\n     - `session`: A dictionary representing the current state of the conversation.\n\n3. **Return Value:**  \n   - `load_conversation` returns the conversation session dictionary associated with `conversation_id` if it exists in the cache, otherwise it returns an empty dictionary.\n   - `save_conversation` does not return a value (returns `None`).\n\n4. **Other Functions/Methods Called Internally:**  \n   - Built-in functions: `print`\n   - Dictionary method: `get` (used on `conversation_cache`)"
    },
    "139": {
      "unit_name": "format_chat_history.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-gradio-hitl/utils/format_chat_history.py",
      "code": "def format_chat_history(history):\n    \"\"\"\n    Format the chat history for LLM\n\n    Args:\n        history (list): The chat history list, each element contains role and content\n\n    Returns:\n        str: The formatted chat history string\n    \"\"\"\n    if not history:\n        return \"No history\"\n\n    formatted_history = []\n    for message in history:\n        role = \"user\" if message[\"role\"] == \"user\" else \"assistant\"\n        content = message[\"content\"]\n        # filter out the thinking content\n        if role == \"assistant\":\n            if (\n                content.startswith(\"- \ud83e\udd14\")\n                or content.startswith(\"- \u27a1\ufe0f\")\n                or content.startswith(\"- \u2b05\ufe0f\")\n            ):\n                continue\n        formatted_history.append(f\"{role}: {content}\")\n\n    return \"\\n\".join(formatted_history)\n",
      "summary": "**Summary for `format_chat_history.py` / function: `format_chat_history`**\n\n1. **Primary purpose:**  \n   The function `format_chat_history` takes a list of chat messages and formats them into a readable string, suitable for input to a large language model (LLM), while filtering out certain \"thinking\" messages from the assistant.\n\n2. **Parameters:**  \n   - `history` (list): A list of chat message dictionaries. Each message dictionary should contain the keys `\"role\"` (either `\"user\"` or `\"assistant\"`) and `\"content\"` (the text of the message).\n\n3. **Return value:**  \n   - Returns a single string representing the formatted chat history. If the input list is empty or None, it returns the string `\"No history\"`.\n\n4. **Other functions or methods called internally:**  \n   - Uses only Python built-in methods:  \n     - `startswith()` (for filtering content)  \n     - `append()` (to add to the formatted list)  \n     - `join()` (to create the final string)  \n   - No external or custom functions are called."
    },
    "140": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-hello-world/flow.py",
      "code": "from pocketflow import Node, Flow\nfrom utils.call_llm import call_llm\n\n# An example node and flow\n# Please replace this with your own node and flow\nclass AnswerNode(Node):\n    def prep(self, shared):\n        # Read question from shared\n        return shared[\"question\"]\n    \n    def exec(self, question):\n        return call_llm(question)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the answer in shared\n        shared[\"answer\"] = exec_res\n\nanswer_node = AnswerNode()\nqa_flow = Flow(start=answer_node)",
      "summary": "**Summary of Code Unit: `flow.py`**\n\n1. **Primary Purpose:**  \n   The code defines a simple node-based workflow for a question-answering application. Specifically, it implements an `AnswerNode` that takes a question, sends it to a language model via `call_llm`, and stores the resulting answer.\n\n2. **Description of Parameters:**  \n   - The `prep` method accepts a `shared` dictionary, expected to contain a `\"question\"` key.\n   - The `exec` method takes a `question` string as its input.\n   - The `post` method takes `shared` (the shared dictionary), `prep_res` (the output from `prep`, i.e., the question), and `exec_res` (the output from `exec`, i.e., the answer).\n\n3. **Description of Return Value:**  \n   - `prep` returns the question string from the shared dictionary.\n   - `exec` returns the answer generated by `call_llm`.\n   - `post` does not return anything; it updates the shared dictionary with the answer.\n\n4. **Internal Function/Method Calls:**  \n   - `call_llm()` from `utils.call_llm` (invoked in the `exec` method).\n   - Inherits methods from `Node` and uses `Flow` from the `pocketflow` module.\n\n**Overall, this code sets up a modular, node-based flow that retrieves a natural language answer to a question via a language model.**"
    },
    "141": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-hello-world/main.py",
      "code": "from flow import qa_flow\n\n# Example main function\n# Please replace this with your own main function\ndef main():\n    shared = {\n        \"question\": \"In one sentence, what's the end of universe?\",\n        \"answer\": None\n    }\n\n    qa_flow.run(shared)\n    print(\"Question:\", shared[\"question\"])\n    print(\"Answer:\", shared[\"answer\"])\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-hello-world/main.py`:**\n\n1. **Primary Purpose:**  \n   The code serves as a simple example for running a question-answering flow. It sets up a shared data structure with a question about the end of the universe, executes the `qa_flow`, and displays the question alongside the received answer.\n\n2. **Parameters:**  \n   The main function does not accept any parameters.\n\n3. **Return Value:**  \n   The main function does not return any value.\n\n4. **Functions/Methods Called Internally:**  \n   - `qa_flow.run(shared)` \u2014 runs the question-answering flow with the shared data.\n   - `print()` \u2014 prints the question and the answer to the console."
    },
    "142": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-hello-world/utils/__init__.py",
      "code": "",
      "summary": "It appears the code unit you referenced\u2014/tmp/PocketFlow/cookbook/pocketflow-hello-world/utils/__init__.py\u2014does not contain any code; you have presented an empty code block.\n\n**Summary:**\n1. **Primary purpose:**  \n   This file, in its current empty state, serves as an implicit initializer for the utils Python package, making the package importable. It does not add any logic or functionality by itself.\n\n2. **Parameters:**  \n   There are no parameters, as there are no functions, classes, or code in the file.\n\n3. **Return value:**  \n   There are no return values, as the file contains no executable code.\n\n4. **Other functions/methods called internally:**  \n   None, since there is no code present.\n\nIf you intended to analyze specific code, please provide it in the code block."
    },
    "143": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-hello-world/utils/call_llm.py",
      "code": "from openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n    \nif __name__ == \"__main__\":\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt))",
      "summary": "**Summary of `call_llm.py`:**\n\n1. **Primary Purpose:**  \n   The code defines a utility for sending a text prompt to OpenAI's GPT-4o language model and retrieving its response. It acts as a simple wrapper to interact with OpenAI's chat API.\n\n2. **Parameters:**  \n   - The main function `call_llm(prompt)` takes a single parameter:\n     - `prompt`: A string representing the user's input or question to send to the language model.\n\n3. **Return Value:**  \n   - `call_llm` returns a string: the content of the model's response generated for the given prompt.\n\n4. **Internal Functions/Methods Called:**  \n   - `OpenAI(api_key=...)` (class instantiation from the `openai` library)\n   - `client.chat.completions.create(...)` (to send the prompt and receive a response)\n   - Accesses `r.choices[0].message.content` (to extract the model's reply)\n\nAdditionally, when executed directly (`__main__`), the script demonstrates usage by sending the prompt \"What is the meaning of life?\" and printing the model's response."
    },
    "144": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-thinking/nodes.py",
      "code": "# cookbook/pocketflow-thinking/nodes.py\nfrom pocketflow import Node\nimport yaml\nfrom utils import call_llm\nimport textwrap\n\n# Helper function to format structured plan for printing\ndef format_plan(plan_items, indent_level=0):\n    indent = \"  \" * indent_level\n    output = []\n    if isinstance(plan_items, list):\n        for item in plan_items:\n            if isinstance(item, dict):\n                status = item.get('status', 'Unknown')\n                desc = item.get('description', 'No description')\n                result = item.get('result', '')\n                mark = item.get('mark', '') # For verification etc.\n\n                # Format the main step line\n                line = f\"{indent}- [{status}] {desc}\"\n                if result:\n                    line += f\": {result}\"\n                if mark:\n                    line += f\" ({mark})\"\n                output.append(line)\n\n                # Recursively format sub-steps if they exist\n                sub_steps = item.get('sub_steps')\n                if sub_steps:\n                    output.append(format_plan(sub_steps, indent_level + 1))\n            elif isinstance(item, str): # Basic fallback for string items\n                 output.append(f\"{indent}- {item}\")\n            else: # Fallback for unexpected types\n                 output.append(f\"{indent}- {str(item)}\")\n\n    elif isinstance(plan_items, str): # Handle case where plan is just an error string\n        output.append(f\"{indent}{plan_items}\")\n    else:\n        output.append(f\"{indent}# Invalid plan format: {type(plan_items)}\")\n\n    return \"\\n\".join(output)\n\n# Helper function to format structured plan for the prompt (simplified view)\ndef format_plan_for_prompt(plan_items, indent_level=0):\n    indent = \"  \" * indent_level\n    output = []\n    # Simplified formatting for prompt clarity\n    if isinstance(plan_items, list):\n        for item in plan_items:\n            if isinstance(item, dict):\n                status = item.get('status', 'Unknown')\n                desc = item.get('description', 'No description')\n                line = f\"{indent}- [{status}] {desc}\"\n                output.append(line)\n                sub_steps = item.get('sub_steps')\n                if sub_steps:\n                    # Indicate nesting without full recursive display in prompt\n                    output.append(format_plan_for_prompt(sub_steps, indent_level + 1))\n            else: # Fallback\n                 output.append(f\"{indent}- {str(item)}\")\n    else:\n        output.append(f\"{indent}{str(plan_items)}\")\n    return \"\\n\".join(output)\n\n\nclass ChainOfThoughtNode(Node):\n    def prep(self, shared):\n        problem = shared.get(\"problem\", \"\")\n        thoughts = shared.get(\"thoughts\", [])\n        current_thought_number = shared.get(\"current_thought_number\", 0)\n\n        shared[\"current_thought_number\"] = current_thought_number + 1\n\n        # Format previous thoughts and extract last plan structure\n        thoughts_text = \"\"\n        last_plan_structure = None # Will store the list of dicts\n        if thoughts:\n            thoughts_text_list = []\n            for i, t in enumerate(thoughts):\n                 thought_block = f\"Thought {t.get('thought_number', i+1)}:\\n\"\n                 thinking = textwrap.dedent(t.get('current_thinking', 'N/A')).strip()\n                 thought_block += f\"  Thinking:\\n{textwrap.indent(thinking, '    ')}\\n\"\n\n                 plan_list = t.get('planning', [])\n                 # Use the recursive helper for display formatting\n                 plan_str_formatted = format_plan(plan_list, indent_level=2)\n                 thought_block += f\"  Plan Status After Thought {t.get('thought_number', i+1)}:\\n{plan_str_formatted}\"\n\n                 if i == len(thoughts) - 1:\n                     last_plan_structure = plan_list # Keep the actual structure\n\n                 thoughts_text_list.append(thought_block)\n\n            thoughts_text = \"\\n--------------------\\n\".join(thoughts_text_list)\n        else:\n            thoughts_text = \"No previous thoughts yet.\"\n            # Suggest an initial plan structure using dictionaries\n            last_plan_structure = [\n                {'description': \"Understand the problem\", 'status': \"Pending\"},\n                {'description': \"Develop a high-level plan\", 'status': \"Pending\"},\n                {'description': \"Conclusion\", 'status': \"Pending\"}\n            ]\n\n        # Format the last plan structure for the prompt context using the specific helper\n        last_plan_text_for_prompt = format_plan_for_prompt(last_plan_structure) if last_plan_structure else \"# No previous plan available.\"\n\n        return {\n            \"problem\": problem,\n            \"thoughts_text\": thoughts_text,\n            \"last_plan_text\": last_plan_text_for_prompt,\n            \"last_plan_structure\": last_plan_structure, # Pass the raw structure too if needed for complex updates\n            \"current_thought_number\": current_thought_number + 1,\n            \"is_first_thought\": not thoughts\n        }\n\n    def exec(self, prep_res):\n        problem = prep_res[\"problem\"]\n        thoughts_text = prep_res[\"thoughts_text\"]\n        last_plan_text = prep_res[\"last_plan_text\"]\n        # last_plan_structure = prep_res[\"last_plan_structure\"] # Can use if needed\n        current_thought_number = prep_res[\"current_thought_number\"]\n        is_first_thought = prep_res[\"is_first_thought\"]\n\n        # --- Construct Prompt ---\n        # Instructions updated for dictionary structure\n        instruction_base = textwrap.dedent(f\"\"\"\n            Your task is to generate the next thought (Thought {current_thought_number}).\n\n            Instructions:\n            1.  **Evaluate Previous Thought:** If not the first thought, start `current_thinking` by evaluating Thought {current_thought_number - 1}. State: \"Evaluation of Thought {current_thought_number - 1}: [Correct/Minor Issues/Major Error - explain]\". Address errors first.\n            2.  **Execute Step:** Execute the first step in the plan with `status: Pending`.\n            3.  **Maintain Plan (Structure):** Generate an updated `planning` list. Each item should be a dictionary with keys: `description` (string), `status` (string: \"Pending\", \"Done\", \"Verification Needed\"), and optionally `result` (string, concise summary when Done) or `mark` (string, reason for Verification Needed). Sub-steps are represented by a `sub_steps` key containing a *list* of these dictionaries.\n            4.  **Update Current Step Status:** In the updated plan, change the `status` of the executed step to \"Done\" and add a `result` key with a concise summary. If verification is needed based on evaluation, change status to \"Verification Needed\" and add a `mark`.\n            5.  **Refine Plan (Sub-steps):** If a \"Pending\" step is complex, add a `sub_steps` key to its dictionary containing a list of new step dictionaries (status: \"Pending\") breaking it down. Keep the parent step's status \"Pending\" until all sub-steps are \"Done\".\n            6.  **Refine Plan (Errors):** Modify the plan logically based on evaluation findings (e.g., change status, add correction steps).\n            7.  **Final Step:** Ensure the plan progresses towards a final step dictionary like `{{'description': \"Conclusion\", 'status': \"Pending\"}}`.\n            8.  **Termination:** Set `next_thought_needed` to `false` ONLY when executing the step with `description: \"Conclusion\"`.\n        \"\"\")\n\n        # Context remains largely the same\n        if is_first_thought:\n            instruction_context = textwrap.dedent(\"\"\"\n                **This is the first thought:** Create an initial plan as a list of dictionaries (keys: description, status). Include sub-steps via the `sub_steps` key if needed. Then, execute the first step in `current_thinking` and provide the updated plan (marking step 1 `status: Done` with a `result`).\n            \"\"\")\n        else:\n            instruction_context = textwrap.dedent(f\"\"\"\n                **Previous Plan (Simplified View):**\n                {last_plan_text}\n\n                Start `current_thinking` by evaluating Thought {current_thought_number - 1}. Then, proceed with the first step where `status: Pending`. Update the plan structure (list of dictionaries) reflecting evaluation, execution, and refinements.\n            \"\"\")\n\n        # Output format example updated for dictionary structure\n        instruction_format = textwrap.dedent(\"\"\"\n            Format your response ONLY as a YAML structure enclosed in ```yaml ... ```:\n            ```yaml\n            current_thinking: |\n              # Evaluation of Thought N: [Assessment] ... (if applicable)\n              # Thinking for the current step...\n            planning:\n              # List of dictionaries (keys: description, status, Optional[result, mark, sub_steps])\n              - description: \"Step 1\"\n                status: \"Done\"\n                result: \"Concise result summary\"\n              - description: \"Step 2 Complex Task\" # Now broken down\n                status: \"Pending\" # Parent remains Pending\n                sub_steps:\n                  - description: \"Sub-task 2a\"\n                    status: \"Pending\"\n                  - description: \"Sub-task 2b\"\n                    status: \"Verification Needed\"\n                    mark: \"Result from Thought X seems off\"\n              - description: \"Step 3\"\n                status: \"Pending\"\n              - description: \"Conclusion\"\n                status: \"Pending\"\n            next_thought_needed: true # Set to false ONLY when executing the Conclusion step.\n            ```\n        \"\"\")\n\n        # Combine prompt parts\n        prompt = textwrap.dedent(f\"\"\"\n            You are a meticulous AI assistant solving a complex problem step-by-step using a structured plan. You critically evaluate previous steps, refine the plan with sub-steps if needed, and handle errors logically. Use the specified YAML dictionary structure for the plan.\n\n            Problem: {problem}\n\n            Previous thoughts:\n            {thoughts_text}\n            --------------------\n            {instruction_base}\n            {instruction_context}\n            {instruction_format}\n        \"\"\")\n        # --- End Prompt Construction ---\n\n        response = call_llm(prompt)\n\n        # Simple YAML extraction\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        thought_data = yaml.safe_load(yaml_str) # Can raise YAMLError\n\n        # --- Validation (using assert) ---\n        assert thought_data is not None, \"YAML parsing failed, result is None\"\n        assert \"current_thinking\" in thought_data, \"LLM response missing 'current_thinking'\"\n        assert \"next_thought_needed\" in thought_data, \"LLM response missing 'next_thought_needed'\"\n        assert \"planning\" in thought_data, \"LLM response missing 'planning'\"\n        assert isinstance(thought_data.get(\"planning\"), list), \"LLM response 'planning' is not a list\"\n        # Optional: Add deeper validation of list items being dicts if needed\n        # --- End Validation ---\n\n        # Add thought number\n        thought_data[\"thought_number\"] = current_thought_number\n        return thought_data\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Add the new thought to the list\n        if \"thoughts\" not in shared:\n            shared[\"thoughts\"] = []\n        shared[\"thoughts\"].append(exec_res)\n\n        # Extract plan for printing using the updated recursive helper function\n        plan_list = exec_res.get(\"planning\", [\"Error: Planning data missing.\"])\n        plan_str_formatted = format_plan(plan_list, indent_level=1)\n\n        thought_num = exec_res.get('thought_number', 'N/A')\n        current_thinking = exec_res.get('current_thinking', 'Error: Missing thinking content.')\n        dedented_thinking = textwrap.dedent(current_thinking).strip()\n\n        # Determine if this is the conclusion step based on description\n        is_conclusion = False\n        if isinstance(plan_list, list):\n             # Check if the currently executed step (likely the last 'Done' or the current 'Pending' if evaluation failed) is Conclusion\n             # This logic is approximate - might need refinement based on how LLM handles status updates\n             for item in reversed(plan_list): # Check recent items first\n                 if isinstance(item, dict) and item.get('description') == \"Conclusion\":\n                     # If Conclusion is Done or it's Pending and we are ending, consider it conclusion\n                     if item.get('status') == \"Done\" or (item.get('status') == \"Pending\" and not exec_res.get(\"next_thought_needed\", True)):\n                         is_conclusion = True\n                         break\n                 # Simple check, might need nested search if Conclusion could be a sub-step\n\n        # Use is_conclusion flag OR the next_thought_needed flag for termination\n        if not exec_res.get(\"next_thought_needed\", True): # Primary termination signal\n            shared[\"solution\"] = dedented_thinking # Solution is the thinking content of the final step\n            print(f\"\\nThought {thought_num} (Conclusion):\")\n            print(f\"{textwrap.indent(dedented_thinking, '  ')}\")\n            print(\"\\nFinal Plan Status:\")\n            print(textwrap.indent(plan_str_formatted, '  '))\n            print(\"\\n=== FINAL SOLUTION ===\")\n            print(dedented_thinking)\n            print(\"======================\\n\")\n            return \"end\"\n\n        # Otherwise, continue the chain\n        print(f\"\\nThought {thought_num}:\")\n        print(f\"{textwrap.indent(dedented_thinking, '  ')}\")\n        print(\"\\nCurrent Plan Status:\")\n        print(textwrap.indent(plan_str_formatted, '  '))\n        print(\"-\" * 50)\n\n        return \"continue\"",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-thinking/nodes.py`:**\n\n1. **Primary Purpose**  \nThis code defines a custom node (`ChainOfThoughtNode`) for a \"chain of thought\" reasoning system, facilitating iterative problem solving using structured plans. It manages the process of generating, executing, evaluating, and refining multi-step plans through multiple reasoning steps (thoughts), each potentially breaking complex tasks into sub-steps, using an LLM (such as GPT) as an engine. The conversation, plan status, and thinking process are tracked and recursively updated until problem resolution.\n\n2. **Parameters (for main Node class methods):**  \n- `prep(self, shared)`:  \n  - `shared`: A dictionary containing shared context, including the problem description, previous thoughts, and counters.\n- `exec(self, prep_res)`:  \n  - `prep_res`: Output of `prep`, containing formatted context and current status.\n- `post(self, shared, prep_res, exec_res)`:  \n  - `shared`: Shared context dictionary (mutable, for updating state).\n  - `prep_res`: Output from `prep`.\n  - `exec_res`: Output from `exec` (the current thought's data).\n\n3. **Return Values:**  \n- `prep`: Returns a dictionary with the problem, formatted thoughts, last plan (both string and structured), incremented thought number, and a flag indicating if this is the first thought.\n- `exec`: Returns a dictionary representing the newly generated \"thought\" from the LLM, with fields for current thinking, an updated plan, whether more steps are needed, and the current thought number.\n- `post`: Returns either the string `\"end\"` (if the solution is reached) or `\"continue\"` (to proceed to the next reasoning step).\n\n4. **Internal Function/Method Calls:**  \n- `format_plan` (helper): Formats a plan structure (list of dictionaries or sub-steps) for human-readable printing.\n- `format_plan_for_prompt` (helper): Formats the plan in a concise, prompt-friendly style for LLM input.\n- `textwrap.dedent` and `textwrap.indent` (standard lib): Improve readability of multi-line strings.\n- `call_llm` (from `utils`): Sends an LLM prompt and retrieves the model's response.\n- `yaml.safe_load` (PyYAML): Parses the LLM's YAML response.\n- Various list/dict manipulations.\n- Standard print statements for feedback."
    },
    "145": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-thinking/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import ChainOfThoughtNode\n\ndef create_chain_of_thought_flow():\n    # Create a ChainOfThoughtNode\n    cot_node = ChainOfThoughtNode(max_retries=3, wait=10)\n    \n    # Connect the node to itself for the \"continue\" action\n    cot_node - \"continue\" >> cot_node\n    \n    # Create the flow\n    cot_flow = Flow(start=cot_node)\n    return cot_flow",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary Purpose:**  \n   This code defines a function to create a chain-of-thought workflow using a node-based system, specifically leveraging a `ChainOfThoughtNode` and the `Flow` orchestrator from PocketFlow. The structure allows the workflow to repeat or continue based on specific actions.\n\n2. **Parameters:**  \n   The function `create_chain_of_thought_flow` takes no parameters.\n\n3. **Return Value:**  \n   The function returns a `Flow` object (`cot_flow`), which has its start node set to a `ChainOfThoughtNode` configured for possible self-loops on a \"continue\" action.\n\n4. **Functions/Methods Called Internally:**  \n   - `ChainOfThoughtNode(...)`: Instantiates a chain-of-thought processing node.\n   - `cot_node - \"continue\" >> cot_node`: Configures the node to loop back to itself on \"continue\".\n   - `Flow(start=cot_node)`: Creates a new `Flow` object using the specified start node."
    },
    "146": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-thinking/main.py",
      "code": "import sys\nfrom flow import create_chain_of_thought_flow\n\ndef main():\n    # Default question\n    default_question = \"You keep rolling a fair die until you roll three, four, five in that order consecutively on three rolls. What is the probability that you roll the die an odd number of times?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    print(f\"\ud83e\udd14 Processing question: {question}\")   \n\n    # Create the flow\n    cot_flow = create_chain_of_thought_flow()\n\n    # Set up shared state\n    shared = {\n        \"problem\": question,\n        \"thoughts\": [],\n        \"current_thought_number\": 0,\n        \"total_thoughts_estimate\": 10,\n        \"solution\": None\n    }\n    \n    # Run the flow\n    cot_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of Code Unit: main.py**\n\n1. **Primary Purpose:**  \n   The code serves as the entry point for a command-line application that processes a \"chain of thought\" solution flow for a given question (defaulting to a probability problem involving rolling dice). It sets up the problem, initializes shared state, and runs a reasoning workflow.\n\n2. **Parameters:**  \n   - The main function does not take any direct parameters, but it can receive a custom question from the command-line arguments prefixed with `--`. Otherwise, it uses a predefined default question.\n\n3. **Return Value:**  \n   - The code does not return any value from its main function; it produces output as side effects (prints to the console and possibly other processing via the flow).\n\n4. **Functions/Methods Called Internally:**  \n   - `create_chain_of_thought_flow()` (imported from `flow`)\n   - `cot_flow.run(shared)` (method of the object created by the above function)\n   - Standard library functions: `print` and methods from the `sys` module for command-line argument processing.\n\n**Summary:**  \nThe script sets up and executes a \"chain of thought\" workflow for solving a user-provided or default question, tracking the reasoning steps in shared state and displaying processing updates to the user."
    },
    "147": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-thinking/utils.py",
      "code": "from anthropic import Anthropic\nimport os\n\ndef call_llm(prompt):\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=6000,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[0].text\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-thinking/utils.py`**\n\n1. **Primary Purpose:**  \n   The code defines a utility function to send prompts to Anthropic's Claude language model API and retrieve the generated response. It includes a simple main block that demonstrates this interaction for testing purposes.\n\n2. **Parameters:**  \n   - The main function `call_llm(prompt)` takes a single parameter:\n     - `prompt` (str): The input string/question to be sent to the language model.\n\n3. **Return Value:**  \n   - The function returns the model's response to the given prompt as a string (pulled from `response.content[0].text`).\n\n4. **Other Functions/Methods Called Internally:**  \n   - `Anthropic()` constructor from the `anthropic` package (to create an API client)\n   - `os.environ.get()` (to fetch the API key from environment variables)\n   - `client.messages.create()` (to send the prompt and receive a response from the model)\n   - Access to `response.content[0].text` (to extract the response text)\n   - In the main block, `print()` is used for demonstration purposes."
    },
    "148": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-majority-vote/main.py",
      "code": "import argparse\nfrom pocketflow import BatchNode, Flow\nimport collections\nfrom utils import call_llm\nimport yaml\n\nclass MajorityVoteNode(BatchNode):\n    def prep(self, shared):\n        question = shared.get(\"question\", \"(No question provided)\")\n        attempts_count = shared.get(\"num_tries\", 3)\n        return [question for _ in range(attempts_count)]\n\n    def exec(self, single_question: str):\n        prompt = f\"\"\"\nYou are a helpful assistant. Please answer the user's question below.\nQuestion: {single_question}\n\nReturn strictly using the following YAML structure:\n```yaml\nthinking: |\n    (Your thinking process here)\nanswer: 0.123 # Final answer as a decimal with 3 decimal places\n```\"\"\"\n        raw_response = call_llm(prompt)\n        yaml_part = raw_response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        parsed = yaml.safe_load(yaml_part)\n\n        # Validate we have at least 'answer' field\n        if not isinstance(parsed, dict) or 'answer' not in parsed:\n            raise RuntimeError(f\"Missing 'answer' in YAML: {parsed}\")\n\n        # Return only the 'answer' field for the majority vote.\n        return str(parsed['answer'])\n    \n    def exec_fallback(self, prep_res, exc):\n        return None\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Count frequency for non-None answers\n        exec_res_list = [res for res in exec_res_list if res is not None]\n        counter = collections.Counter(exec_res_list)\n        best_answer, freq = counter.most_common(1)[0]\n\n        # Store final\n        shared[\"majority_answer\"] = best_answer\n\n        print(\"========================\")\n        print(\"All structured answers:\", exec_res_list)\n        print(\"Majority vote =>\", best_answer)\n        print(\"Frequency =>\", freq)\n        print(\"========================\")\n\n        # End the flow\n        return \"end\"\n\nif __name__ == \"__main__\":\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description=\"Run majority vote reasoning on a problem\")\n    parser.add_argument(\"--problem\", type=str, help=\"Your reasoning problem to solve\")\n    parser.add_argument(\"--tries\", type=int, default=5, help=\"Number of attempts to make (default: 5)\")\n    args = parser.parse_args()\n    \n    # Default problem if none provided\n    default_problem = \"\"\"You work at a shoe factory. In front of you, there are three pairs of shoes (six individual shoes) with the following sizes: two size 4s, two size 5s, and two size 6s. The factory defines an \"acceptable pair\" as two shoes that differ in size by a maximum of one size (e.g., a size 5 and a size 6 would be an acceptable pair). If you close your eyes and randomly pick three pairs of shoes without replacement, what is the probability that you end up drawing three acceptable pairs?\"\"\"\n    \n    shared = {\n        \"question\": args.problem if args.problem else default_problem,\n        \"num_tries\": args.tries\n    }\n\n    majority_node = MajorityVoteNode()\n    flow = Flow(start=majority_node)\n    flow.run(shared)\n\n    print(\"\\n=== Final Answer ===\")\n    print(shared[\"majority_answer\"])\n    print(\"====================\")",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-majority-vote/main.py`:**\n\n1. **Primary Purpose**:  \n   This code implements a \"majority vote\" framework for reasoning with language models. Given a question (or \"problem\"), it automatically prompts a language model (via `call_llm`) multiple times for structured YAML-formatted answers, collects the results, and outputs the answer that occurs most frequently among the attempts.\n\n2. **Parameters (for main execution):**  \n   - `--problem` (str, optional): The reasoning problem/question to solve. If omitted, a default logic puzzle about matching pairs of shoes is used.\n   - `--tries` (int, optional, default=5): The number of independent LLM attempts to make (i.e., how many times the question is asked for a majority vote).\n\n3. **Return Value:**  \n   The script does not return a value, but as a side effect, it prints the most frequent (\"majority\") answer found among LLM attempts to stdout, and stores it in the `shared` dictionary as `shared[\"majority_answer\"]`.\n\n4. **Internally Called Functions/Methods:**\n   - **`call_llm(prompt)`** (from `utils`): Interfaces with an LLM to get an answer for a prompt.\n   - **`yaml.safe_load()`**: Parses YAML from the LLM's response.\n   - **Python built-in/standard library:**  \n     - `collections.Counter` for tallying responses.\n     - `argparse.ArgumentParser` and its methods for CLI parsing.\n     - `print` for output.\n   - **Framework Classes:**  \n     - `BatchNode` and `Flow` (from `pocketflow`): For orchestrating repeated prompt execution and post-processing.\n\n**In brief:**  \nThis code repeatedly queries an LLM for a structured answer to a reasoned question, collects the responses, and outputs the answer that is returned most frequently, supporting configurable input and number of attempts via command line."
    },
    "149": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-majority-vote/utils.py",
      "code": "from anthropic import Anthropic\nimport os\n\ndef call_llm(prompt):\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=10000,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[0].text\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")",
      "summary": "**Summary of Code Unit: `/tmp/PocketFlow/cookbook/pocketflow-majority-vote/utils.py`**\n\n1. **Primary Purpose**:  \n   The code provides a utility function, `call_llm`, to send prompts to Anthropic's Claude LLM API and retrieve the generated response. It also contains a main block to test this function interactively.\n\n2. **Parameters**:  \n   - `prompt` (string): The text input (question or instruction) that will be sent to the language model as the user's message.\n\n3. **Return Value**:  \n   - Returns the generated text response (string) from Claude, specifically the text content of the model's reply to the given prompt.\n\n4. **Functions and Methods Called Internally**:  \n   - `Anthropic()` constructor (from the `anthropic` Python SDK)\n   - `os.environ.get()` (to read the API key from environment variables)\n   - `client.messages.create()` (SDK method to interact with the LLM)\n   - The main block also uses Python\u2019s built-in `print()` function for demonstration."
    },
    "150": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-supervisor/nodes.py",
      "code": "from pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\nimport random\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nsearch_query: <specific search query if action is search>\n```\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass UnreliableAnswerNode(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer with 50% chance of returning a dummy answer.\"\"\"\n        question, context = inputs\n        \n        # 50% chance to return a dummy answer\n        if random.random() < 0.5:\n            print(f\"\ud83e\udd2a Generating unreliable dummy answer...\")\n            return \"Sorry, I'm on a coffee break right now. All information I provide is completely made up anyway. The answer to your question is 42, or maybe purple unicorns. Who knows? Certainly not me!\"\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n\nclass SupervisorNode(Node):\n    def prep(self, shared):\n        \"\"\"Get the current answer for evaluation.\"\"\"\n        return shared[\"answer\"]\n    \n    def exec(self, answer):\n        \"\"\"Check if the answer is valid or nonsensical.\"\"\"\n        print(f\"    \ud83d\udd0d Supervisor checking answer quality...\")\n        \n        # Check for obvious markers of the nonsense answers\n        nonsense_markers = [\n            \"coffee break\", \n            \"purple unicorns\", \n            \"made up\", \n            \"42\", \n            \"Who knows?\"\n        ]\n        \n        # Check if the answer contains any nonsense markers\n        is_nonsense = any(marker in answer for marker in nonsense_markers)\n        \n        if is_nonsense:\n            return {\"valid\": False, \"reason\": \"Answer appears to be nonsensical or unhelpful\"}\n        else:\n            return {\"valid\": True, \"reason\": \"Answer appears to be legitimate\"}\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Decide whether to accept the answer or restart the process.\"\"\"\n        if exec_res[\"valid\"]:\n            print(f\"    \u2705 Supervisor approved answer: {exec_res['reason']}\")\n        else:\n            print(f\"    \u274c Supervisor rejected answer: {exec_res['reason']}\")\n            # Clean up the bad answer\n            shared[\"answer\"] = None\n            # Add a note about the rejected answer\n            context = shared.get(\"context\", \"\")\n            shared[\"context\"] = context + \"\\n\\nNOTE: Previous answer attempt was rejected by supervisor.\"\n            return \"retry\" ",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary Purpose:**\n   - This code defines a pipeline of \"nodes\" that orchestrate a research assistant agent's workflow. The agent can decide (via LLM) whether to search the web or answer a question, perform the web search if needed, generate an answer (sometimes unreliably), and finally have a \"supervisor\" node check the answer's validity. The workflow is designed to cycle until a suitable answer is produced.\n\n2. **Parameters:**\n   - Each node operates primarily on a shared `shared` dictionary, which contains items like `\"question\"`, `\"context\"`, `\"search_query\"`, and `\"answer\"`.\n   - The `exec` method of each node uses data prepared in `prep`, usually taken from `shared`. For instance:  \n     - `DecideAction.exec(inputs)`: receives (question, context).\n     - `SearchWeb.exec(search_query)`: receives the search string.\n     - `UnreliableAnswerNode.exec(inputs)`: receives (question, context).\n     - `SupervisorNode.exec(answer)`: receives the answer string.\n\n3. **Return Value:**\n   - Each node's `exec` returns outputs for the next step in the workflow:\n     - `DecideAction`: returns a decision dictionary parsed from LLM (action: search/answer, reason, etc.).\n     - `SearchWeb`: returns search results.\n     - `UnreliableAnswerNode`: returns the answer string.\n     - `SupervisorNode`: returns validation result (`{\"valid\": bool, \"reason\": str}`).\n   - Each node's `post` often returns a string to determine what the next node in the flow will be (e.g., \"search\", \"answer\", \"decide\", \"retry\").\n\n4. **Internal Function/Method Calls:**\n   - **call_llm** (from utils): Used in `DecideAction` and `UnreliableAnswerNode` to interact with the language model.\n   - **search_web** (from utils): Used in `SearchWeb` to perform information retrieval.\n   - **yaml.safe_load** (from PyYAML): Used in `DecideAction` to parse the LLM's YAML-formatted output.\n   - **random.random** (from Python standard library): Used in `UnreliableAnswerNode` to randomly choose between a dummy answer and a genuine one.\n\n**In summary:**  \n`nodes.py` implements a flexible, modular research agent with logic for LLM-based decision-making, external search, variable reliability in answers, and answer supervision, tying them together in a reactive workflow. Data is passed and managed via a shared dictionary, and several utility functions are leveraged for LLM interaction and web search."
    },
    "151": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-supervisor/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, UnreliableAnswerNode, SupervisorNode\n\ndef create_agent_inner_flow():\n    \"\"\"\n    Create the inner research agent flow without supervision.\n    \n    This flow handles the research cycle:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node and return to decide\n    3. If answer, go to UnreliableAnswerNode\n    \n    Returns:\n        Flow: A research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = UnreliableAnswerNode()\n    \n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n    \n    # If DecideAction returns \"answer\", go to UnreliableAnswerNode\n    decide - \"answer\" >> answer\n    \n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n    \n    # Create and return the inner flow, starting with the DecideAction node\n    return Flow(start=decide)\n\ndef create_agent_flow():\n    \"\"\"\n    Create a supervised agent flow by treating the entire agent flow as a node\n    and placing the supervisor outside of it.\n    \n    The flow works like this:\n    1. Inner agent flow does research and generates an answer\n    2. SupervisorNode checks if the answer is valid\n    3. If answer is valid, flow completes\n    4. If answer is invalid, restart the inner agent flow\n    \n    Returns:\n        Flow: A complete research agent flow with supervision\n    \"\"\"\n    # Create the inner flow\n    agent_flow = create_agent_inner_flow()\n    \n    # Create the supervisor node\n    supervisor = SupervisorNode()\n    \n    # Connect the components\n    # After agent_flow completes, go to supervisor\n    agent_flow >> supervisor\n    \n    # If supervisor rejects the answer, go back to agent_flow\n    supervisor - \"retry\" >> agent_flow\n    \n    # Create and return the outer flow, starting with the agent_flow\n    return Flow(start=agent_flow) ",
      "summary": "**Summary of flow.py**\n\n1. **Primary purpose:**  \n   This code defines and constructs flexible research agent workflows for a PocketFlow-based application. It creates both an inner unsupervised research cycle and an outer supervised flow, where a supervisor reviews and can request retries of generated answers.\n\n2. **Parameters:**  \n   The functions defined in the code unit, `create_agent_inner_flow` and `create_agent_flow`, do not take any parameters.\n\n3. **Return value:**  \n   Both functions return a `Flow` object.  \n   - `create_agent_inner_flow` returns a research agent flow without supervision.  \n   - `create_agent_flow` returns a supervised agent flow, where generated answers are reviewed and possibly retried.\n\n4. **Internal function or method calls:**  \n   - Class constructors: `DecideAction()`, `SearchWeb()`, `UnreliableAnswerNode()`, `SupervisorNode()`\n   - Method or operator overloads for node connections (likely implemented in the `Flow` framework): `-` and `>>`\n   - Internal function call: `create_agent_inner_flow()` is called inside `create_agent_flow()`\n   - Constructor for creating flows: `Flow(start=...)`"
    },
    "152": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-supervisor/main.py",
      "code": "import sys\nfrom flow import create_agent_flow\n\ndef main():\n    \"\"\"Simple function to process a question with supervised answers.\"\"\"\n    # Default question\n    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    # Create the agent flow with supervision\n    agent_flow = create_agent_flow()\n    \n    # Process the question\n    shared = {\"question\": question}\n    print(f\"\ud83e\udd14 Processing question: {question}\")\n    agent_flow.run(shared)\n    print(\"\\n\ud83c\udfaf Final Answer:\")\n    print(shared.get(\"answer\", \"No answer found\"))\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-supervisor/main.py`:**\n\n1. **Primary Purpose:**  \n   This script serves as an entry point for a question-answering workflow using a supervised agent, processing a user-provided (or default) question and printing the resulting answer.\n\n2. **Parameters:**  \n   - The script can take an optional command-line argument in the form `--<question>`, which replaces the default question with one supplied by the user.\n\n3. **Return Value:**  \n   - The `main()` function does not return a value; it prints outputs directly to the console.\n\n4. **Other Functions/Methods Called Internally:**\n   - `sys.argv` (to access command-line arguments)\n   - `create_agent_flow()` (imported from `flow`; creates the agent flow object)\n   - `agent_flow.run(shared)` (runs the agent flow with the provided question)\n   - Standard `print()` function for output"
    },
    "153": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-supervisor/utils.py",
      "code": "from openai import OpenAI\nimport os\nfrom duckduckgo_search import DDGS\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef search_web(query):\n    results = DDGS().text(query, max_results=5)\n    # Convert results to a string\n    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n    return results_str\n    \nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n    print(\"## Testing search_web\")\n    query = \"Who won the Nobel Prize in Physics 2024?\"\n    print(f\"## Query: {query}\")\n    results = search_web(query)\n    print(f\"## Results: {results}\")",
      "summary": "**Summary of Code Unit: /tmp/PocketFlow/cookbook/pocketflow-supervisor/utils.py**\n\n1. **Primary Purpose:**  \n   The primary purpose of this code unit is to provide utility functions for interacting with OpenAI's language models and DuckDuckGo's web search API. It enables generating natural language responses from LLMs and performing web searches programmatically.\n\n2. **Description of Parameters:**  \n   - `call_llm(prompt)`:  \n     - **prompt** (str): The input message or question to be sent to the OpenAI language model.\n   - `search_web(query)`:  \n     - **query** (str): The search string to be queried using DuckDuckGo's search API.\n\n3. **Description of Return Values:**  \n   - `call_llm(prompt)`:  \n     - Returns a string containing the OpenAI model's response to the provided prompt.\n   - `search_web(query)`:  \n     - Returns a string aggregating up to 5 web search results, including each result\u2019s title, URL, and snippet.\n\n4. **Functions/Methods Called Internally:**  \n   - `OpenAI()` and related methods:  \n     - `OpenAI(api_key=...)`  \n     - `client.chat.completions.create()`\n   - `DDGS().text()`\n   - Built-in:  \n     - `os.environ.get()`\n     - Python string formatting and joining (list comprehensions, `\"\\n\\n\".join()`)\n     - `print()` (in the `__main__` test block)\n\nOverall, the module provides tested programmatic access to language model completions and summarized web search results."
    },
    "154": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-text2sql/nodes.py",
      "code": "import sqlite3\nimport time\nimport yaml # Import yaml here as nodes use it\nfrom pocketflow import Node\nfrom utils.call_llm import call_llm\n\nclass GetSchema(Node):\n    def prep(self, shared):\n        return shared[\"db_path\"]\n\n    def exec(self, db_path):\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = cursor.fetchall()\n        schema = []\n        for table_name_tuple in tables:\n            table_name = table_name_tuple[0]\n            schema.append(f\"Table: {table_name}\")\n            cursor.execute(f\"PRAGMA table_info({table_name});\")\n            columns = cursor.fetchall()\n            for col in columns:\n                schema.append(f\"  - {col[1]} ({col[2]})\")\n            schema.append(\"\")\n        conn.close()\n        return \"\\n\".join(schema).strip()\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"schema\"] = exec_res\n        print(\"\\n===== DB SCHEMA =====\\n\")\n        print(exec_res)\n        print(\"\\n=====================\\n\")\n        # return \"default\"\n\nclass GenerateSQL(Node):\n    def prep(self, shared):\n        return shared[\"natural_query\"], shared[\"schema\"]\n\n    def exec(self, prep_res):\n        natural_query, schema = prep_res\n        prompt = f\"\"\"\nGiven SQLite schema:\n{schema}\n\nQuestion: \"{natural_query}\"\n\nRespond ONLY with a YAML block containing the SQL query under the key 'sql':\n```yaml\nsql: |\n  SELECT ...\n```\"\"\"\n        llm_response = call_llm(prompt)\n        yaml_str = llm_response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        structured_result = yaml.safe_load(yaml_str)\n        sql_query = structured_result[\"sql\"].strip().rstrip(';')\n        return sql_query\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is now the parsed SQL query string\n        shared[\"generated_sql\"] = exec_res\n        # Reset debug attempts when *successfully* generating new SQL\n        shared[\"debug_attempts\"] = 0\n        print(f\"\\n===== GENERATED SQL (Attempt {shared.get('debug_attempts', 0) + 1}) =====\\n\")\n        print(exec_res)\n        print(\"\\n====================================\\n\")\n        # return \"default\"\n\nclass ExecuteSQL(Node):\n    def prep(self, shared):\n        return shared[\"db_path\"], shared[\"generated_sql\"]\n\n    def exec(self, prep_res):\n        db_path, sql_query = prep_res\n        try:\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n            start_time = time.time()\n            cursor.execute(sql_query)\n\n            is_select = sql_query.strip().upper().startswith((\"SELECT\", \"WITH\"))\n            if is_select:\n                results = cursor.fetchall()\n                column_names = [desc[0] for desc in cursor.description] if cursor.description else []\n            else:\n                conn.commit()\n                results = f\"Query OK. Rows affected: {cursor.rowcount}\"\n                column_names = []\n            conn.close()\n            duration = time.time() - start_time\n            print(f\"SQL executed in {duration:.3f} seconds.\")\n            return (True, results, column_names)\n        except sqlite3.Error as e:\n            print(f\"SQLite Error during execution: {e}\")\n            if 'conn' in locals() and conn:\n                 try:\n                     conn.close()\n                 except Exception:\n                     pass\n            return (False, str(e), [])\n\n    def post(self, shared, prep_res, exec_res):\n        success, result_or_error, column_names = exec_res\n\n        if success:\n            shared[\"final_result\"] = result_or_error\n            shared[\"result_columns\"] = column_names\n            print(\"\\n===== SQL EXECUTION SUCCESS =====\\n\")\n            # (Same result printing logic as before)\n            if isinstance(result_or_error, list):\n                 if column_names: print(\" | \".join(column_names)); print(\"-\" * (sum(len(str(c)) for c in column_names) + 3 * (len(column_names) -1)))\n                 if not result_or_error: print(\"(No results found)\")\n                 else:\n                     for row in result_or_error: print(\" | \".join(map(str, row)))\n            else: print(result_or_error)\n            print(\"\\n=================================\\n\")\n            return\n        else:\n            # Execution failed (SQLite error caught in exec)\n            shared[\"execution_error\"] = result_or_error # Store the error message\n            shared[\"debug_attempts\"] = shared.get(\"debug_attempts\", 0) + 1\n            max_attempts = shared.get(\"max_debug_attempts\", 3) # Get max attempts from shared\n\n            print(f\"\\n===== SQL EXECUTION FAILED (Attempt {shared['debug_attempts']}) =====\\n\")\n            print(f\"Error: {shared['execution_error']}\")\n            print(\"=========================================\\n\")\n\n            if shared[\"debug_attempts\"] >= max_attempts:\n                print(f\"Max debug attempts ({max_attempts}) reached. Stopping.\")\n                shared[\"final_error\"] = f\"Failed to execute SQL after {max_attempts} attempts. Last error: {shared['execution_error']}\"\n                return\n            else:\n                print(\"Attempting to debug the SQL...\")\n                return \"error_retry\" # Signal to go to DebugSQL\n\nclass DebugSQL(Node):\n    def prep(self, shared):\n        return (\n            shared.get(\"natural_query\"),\n            shared.get(\"schema\"),\n            shared.get(\"generated_sql\"),\n            shared.get(\"execution_error\")\n        )\n\n    def exec(self, prep_res):\n        natural_query, schema, failed_sql, error_message = prep_res\n        prompt = f\"\"\"\nThe following SQLite SQL query failed:\n```sql\n{failed_sql}\n```\nIt was generated for: \"{natural_query}\"\nSchema:\n{schema}\nError: \"{error_message}\"\n\nProvide a corrected SQLite query.\n\nRespond ONLY with a YAML block containing the corrected SQL under the key 'sql':\n```yaml\nsql: |\n  SELECT ... -- corrected query\n```\"\"\"\n        llm_response = call_llm(prompt)\n\n        yaml_str = llm_response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        structured_result = yaml.safe_load(yaml_str)\n        corrected_sql = structured_result[\"sql\"].strip().rstrip(';')\n        return corrected_sql\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is the corrected SQL string\n        shared[\"generated_sql\"] = exec_res # Overwrite with the new attempt\n        shared.pop(\"execution_error\", None) # Clear the previous error for the next ExecuteSQL attempt\n\n        print(f\"\\n===== REVISED SQL (Attempt {shared.get('debug_attempts', 0) + 1}) =====\\n\")\n        print(exec_res)\n        print(\"\\n====================================\\n\")",
      "summary": "Certainly! Here\u2019s a concise summary of the code unit **nodes.py**:\n\n---\n\n**1. Primary Purpose**  \nThis code unit defines a sequence of Node classes that orchestrate an automated pipeline for **converting a natural language question into a SQL query, executing it against an SQLite database, and handling potential errors using an LLM (Large Language Model) for both query generation and correction**. This is a component of a text-to-SQL workflow.\n\n**2. Parameters (per-node basis):**\n\n- **GetSchema**\n  - Input: `shared[\"db_path\"]` (path to SQLite database)\n- **GenerateSQL**\n  - Inputs: \n    - `shared[\"natural_query\"]` (user question in natural language)\n    - `shared[\"schema\"]` (string schema representation)\n- **ExecuteSQL**\n  - Inputs: \n    - `shared[\"db_path\"]`\n    - `shared[\"generated_sql\"]` (SQL query to run)\n- **DebugSQL**\n  - Inputs: \n    - `shared.get(\"natural_query\")`\n    - `shared.get(\"schema\")`\n    - `shared.get(\"generated_sql\")`\n    - `shared.get(\"execution_error\")`\n\nAll communication between nodes occurs through the shared state dictionary.\n\n**3. Return Values (per-node basis):**\n- **GetSchema:**  \n  Returns a string representation of the schema (table/column list).\n- **GenerateSQL:**  \n  Returns the generated SQL string, parsed from a YAML response from LLM.\n- **ExecuteSQL:**  \n  Returns a tuple:  \n  - (True, query results, column names) on success  \n  - (False, error message, empty list) on failure\n- **DebugSQL:**  \n  Returns a corrected SQL string, parsed from LLM's YAML response.\n\nAdditional information (like errors, results, debug attempts) is stored in the shared dictionary to pass context between steps.\n\n**4. Other Functions/Methods Called Internally:**\n- **sqlite3.connect()** (and associated cursor methods like execute, fetchall, commit)\n- **print()** (for logging outputs)\n- **call_llm()** (external utility for LLM interaction\u2014generates/corrects SQL or parses YAML responses)\n- **yaml.safe_load()** (parses YAML-formatted model responses)\n- **time.time()** (for query execution timing)\n\n---\n\n**Summary:**  \nThe file implements a multi-step, robust text-to-SQL pipeline: extracting a DB schema, generating a SQL query from natural language via LLM, executing the query, and, if errors occur, debugging/correcting the SQL using LLM feedback\u2014all using a shared in-memory state to coordinate between steps."
    },
    "155": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-text2sql/flow.py",
      "code": "from pocketflow import Flow, Node\nfrom nodes import GetSchema, GenerateSQL, ExecuteSQL, DebugSQL\n\ndef create_text_to_sql_flow():\n    \"\"\"Creates the text-to-SQL workflow with a debug loop.\"\"\"\n    get_schema_node = GetSchema()\n    generate_sql_node = GenerateSQL()\n    execute_sql_node = ExecuteSQL()\n    debug_sql_node = DebugSQL()\n\n    # Define the main flow sequence using the default transition operator\n    get_schema_node >> generate_sql_node >> execute_sql_node\n\n    # --- Define the debug loop connections using the correct operator ---\n    # If ExecuteSQL returns \"error_retry\", go to DebugSQL\n    execute_sql_node - \"error_retry\" >> debug_sql_node\n\n    # If DebugSQL returns \"default\", go back to ExecuteSQL\n    # debug_sql_node - \"default\" >> execute_sql_node # Explicitly for \"default\"\n    # OR using the shorthand for default:\n    debug_sql_node >> execute_sql_node\n\n    # Create the flow\n    text_to_sql_flow = Flow(start=get_schema_node)\n    return text_to_sql_flow",
      "summary": "**Summary of `flow.py` from `/tmp/PocketFlow/cookbook/pocketflow-text2sql/flow.py`:**\n\n1. **Primary Purpose:**  \n   The code defines a function to construct a text-to-SQL workflow using a series of nodes, including a debug loop for error handling. The workflow automates the process of generating SQL from natural language, executing it, and handling errors via debugging.\n\n2. **Parameters:**  \n   - The main function `create_text_to_sql_flow()` takes no parameters.\n\n3. **Return Value:**  \n   - Returns a `Flow` object (`text_to_sql_flow`) representing the configured text-to-SQL automation sequence.\n\n4. **Functions/Methods Called Internally:**  \n   - Class constructors: `GetSchema()`, `GenerateSQL()`, `ExecuteSQL()`, `DebugSQL()`\n   - Operator overloading for workflow construction:\n     - `>>` (sequence operator for connecting nodes)\n     - `- \"error_retry\" >> debug_sql_node` (conditional transition based on execution result)\n   - `Flow(start=...)` to create the workflow object."
    },
    "156": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-text2sql/main.py",
      "code": "import sys\nimport os\nfrom flow import create_text_to_sql_flow\nfrom populate_db import populate_database, DB_FILE\n\ndef run_text_to_sql(natural_query, db_path=DB_FILE, max_debug_retries=3):\n    if not os.path.exists(db_path) or os.path.getsize(db_path) == 0:\n        print(f\"Database at {db_path} missing or empty. Populating...\")\n        populate_database(db_path)\n\n    shared = {\n        \"db_path\": db_path,\n        \"natural_query\": natural_query,\n        \"max_debug_attempts\": max_debug_retries,\n        \"debug_attempts\": 0,\n        \"final_result\": None,\n        \"final_error\": None\n    }\n\n    print(f\"\\n=== Starting Text-to-SQL Workflow ===\")\n    print(f\"Query: '{natural_query}'\")\n    print(f\"Database: {db_path}\")\n    print(f\"Max Debug Retries on SQL Error: {max_debug_retries}\")\n    print(\"=\" * 45)\n\n    flow = create_text_to_sql_flow()\n    flow.run(shared) # Let errors inside the loop be handled by the flow logic\n\n    # Check final state based on shared data\n    if shared.get(\"final_error\"):\n            print(\"\\n=== Workflow Completed with Error ===\")\n            print(f\"Error: {shared['final_error']}\")\n    elif shared.get(\"final_result\") is not None:\n            print(\"\\n=== Workflow Completed Successfully ===\")\n            # Result already printed by ExecuteSQL node\n    else:\n            # Should not happen if flow logic is correct and covers all end states\n            print(\"\\n=== Workflow Completed (Unknown State) ===\")\n\n    print(\"=\" * 36)\n    return shared\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        query = \" \".join(sys.argv[1:])\n    else:\n        query = \"total products per category\"\n\n    run_text_to_sql(query) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-text2sql/main.py`:**\n\n1. **Primary Purpose:**  \n   The code serves as a command-line entry point for a text-to-SQL workflow. It takes a natural language query, ensures the underlying database is initialized, and coordinates the execution of an automated pipeline that converts text queries into SQL and executes them, managing retries in the case of SQL errors.\n\n2. **Parameters:**  \n   - The main function `run_text_to_sql` accepts:\n     - `natural_query` (str): A natural language question to be converted to SQL and executed.\n     - `db_path` (str, optional): The path to the SQLite database file (defaults to a constant `DB_FILE`).\n     - `max_debug_retries` (int, optional): The maximum number of debug attempts on SQL error (defaults to 3).\n\n3. **Return Value:**  \n   - The function returns the `shared` dictionary, which includes:\n     - Input parameters and state (like `db_path`, `natural_query`, `max_debug_attempts`, `debug_attempts`)\n     - The final result (`final_result`) if successful\n     - The final error (`final_error`) if failed\n\n4. **Functions and Methods Called Internally:**  \n   - `os.path.exists` and `os.path.getsize`: To check for the existence and content of the database.\n   - `populate_database`: To initialize the database if missing or empty.\n   - `create_text_to_sql_flow`: To obtain the workflow pipeline for text-to-SQL conversion and execution.\n   - `flow.run`: To execute the workflow using the shared state.\n   - Various print statements for CLI feedback.\n\n**Command-line Execution:**  \nWhen run as a script, it gets the natural language query from command-line arguments (defaults to \u201ctotal products per category\u201d if none given) and initiates the workflow."
    },
    "157": {
      "unit_name": "populate_db.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-text2sql/populate_db.py",
      "code": "import sqlite3\nimport os\nimport random\nfrom datetime import datetime, timedelta\n\nDB_FILE = \"ecommerce.db\"\n\ndef populate_database(db_file=DB_FILE):\n    \"\"\"Creates and populates the SQLite database.\"\"\"\n    if os.path.exists(db_file):\n        os.remove(db_file)\n        print(f\"Removed existing database: {db_file}\")\n\n    conn = sqlite3.connect(db_file)\n    cursor = conn.cursor()\n\n    # Create Tables\n    cursor.execute(\"\"\"\n    CREATE TABLE customers (\n        customer_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        first_name TEXT NOT NULL,\n        last_name TEXT NOT NULL,\n        email TEXT UNIQUE NOT NULL,\n        registration_date DATE NOT NULL,\n        city TEXT,\n        country TEXT DEFAULT 'USA'\n    );\n    \"\"\")\n    print(\"Created 'customers' table.\")\n\n    cursor.execute(\"\"\"\n    CREATE TABLE products (\n        product_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        name TEXT NOT NULL,\n        description TEXT,\n        category TEXT NOT NULL,\n        price REAL NOT NULL CHECK (price > 0),\n        stock_quantity INTEGER NOT NULL DEFAULT 0 CHECK (stock_quantity >= 0)\n    );\n    \"\"\")\n    print(\"Created 'products' table.\")\n\n    cursor.execute(\"\"\"\n    CREATE TABLE orders (\n        order_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        customer_id INTEGER NOT NULL,\n        order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        status TEXT NOT NULL CHECK (status IN ('pending', 'processing', 'shipped', 'delivered', 'cancelled')),\n        total_amount REAL,\n        shipping_address TEXT,\n        FOREIGN KEY (customer_id) REFERENCES customers (customer_id)\n    );\n    \"\"\")\n    print(\"Created 'orders' table.\")\n\n    cursor.execute(\"\"\"\n    CREATE TABLE order_items (\n        order_item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        order_id INTEGER NOT NULL,\n        product_id INTEGER NOT NULL,\n        quantity INTEGER NOT NULL CHECK (quantity > 0),\n        price_per_unit REAL NOT NULL,\n        FOREIGN KEY (order_id) REFERENCES orders (order_id),\n        FOREIGN KEY (product_id) REFERENCES products (product_id)\n    );\n    \"\"\")\n    print(\"Created 'order_items' table.\")\n\n    # Insert Sample Data\n    customers_data = [\n        ('Alice', 'Smith', 'alice.s@email.com', '2023-01-15', 'New York', 'USA'),\n        ('Bob', 'Johnson', 'b.johnson@email.com', '2023-02-20', 'Los Angeles', 'USA'),\n        ('Charlie', 'Williams', 'charlie.w@email.com', '2023-03-10', 'Chicago', 'USA'),\n        ('Diana', 'Brown', 'diana.b@email.com', '2023-04-05', 'Houston', 'USA'),\n        ('Ethan', 'Davis', 'ethan.d@email.com', '2023-05-12', 'Phoenix', 'USA'),\n        ('Fiona', 'Miller', 'fiona.m@email.com', '2023-06-18', 'Philadelphia', 'USA'),\n        ('George', 'Wilson', 'george.w@email.com', '2023-07-22', 'San Antonio', 'USA'),\n        ('Hannah', 'Moore', 'hannah.m@email.com', '2023-08-30', 'San Diego', 'USA'),\n        ('Ian', 'Taylor', 'ian.t@email.com', '2023-09-05', 'Dallas', 'USA'),\n        ('Julia', 'Anderson', 'julia.a@email.com', '2023-10-11', 'San Jose', 'USA')\n    ]\n    cursor.executemany(\"INSERT INTO customers (first_name, last_name, email, registration_date, city, country) VALUES (?, ?, ?, ?, ?, ?)\", customers_data)\n    print(f\"Inserted {len(customers_data)} customers.\")\n\n    products_data = [\n        ('Laptop Pro', 'High-end laptop for professionals', 'Electronics', 1200.00, 50),\n        ('Wireless Mouse', 'Ergonomic wireless mouse', 'Accessories', 25.50, 200),\n        ('Mechanical Keyboard', 'RGB backlit mechanical keyboard', 'Accessories', 75.00, 150),\n        ('4K Monitor', '27-inch 4K UHD Monitor', 'Electronics', 350.00, 80),\n        ('Smartphone X', 'Latest generation smartphone', 'Electronics', 999.00, 120),\n        ('Coffee Maker', 'Drip coffee maker', 'Home Goods', 50.00, 300),\n        ('Running Shoes', 'Comfortable running shoes', 'Apparel', 90.00, 250),\n        ('Yoga Mat', 'Eco-friendly yoga mat', 'Sports', 30.00, 400),\n        ('Desk Lamp', 'Adjustable LED desk lamp', 'Home Goods', 45.00, 180),\n        ('Backpack', 'Durable backpack for travel', 'Accessories', 60.00, 220)\n    ]\n    cursor.executemany(\"INSERT INTO products (name, description, category, price, stock_quantity) VALUES (?, ?, ?, ?, ?)\", products_data)\n    print(f\"Inserted {len(products_data)} products.\")\n\n    orders_data = []\n    start_date = datetime.now() - timedelta(days=60)\n    order_statuses = ['pending', 'processing', 'shipped', 'delivered', 'cancelled']\n    for i in range(1, 21): # Create 20 orders\n        customer_id = random.randint(1, 10)\n        order_date = start_date + timedelta(days=random.randint(0, 59), hours=random.randint(0, 23))\n        status = random.choice(order_statuses)\n        shipping_address = f\"{random.randint(100, 999)} Main St, Anytown\"\n        orders_data.append((customer_id, order_date.strftime('%Y-%m-%d %H:%M:%S'), status, None, shipping_address)) # Total amount calculated later\n\n    cursor.executemany(\"INSERT INTO orders (customer_id, order_date, status, total_amount, shipping_address) VALUES (?, ?, ?, ?, ?)\", orders_data)\n    print(f\"Inserted {len(orders_data)} orders.\")\n\n    order_items_data = []\n    order_totals = {} # Keep track of totals per order\n    for order_id in range(1, 21):\n        num_items = random.randint(1, 4)\n        order_total = 0\n        for _ in range(num_items):\n            product_id = random.randint(1, 10)\n            quantity = random.randint(1, 5)\n            # Get product price\n            cursor.execute(\"SELECT price FROM products WHERE product_id = ?\", (product_id,))\n            price_per_unit = cursor.fetchone()[0]\n            order_items_data.append((order_id, product_id, quantity, price_per_unit))\n            order_total += quantity * price_per_unit\n        order_totals[order_id] = round(order_total, 2)\n\n    cursor.executemany(\"INSERT INTO order_items (order_id, product_id, quantity, price_per_unit) VALUES (?, ?, ?, ?)\", order_items_data)\n    print(f\"Inserted {len(order_items_data)} order items.\")\n\n    # Update order totals\n    for order_id, total_amount in order_totals.items():\n        cursor.execute(\"UPDATE orders SET total_amount = ? WHERE order_id = ?\", (total_amount, order_id))\n    print(\"Updated order totals.\")\n\n    conn.commit()\n    conn.close()\n    print(f\"Database '{db_file}' created and populated successfully.\")\n\nif __name__ == \"__main__\":\n    populate_database()",
      "summary": "**Summary of `populate_db.py`:**\n\n1. **Primary Purpose:**  \n   The script creates and populates a sample SQLite database (`ecommerce.db`) for an e-commerce scenario, including tables for customers, products, orders, and order items, and fills them with realistic sample data.\n\n2. **Parameters:**  \n   - The main function `populate_database` takes an optional parameter `db_file`, which is the path/filename of the SQLite database to create (defaults to `'ecommerce.db'`).\n\n3. **Return Value:**  \n   - The `populate_database` function does not explicitly return any value (returns `None`). Its effect is to create and populate the SQLite database file.\n\n4. **Functions/Methods Called Internally:**  \n   - `os.path.exists()`\n   - `os.remove()`\n   - `sqlite3.connect()`\n   - `conn.cursor()`\n   - `cursor.execute()`\n   - `cursor.executemany()`\n   - `cursor.fetchone()`\n   - `conn.commit()`\n   - `conn.close()`\n   - Random data generation utilities: `random.randint()`, `random.choice()`\n   - Date and time utilities: `datetime.now()`, `timedelta()`\n   - Standard Python features: string formatting, f-strings, list and dictionary usage\n\n**Note:**  \nThe script is meant to be run directly and does not require any input arguments when executed."
    },
    "158": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-text2sql/utils/call_llm.py",
      "code": "import os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-text2sql/utils/call_llm.py`:**\n\n1. **Primary purpose:**  \n   The code defines a utility function to interact with the OpenAI API, sending a text prompt to the GPT-4o language model and retrieving its response.\n\n2. **Parameters:**  \n   - The function `call_llm` takes a single parameter:  \n     - `prompt` (str): The text input or question to send to the language model.\n\n3. **Return value:**  \n   - The function returns the content of the model's response (str): the generated reply to the input prompt.\n\n4. **Functions/methods called internally:**  \n   - `os.environ.get()` (from the `os` module)\n   - `OpenAI()` constructor (from the `openai` package)  \n   - `client.chat.completions.create()` (method for generating a completion using the model)\n\n**Note:**  \nThere is also a main block that demonstrates usage by printing the model's response to the prompt \"Tell me a short joke\"."
    },
    "159": {
      "unit_name": "test_tracing.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/test_tracing.py",
      "code": "#!/usr/bin/env python3\n\"\"\"\nTest script for PocketFlow tracing functionality.\n\nThis script tests the tracing implementation to ensure it works correctly\nwith Langfuse integration.\n\"\"\"\n\nimport sys\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Add paths for imports\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\nsys.path.insert(0, os.path.dirname(__file__))\n\nfrom pocketflow import Node, Flow, AsyncNode, AsyncFlow\nfrom tracing import trace_flow, TracingConfig\nfrom utils import setup_tracing\n\n\nclass TestNode(Node):\n    \"\"\"Simple test node for tracing verification.\"\"\"\n\n    def prep(self, shared):\n        \"\"\"Test prep phase.\"\"\"\n        return shared.get(\"input\", \"test_input\")\n\n    def exec(self, prep_res):\n        \"\"\"Test exec phase.\"\"\"\n        return f\"processed_{prep_res}\"\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Test post phase.\"\"\"\n        shared[\"output\"] = exec_res\n        return \"default\"\n\n\nclass TestAsyncNode(AsyncNode):\n    \"\"\"Simple async test node for tracing verification.\"\"\"\n\n    async def prep_async(self, shared):\n        \"\"\"Test async prep phase.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate async work\n        return shared.get(\"input\", \"async_test_input\")\n\n    async def exec_async(self, prep_res):\n        \"\"\"Test async exec phase.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate async work\n        return f\"async_processed_{prep_res}\"\n\n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Test async post phase.\"\"\"\n        shared[\"output\"] = exec_res\n        return \"default\"\n\n\n@trace_flow(flow_name=\"TestSyncFlow\")\nclass TestSyncFlow(Flow):\n    \"\"\"Test synchronous flow with tracing.\"\"\"\n\n    def __init__(self):\n        super().__init__(start=TestNode())\n\n\n@trace_flow(flow_name=\"TestAsyncFlow\")\nclass TestAsyncFlow(AsyncFlow):\n    \"\"\"Test asynchronous flow with tracing.\"\"\"\n\n    def __init__(self):\n        super().__init__(start=TestAsyncNode())\n\n\ndef test_sync_flow():\n    \"\"\"Test synchronous flow tracing.\"\"\"\n    print(\"\ud83e\uddea Testing synchronous flow tracing...\")\n\n    flow = TestSyncFlow()\n    shared = {\"input\": \"sync_test_data\"}\n\n    print(f\"   Input: {shared}\")\n    result = flow.run(shared)\n    print(f\"   Output: {shared}\")\n    print(f\"   Result: {result}\")\n\n    # Verify the flow worked\n    assert \"output\" in shared\n    assert shared[\"output\"] == \"processed_sync_test_data\"\n    print(\"   \u2705 Sync flow test passed\")\n\n\nasync def test_async_flow():\n    \"\"\"Test asynchronous flow tracing.\"\"\"\n    print(\"\ud83e\uddea Testing asynchronous flow tracing...\")\n\n    flow = TestAsyncFlow()\n    shared = {\"input\": \"async_test_data\"}\n\n    print(f\"   Input: {shared}\")\n    result = await flow.run_async(shared)\n    print(f\"   Output: {shared}\")\n    print(f\"   Result: {result}\")\n\n    # Verify the flow worked\n    assert \"output\" in shared\n    assert shared[\"output\"] == \"async_processed_async_test_data\"\n    print(\"   \u2705 Async flow test passed\")\n\n\ndef test_configuration():\n    \"\"\"Test configuration loading and validation.\"\"\"\n    print(\"\ud83e\uddea Testing configuration...\")\n\n    # Test loading from environment\n    config = TracingConfig.from_env()\n    print(f\"   Loaded config: debug={config.debug}\")\n\n    # Test validation\n    is_valid = config.validate()\n    print(f\"   Config valid: {is_valid}\")\n\n    if is_valid:\n        print(\"   \u2705 Configuration test passed\")\n    else:\n        print(\n            \"   \u26a0\ufe0f Configuration test failed (this may be expected if env vars not set)\"\n        )\n\n\ndef test_error_handling():\n    \"\"\"Test error handling in traced flows.\"\"\"\n    print(\"\ud83e\uddea Testing error handling...\")\n\n    class ErrorNode(Node):\n        def exec(self, prep_res):\n            raise ValueError(\"Test error for tracing\")\n\n    @trace_flow(flow_name=\"TestErrorFlow\")\n    class ErrorFlow(Flow):\n        def __init__(self):\n            super().__init__(start=ErrorNode())\n\n    flow = ErrorFlow()\n    shared = {\"input\": \"error_test\"}\n\n    try:\n        flow.run(shared)\n        print(\"   \u274c Expected error but flow succeeded\")\n    except ValueError as e:\n        print(f\"   \u2705 Error correctly caught and traced: {e}\")\n    except Exception as e:\n        print(f\"   \u26a0\ufe0f Unexpected error type: {e}\")\n\n\nasync def main():\n    \"\"\"Run all tests.\"\"\"\n    print(\"\ud83d\ude80 Starting PocketFlow Tracing Tests\")\n    print(\"=\" * 50)\n\n    # Test configuration first\n    test_configuration()\n    print()\n\n    # Test setup (optional - only if environment is configured)\n    try:\n        print(\"\ud83d\udd27 Testing setup...\")\n        config = setup_tracing()\n        print(\"   \u2705 Setup test passed\")\n    except Exception as e:\n        print(f\"   \u26a0\ufe0f Setup test failed: {e}\")\n        print(\"   (This is expected if Langfuse is not configured)\")\n    print()\n\n    # Test sync flow\n    test_sync_flow()\n    print()\n\n    # Test async flow\n    await test_async_flow()\n    print()\n\n    # Test error handling\n    test_error_handling()\n    print()\n\n    print(\"\ud83c\udf89 All tests completed!\")\n    print(\"\\n\ud83d\udcca If Langfuse is configured, check your dashboard for traces:\")\n    langfuse_host = os.getenv(\"LANGFUSE_HOST\", \"your-langfuse-host\")\n    print(f\"   Dashboard URL: {langfuse_host}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tracing/test_tracing.py`**\n\n1. **Primary Purpose:**  \n   The code is a test suite designed to verify the tracing functionality of the PocketFlow framework, particularly its integration with Langfuse for observability. It tests both synchronous and asynchronous flow execution, configuration, error handling, and tracing setup.\n\n2. **Parameters:**  \n   - Most test functions do not accept external parameters.  \n   - Internal parameters typically include shared data dictionaries (e.g., `shared = {\"input\": ...}`) used to mimic input data passed through flows and nodes.\n\n3. **Return Value:**  \n   - The main test functions (`test_sync_flow`, `test_async_flow`, `test_configuration`, `test_error_handling`, `main`) do not return explicit values\u2014their purpose is to print test results and raise assertions if tests fail.  \n   - Nodes' and flows' methods manipulate and return results within the code flow, but these are for internal logic rather than exposed return values.\n\n4. **Other Functions or Methods Called Internally:**  \n   - **PocketFlow/Core Imports:**\n     - `Node`, `Flow`, `AsyncNode`, `AsyncFlow` (base classes for test nodes and flows)\n   - **Tracing & Utility Functions:**\n     - `trace_flow` (decorator for tracing flows)\n     - `TracingConfig.from_env()` (loads tracing configuration from environment variables)\n     - `TracingConfig.validate()` (validates the loaded configuration)\n     - `setup_tracing()` (sets up Langfuse or tracing integration)\n   - **Testing Functions:**\n     - `flow.run(shared)` (runs a synchronous flow)\n     - `flow.run_async(shared)` (runs an asynchronous flow)\n   - **Python Standard Library:**\n     - `asyncio.sleep()` (used for simulating async work in nodes)\n     - `assert` statements (for validation within tests)\n     - Exception handling (`try/except`)\n     - Printing to console for progress and results\n   - **Miscellaneous:**\n     - Use of environment variables via `os.getenv`\n     - Use of `dotenv.load_dotenv()` for loading additional environment settings\n\n**In summary:**  \nThis script tests that the PocketFlow tracing integration (with Langfuse) works as expected for synchronous and asynchronous workflows, ensures configuration can be loaded and validated, verifies correct trace output for successful and error flows, and optionally sets up tracing if configured. All relevant methods for running and validating flows, as well as tracing setup, are invoked internally. No external parameters or return values are required; test outcomes are logged to the console."
    },
    "160": {
      "unit_name": "setup.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/setup.py",
      "code": "#!/usr/bin/env python3\n\"\"\"\nSetup script for PocketFlow Tracing cookbook.\n\nThis script helps install dependencies and verify the setup.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\n\n\ndef install_dependencies():\n    \"\"\"Install required dependencies.\"\"\"\n    print(\"\ud83d\udce6 Installing dependencies...\")\n    try:\n        subprocess.check_call(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"]\n        )\n        print(\"\u2705 Dependencies installed successfully!\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"\u274c Failed to install dependencies: {e}\")\n        return False\n\n\ndef verify_setup():\n    \"\"\"Verify that the setup is working.\"\"\"\n    print(\"\ud83d\udd0d Verifying setup...\")\n    try:\n        # Try to import the tracing module\n        from tracing import trace_flow, TracingConfig\n\n        print(\"\u2705 Tracing module imported successfully!\")\n\n        # Try to load configuration\n        config = TracingConfig.from_env()\n        if config.validate():\n            print(\"\u2705 Configuration is valid!\")\n        else:\n            print(\"\u26a0\ufe0f Configuration validation failed - check your .env file\")\n\n        return True\n    except ImportError as e:\n        print(f\"\u274c Failed to import tracing module: {e}\")\n        return False\n    except Exception as e:\n        print(f\"\u274c Setup verification failed: {e}\")\n        return False\n\n\ndef main():\n    \"\"\"Main setup function.\"\"\"\n    print(\"\ud83d\ude80 PocketFlow Tracing Setup\")\n    print(\"=\" * 40)\n\n    # Check if we're in the right directory\n    if not os.path.exists(\"requirements.txt\"):\n        print(\n            \"\u274c requirements.txt not found. Please run this script from the pocketflow-tracing directory.\"\n        )\n        sys.exit(1)\n\n    # Install dependencies\n    if not install_dependencies():\n        sys.exit(1)\n\n    # Verify setup\n    if not verify_setup():\n        sys.exit(1)\n\n    print(\"\\n\ud83c\udf89 Setup completed successfully!\")\n    print(\"\\n\ud83d\udcda Next steps:\")\n    print(\"1. Check the README.md for usage instructions\")\n    print(\"2. Run the examples: python examples/basic_example.py\")\n    print(\"3. Run the test suite: python test_tracing.py\")\n    print(\"4. Check your Langfuse dashboard (URL configured in LANGFUSE_HOST)\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tracing/setup.py`:**\n\n1. **Primary Purpose:**  \n   This script is a setup utility for the PocketFlow Tracing cookbook. Its main function is to automate the installation of dependencies (from requirements.txt) and verify that the tracing module and its configuration are correctly set up.\n\n2. **Parameters:**  \n   The script defines its functions (`install_dependencies`, `verify_setup`, `main`) without external parameters; it is intended to be run directly as a script and operates in the current directory.\n\n3. **Return Values:**  \n   - `install_dependencies()` returns `True` if dependencies are installed successfully, otherwise `False`.\n   - `verify_setup()` returns `True` if setup verification is successful, otherwise `False`.\n   - `main()` does not explicitly return a value; it orchestrates the setup and exits the script with `sys.exit(1)` upon errors.\n\n4. **Internal Function/Method Calls:**  \n   - `subprocess.check_call()` \u2014 executes pip install command for the requirements.\n   - `sys.exit()` \u2014 exits the script when errors occur.\n   - `os.path.exists()` \u2014 checks for the presence of `requirements.txt`.\n   - `tracing.trace_flow`, `tracing.TracingConfig`, and `TracingConfig.from_env()` \u2014 attempts to verify that the tracing module works.\n   - `TracingConfig.validate()` \u2014 validates the loaded configuration.\n   - Standard functions: `print()`, exception handling, and Python module imports.  \n   - Functions defined and called internally: `install_dependencies()`, `verify_setup()`, `main()`.\n\n**In summary, the script is an interactive setup tool that ensures all requirements for PocketFlow Tracing are installed and functional, giving users clear feedback at each step.**"
    },
    "161": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/utils/__init__.py",
      "code": "\"\"\"\nUtility functions for PocketFlow tracing.\n\"\"\"\n\nfrom .setup import setup_tracing, test_langfuse_connection\n\n__all__ = ['setup_tracing', 'test_langfuse_connection']\n",
      "summary": "**Summary:**\n\n1. **Primary Purpose:**  \nThis code unit serves as the `__init__.py` initializer for the `utils` module in PocketFlow's tracing utilities. Its main purpose is to expose the utility functions `setup_tracing` and `test_langfuse_connection` for external use by importing them from the submodule `setup`.\n\n2. **Parameters:**  \nThis code unit itself does not accept any parameters.\n\n3. **Return Value:**  \nThis code unit itself does not return any value.\n\n4. **Other Functions or Methods Called Internally:**  \nThe code imports (but does not itself call) the following functions from the local `setup` module:\n- `setup_tracing`\n- `test_langfuse_connection`\n\n**Note:** Actual invocation and parameter/return documentation of the utility functions would be found in `setup.py`, not in this `__init__.py` file."
    },
    "162": {
      "unit_name": "setup.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/utils/setup.py",
      "code": "\"\"\"\nSetup and testing utilities for PocketFlow tracing.\n\"\"\"\n\nimport os\nimport sys\nfrom typing import Optional\n\n# Add parent directory to path for imports\nsys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))\n\ntry:\n    from langfuse import Langfuse\n    LANGFUSE_AVAILABLE = True\nexcept ImportError:\n    LANGFUSE_AVAILABLE = False\n\nfrom tracing import TracingConfig, LangfuseTracer\n\n\ndef setup_tracing(env_file: Optional[str] = None) -> TracingConfig:\n    \"\"\"\n    Set up tracing configuration and validate the setup.\n    \n    Args:\n        env_file: Optional path to .env file. If None, uses default location.\n        \n    Returns:\n        TracingConfig instance.\n        \n    Raises:\n        RuntimeError: If setup fails.\n    \"\"\"\n    print(\"\ud83d\udd27 Setting up PocketFlow tracing...\")\n    \n    # Check if langfuse is installed\n    if not LANGFUSE_AVAILABLE:\n        raise RuntimeError(\n            \"Langfuse package not installed. Install with: pip install langfuse\"\n        )\n    \n    # Load configuration\n    if env_file:\n        config = TracingConfig.from_env(env_file)\n        print(f\"\u2713 Loaded configuration from: {env_file}\")\n    else:\n        config = TracingConfig.from_env()\n        print(\"\u2713 Loaded configuration from environment\")\n    \n    # Validate configuration\n    if not config.validate():\n        raise RuntimeError(\n            \"Invalid tracing configuration. Please check your environment variables:\\n\"\n            \"- LANGFUSE_SECRET_KEY\\n\"\n            \"- LANGFUSE_PUBLIC_KEY\\n\" \n            \"- LANGFUSE_HOST\"\n        )\n    \n    print(\"\u2713 Configuration validated\")\n    \n    # Test connection\n    if test_langfuse_connection(config):\n        print(\"\u2713 Langfuse connection successful\")\n    else:\n        raise RuntimeError(\"Failed to connect to Langfuse. Check your configuration and network.\")\n    \n    print(\"\ud83c\udf89 PocketFlow tracing setup complete!\")\n    return config\n\n\ndef test_langfuse_connection(config: TracingConfig) -> bool:\n    \"\"\"\n    Test connection to Langfuse.\n    \n    Args:\n        config: TracingConfig instance.\n        \n    Returns:\n        True if connection successful, False otherwise.\n    \"\"\"\n    try:\n        # Create a test tracer\n        tracer = LangfuseTracer(config)\n        \n        if not tracer.client:\n            return False\n        \n        # Try to start and end a test trace\n        trace_id = tracer.start_trace(\"test_connection\", {\"test\": True})\n        if trace_id:\n            tracer.end_trace({\"test\": \"completed\"}, \"success\")\n            tracer.flush()\n            return True\n        \n        return False\n        \n    except Exception as e:\n        if config.debug:\n            print(f\"Connection test failed: {e}\")\n        return False\n\n\ndef print_configuration_help():\n    \"\"\"Print help information for configuring tracing.\"\"\"\n    print(\"\"\"\n\ud83d\udd27 PocketFlow Tracing Configuration Help\n\nTo use PocketFlow tracing, you need to configure Langfuse credentials.\n\n1. Create or update your .env file with:\n\nLANGFUSE_SECRET_KEY=your-secret-key\nLANGFUSE_PUBLIC_KEY=your-public-key\nLANGFUSE_HOST=your-langfuse-host\nPOCKETFLOW_TRACING_DEBUG=true\n\n2. Optional configuration:\n\nPOCKETFLOW_TRACE_INPUTS=true\nPOCKETFLOW_TRACE_OUTPUTS=true\nPOCKETFLOW_TRACE_PREP=true\nPOCKETFLOW_TRACE_EXEC=true\nPOCKETFLOW_TRACE_POST=true\nPOCKETFLOW_TRACE_ERRORS=true\nPOCKETFLOW_SESSION_ID=your-session-id\nPOCKETFLOW_USER_ID=your-user-id\n\n3. Install required packages:\n\npip install -r requirements.txt\n\n4. Test your setup:\n\npython -c \"from utils import setup_tracing; setup_tracing()\"\n\"\"\")\n\n\nif __name__ == \"__main__\":\n    \"\"\"Command-line interface for setup and testing.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"PocketFlow Tracing Setup\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"Test Langfuse connection\")\n    parser.add_argument(\"--help-config\", action=\"store_true\", help=\"Show configuration help\")\n    parser.add_argument(\"--env-file\", type=str, help=\"Path to .env file\")\n    \n    args = parser.parse_args()\n    \n    if args.help_config:\n        print_configuration_help()\n        sys.exit(0)\n    \n    if args.test:\n        try:\n            config = setup_tracing(args.env_file)\n            print(\"\\n\u2705 All tests passed! Your tracing setup is ready.\")\n        except Exception as e:\n            print(f\"\\n\u274c Setup failed: {e}\")\n            print(\"\\nFor help with configuration, run:\")\n            print(\"python utils/setup.py --help-config\")\n            sys.exit(1)\n    else:\n        print_configuration_help()\n",
      "summary": "**Summary of `setup.py` in `/tmp/PocketFlow/cookbook/pocketflow-tracing/utils/`:**\n\n1. **Primary Purpose:**  \n   This code unit provides setup utilities and a command-line tool for configuring and testing the tracing integration for PocketFlow projects using Langfuse. It helps users load configuration, validate credentials, and verify connectivity to the Langfuse service.\n\n2. **Description of Parameters:**  \n   - The main setup function (`setup_tracing`) takes an optional `env_file` parameter (a string path to a `.env` file to load tracing configuration; if not provided, it uses environment variables).\n   - The command-line interface allows the user to provide `--test`, `--env-file`, and `--help-config` as parameters.\n\n3. **Description of Return Value:**  \n   - `setup_tracing` returns a `TracingConfig` instance if configuration and connection checks are successful.\n   - `test_langfuse_connection` returns a boolean indicating if the test trace succeeded.\n   - The script prints messages to the console and exits with appropriate status codes depending on outcomes.\n\n4. **Internal Functions/Methods Called:**  \n   - `TracingConfig.from_env` (to load configuration)\n   - `TracingConfig.validate` (to validate config)\n   - `LangfuseTracer` (to create a tracer)\n   - `LangfuseTracer.start_trace`, `LangfuseTracer.end_trace`, `LangfuseTracer.flush` (to test tracing connection)\n   - `test_langfuse_connection` (to check Langfuse connectivity)\n   - `print_configuration_help` (to print setup instructions)\n   - Standard library: `os`, `sys`, `argparse`, etc."
    },
    "163": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/tracing/__init__.py",
      "code": "\"\"\"\nPocketFlow Tracing Module\n\nThis module provides observability and tracing capabilities for PocketFlow workflows\nusing Langfuse as the backend. It includes decorators and utilities to automatically\ntrace node execution, inputs, and outputs.\n\"\"\"\n\nfrom .config import TracingConfig\nfrom .core import LangfuseTracer\nfrom .decorator import trace_flow\n\n__all__ = [\"trace_flow\", \"TracingConfig\", \"LangfuseTracer\"]\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tracing/tracing/__init__.py`:**\n\n1. **Primary purpose:**  \n   This code unit serves as the package initializer for the PocketFlow tracing module. Its main role is to expose key tracing components\u2014such as configuration, tracer classes, and a tracing decorator\u2014for use in PocketFlow workflows. This module enables observability and tracing using Langfuse as the backend.\n\n2. **Parameters:**  \n   This unit does not define any parameters; it only imports and exposes objects.\n\n3. **Return value:**  \n   No functions or return values are defined in this code unit. It simply specifies the module's public API by setting `__all__`.\n\n4. **Other functions or methods it calls internally:**  \n   Internally, it does not call any functions or methods. It only imports the following symbols from its submodules:\n   - `TracingConfig` (from `.config`)\n   - `LangfuseTracer` (from `.core`)\n   - `trace_flow` (from `.decorator`)\n\nThese imports and the `__all__` variable define what is accessible when someone imports from this tracing module."
    },
    "164": {
      "unit_name": "decorator.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/tracing/decorator.py",
      "code": "\"\"\"\nDecorator for tracing PocketFlow workflows with Langfuse.\n\"\"\"\n\nimport functools\nimport inspect\nimport uuid\nfrom typing import Any, Callable, Dict, Optional, Union\n\nfrom .config import TracingConfig\nfrom .core import LangfuseTracer\n\n\ndef trace_flow(\n    config: Optional[TracingConfig] = None,\n    flow_name: Optional[str] = None,\n    session_id: Optional[str] = None,\n    user_id: Optional[str] = None\n):\n    \"\"\"\n    Decorator to add Langfuse tracing to PocketFlow flows.\n    \n    This decorator automatically traces:\n    - Flow execution start/end\n    - Each node's prep, exec, and post phases\n    - Input and output data for each phase\n    - Errors and exceptions\n    \n    Args:\n        config: TracingConfig instance. If None, loads from environment.\n        flow_name: Custom name for the flow. If None, uses the flow class name.\n        session_id: Session ID for grouping related traces.\n        user_id: User ID for the trace.\n        \n    Returns:\n        Decorated flow class or function.\n        \n    Example:\n        ```python\n        from tracing import trace_flow\n        \n        @trace_flow()\n        class MyFlow(Flow):\n            def __init__(self):\n                super().__init__(start=MyNode())\n        \n        # Or with custom configuration\n        config = TracingConfig.from_env()\n        \n        @trace_flow(config=config, flow_name=\"CustomFlow\")\n        class MyFlow(Flow):\n            pass\n        ```\n    \"\"\"\n    def decorator(flow_class_or_func):\n        # Handle both class and function decoration\n        if inspect.isclass(flow_class_or_func):\n            return _trace_flow_class(flow_class_or_func, config, flow_name, session_id, user_id)\n        else:\n            return _trace_flow_function(flow_class_or_func, config, flow_name, session_id, user_id)\n    \n    return decorator\n\n\ndef _trace_flow_class(flow_class, config, flow_name, session_id, user_id):\n    \"\"\"Trace a Flow class by wrapping its methods.\"\"\"\n    \n    # Get or create config\n    if config is None:\n        config = TracingConfig.from_env()\n    \n    # Override session/user if provided\n    if session_id:\n        config.session_id = session_id\n    if user_id:\n        config.user_id = user_id\n    \n    # Get flow name\n    if flow_name is None:\n        flow_name = flow_class.__name__\n    \n    # Store original methods\n    original_init = flow_class.__init__\n    original_run = getattr(flow_class, 'run', None)\n    original_run_async = getattr(flow_class, 'run_async', None)\n    \n    def traced_init(self, *args, **kwargs):\n        \"\"\"Initialize the flow with tracing capabilities.\"\"\"\n        # Call original init\n        original_init(self, *args, **kwargs)\n        \n        # Add tracing attributes\n        self._tracer = LangfuseTracer(config)\n        self._flow_name = flow_name\n        self._trace_id = None\n        \n        # Patch all nodes in the flow\n        self._patch_nodes()\n    \n    def traced_run(self, shared):\n        \"\"\"Traced version of the run method.\"\"\"\n        if not hasattr(self, '_tracer'):\n            # Fallback if not properly initialized\n            return original_run(self, shared) if original_run else None\n            \n        # Start trace\n        self._trace_id = self._tracer.start_trace(self._flow_name, shared)\n        \n        try:\n            # Run the original flow\n            result = original_run(self, shared) if original_run else None\n            \n            # End trace successfully\n            self._tracer.end_trace(shared, \"success\")\n            \n            return result\n            \n        except Exception as e:\n            # End trace with error\n            self._tracer.end_trace(shared, \"error\")\n            raise\n        finally:\n            # Ensure cleanup\n            self._tracer.flush()\n    \n    async def traced_run_async(self, shared):\n        \"\"\"Traced version of the async run method.\"\"\"\n        if not hasattr(self, '_tracer'):\n            # Fallback if not properly initialized\n            return await original_run_async(self, shared) if original_run_async else None\n            \n        # Start trace\n        self._trace_id = self._tracer.start_trace(self._flow_name, shared)\n        \n        try:\n            # Run the original flow\n            result = await original_run_async(self, shared) if original_run_async else None\n            \n            # End trace successfully\n            self._tracer.end_trace(shared, \"success\")\n            \n            return result\n            \n        except Exception as e:\n            # End trace with error\n            self._tracer.end_trace(shared, \"error\")\n            raise\n        finally:\n            # Ensure cleanup\n            self._tracer.flush()\n    \n    def patch_nodes(self):\n        \"\"\"Patch all nodes in the flow to add tracing.\"\"\"\n        if not hasattr(self, 'start_node') or not self.start_node:\n            return\n            \n        visited = set()\n        nodes_to_patch = [self.start_node]\n        \n        while nodes_to_patch:\n            node = nodes_to_patch.pop(0)\n            if id(node) in visited:\n                continue\n                \n            visited.add(id(node))\n            \n            # Patch this node\n            self._patch_node(node)\n            \n            # Add successors to patch list\n            if hasattr(node, 'successors'):\n                for successor in node.successors.values():\n                    if successor and id(successor) not in visited:\n                        nodes_to_patch.append(successor)\n    \n    def patch_node(self, node):\n        \"\"\"Patch a single node to add tracing.\"\"\"\n        if hasattr(node, '_pocketflow_traced'):\n            return  # Already patched\n            \n        node_id = str(uuid.uuid4())\n        node_name = type(node).__name__\n        \n        # Store original methods\n        original_prep = getattr(node, 'prep', None)\n        original_exec = getattr(node, 'exec', None)\n        original_post = getattr(node, 'post', None)\n        original_prep_async = getattr(node, 'prep_async', None)\n        original_exec_async = getattr(node, 'exec_async', None)\n        original_post_async = getattr(node, 'post_async', None)\n        \n        # Create traced versions\n        if original_prep:\n            node.prep = self._create_traced_method(original_prep, node_id, node_name, 'prep')\n        if original_exec:\n            node.exec = self._create_traced_method(original_exec, node_id, node_name, 'exec')\n        if original_post:\n            node.post = self._create_traced_method(original_post, node_id, node_name, 'post')\n        if original_prep_async:\n            node.prep_async = self._create_traced_async_method(original_prep_async, node_id, node_name, 'prep')\n        if original_exec_async:\n            node.exec_async = self._create_traced_async_method(original_exec_async, node_id, node_name, 'exec')\n        if original_post_async:\n            node.post_async = self._create_traced_async_method(original_post_async, node_id, node_name, 'post')\n        \n        # Mark as traced\n        node._pocketflow_traced = True\n    \n    def create_traced_method(self, original_method, node_id, node_name, phase):\n        \"\"\"Create a traced version of a synchronous method.\"\"\"\n        @functools.wraps(original_method)\n        def traced_method(*args, **kwargs):\n            span_id = self._tracer.start_node_span(node_name, node_id, phase)\n            \n            try:\n                result = original_method(*args, **kwargs)\n                self._tracer.end_node_span(span_id, input_data=args, output_data=result)\n                return result\n            except Exception as e:\n                self._tracer.end_node_span(span_id, input_data=args, error=e)\n                raise\n                \n        return traced_method\n    \n    def create_traced_async_method(self, original_method, node_id, node_name, phase):\n        \"\"\"Create a traced version of an asynchronous method.\"\"\"\n        @functools.wraps(original_method)\n        async def traced_async_method(*args, **kwargs):\n            span_id = self._tracer.start_node_span(node_name, node_id, phase)\n            \n            try:\n                result = await original_method(*args, **kwargs)\n                self._tracer.end_node_span(span_id, input_data=args, output_data=result)\n                return result\n            except Exception as e:\n                self._tracer.end_node_span(span_id, input_data=args, error=e)\n                raise\n                \n        return traced_async_method\n    \n    # Replace methods on the class\n    flow_class.__init__ = traced_init\n    flow_class._patch_nodes = patch_nodes\n    flow_class._patch_node = patch_node\n    flow_class._create_traced_method = create_traced_method\n    flow_class._create_traced_async_method = create_traced_async_method\n    \n    if original_run:\n        flow_class.run = traced_run\n    if original_run_async:\n        flow_class.run_async = traced_run_async\n    \n    return flow_class\n\n\ndef _trace_flow_function(flow_func, config, flow_name, session_id, user_id):\n    \"\"\"Trace a flow function (for functional-style flows).\"\"\"\n    \n    # Get or create config\n    if config is None:\n        config = TracingConfig.from_env()\n    \n    # Override session/user if provided\n    if session_id:\n        config.session_id = session_id\n    if user_id:\n        config.user_id = user_id\n    \n    # Get flow name\n    if flow_name is None:\n        flow_name = flow_func.__name__\n    \n    tracer = LangfuseTracer(config)\n    \n    @functools.wraps(flow_func)\n    def traced_flow_func(*args, **kwargs):\n        # Assume first argument is shared data\n        shared = args[0] if args else {}\n        \n        # Start trace\n        trace_id = tracer.start_trace(flow_name, shared)\n        \n        try:\n            result = flow_func(*args, **kwargs)\n            tracer.end_trace(shared, \"success\")\n            return result\n        except Exception as e:\n            tracer.end_trace(shared, \"error\")\n            raise\n        finally:\n            tracer.flush()\n    \n    return traced_flow_func\n",
      "summary": "**Summary of `decorator.py`**\n\n1. **Primary Purpose**  \nThis code defines a decorator (`trace_flow`) for automatically tracing the execution of PocketFlow workflows (either as classes or functions) using the Langfuse tracing system. The decorator instruments flows and their nodes to provide detailed execution traces, including phase-level information and error reporting.\n\n2. **Parameters**\n   - `config` (Optional[TracingConfig]): Custom tracing configuration. If not provided, loads from the environment.\n   - `flow_name` (Optional[str]): Optional custom name for the flow; defaults to the class or function name if not specified.\n   - `session_id` (Optional[str]): Optional session identifier for grouping traces.\n   - `user_id` (Optional[str]): Optional user identifier for the trace.\n\n3. **Return Value**\n   - Returns a decorated version of the input flow class or function. For classes, it patches methods to insert tracing; for functions, it wraps them with tracing logic.\n\n4. **Internal Function and Method Calls**\n   - **Classes/Modules from PocketFlow Tracing:**\n     - `TracingConfig.from_env()`\n     - `LangfuseTracer (... methods below)`\n         - `start_trace`, `end_trace`, `flush`\n         - `start_node_span`, `end_node_span`\n   - **Python Standard Library:**\n     - `functools.wraps`\n     - `inspect.isclass`\n     - `uuid.uuid4`\n   - **Internal Helper Functions:**\n     - `_trace_flow_class` (wraps and patches flow classes)\n     - `_trace_flow_function` (wraps flow functions)\n     - Several class-patching helpers (e.g., `patch_nodes`, `patch_node`, `_create_traced_method`, `_create_traced_async_method`)\n\n**In summary:**  \nThis code enables automatic, configurable tracing of PocketFlow workflow executions with Langfuse, at both the flow and node levels, by providing a fully-featured decorator for both class-based and function-based flows."
    },
    "165": {
      "unit_name": "core.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/tracing/core.py",
      "code": "\"\"\"\nCore tracing functionality for PocketFlow with Langfuse integration.\n\"\"\"\n\nimport json\nimport time\nimport uuid\nfrom typing import Any, Dict, Optional, Union\nfrom datetime import datetime\n\ntry:\n    from langfuse import Langfuse\n\n    LANGFUSE_AVAILABLE = True\nexcept ImportError:\n    LANGFUSE_AVAILABLE = False\n    print(\"Warning: langfuse package not installed. Install with: pip install langfuse\")\n\nfrom .config import TracingConfig\n\n\nclass LangfuseTracer:\n    \"\"\"\n    Core tracer class that handles Langfuse integration for PocketFlow.\n    \"\"\"\n\n    def __init__(self, config: TracingConfig):\n        \"\"\"\n        Initialize the LangfuseTracer.\n\n        Args:\n            config: TracingConfig instance with Langfuse settings.\n        \"\"\"\n        self.config = config\n        self.client = None\n        self.current_trace = None\n        self.spans = {}  # Store spans by node ID\n\n        if LANGFUSE_AVAILABLE and config.validate():\n            try:\n                # Initialize Langfuse client with proper parameters\n                kwargs = {}\n                if config.langfuse_secret_key:\n                    kwargs[\"secret_key\"] = config.langfuse_secret_key\n                if config.langfuse_public_key:\n                    kwargs[\"public_key\"] = config.langfuse_public_key\n                if config.langfuse_host:\n                    kwargs[\"host\"] = config.langfuse_host\n                if config.debug:\n                    kwargs[\"debug\"] = True\n\n                self.client = Langfuse(**kwargs)\n                if config.debug:\n                    print(\n                        f\"\u2713 Langfuse client initialized with host: {config.langfuse_host}\"\n                    )\n            except Exception as e:\n                if config.debug:\n                    print(f\"\u2717 Failed to initialize Langfuse client: {e}\")\n                self.client = None\n        else:\n            if config.debug:\n                print(\"\u2717 Langfuse not available or configuration invalid\")\n\n    def start_trace(self, flow_name: str, input_data: Dict[str, Any]) -> Optional[str]:\n        \"\"\"\n        Start a new trace for a flow execution.\n\n        Args:\n            flow_name: Name of the flow being traced.\n            input_data: Input data for the flow.\n\n        Returns:\n            Trace ID if successful, None otherwise.\n        \"\"\"\n        if not self.client:\n            return None\n\n        try:\n            # Serialize input data safely\n            serialized_input = self._serialize_data(input_data)\n\n            # Use Langfuse v2 API to create a trace\n            self.current_trace = self.client.trace(\n                name=flow_name,\n                input=serialized_input,\n                metadata={\n                    \"framework\": \"PocketFlow\",\n                    \"trace_type\": \"flow_execution\",\n                    \"timestamp\": datetime.now().isoformat(),\n                },\n                session_id=self.config.session_id,\n                user_id=self.config.user_id,\n            )\n\n            # Get the trace ID\n            trace_id = self.current_trace.id\n\n            if self.config.debug:\n                print(f\"\u2713 Started trace: {trace_id} for flow: {flow_name}\")\n\n            return trace_id\n\n        except Exception as e:\n            if self.config.debug:\n                print(f\"\u2717 Failed to start trace: {e}\")\n            return None\n\n    def end_trace(self, output_data: Dict[str, Any], status: str = \"success\") -> None:\n        \"\"\"\n        End the current trace.\n\n        Args:\n            output_data: Output data from the flow.\n            status: Status of the trace execution.\n        \"\"\"\n        if not self.current_trace:\n            return\n\n        try:\n            # Serialize output data safely\n            serialized_output = self._serialize_data(output_data)\n\n            # Update the trace with output data using v2 API\n            self.current_trace.update(\n                output=serialized_output,\n                metadata={\n                    \"status\": status,\n                    \"end_timestamp\": datetime.now().isoformat(),\n                },\n            )\n\n            if self.config.debug:\n                print(f\"\u2713 Ended trace with status: {status}\")\n\n        except Exception as e:\n            if self.config.debug:\n                print(f\"\u2717 Failed to end trace: {e}\")\n        finally:\n            self.current_trace = None\n            self.spans.clear()\n\n    def start_node_span(\n        self, node_name: str, node_id: str, phase: str\n    ) -> Optional[str]:\n        \"\"\"\n        Start a span for a node execution phase.\n\n        Args:\n            node_name: Name/type of the node.\n            node_id: Unique identifier for the node instance.\n            phase: Execution phase (prep, exec, post).\n\n        Returns:\n            Span ID if successful, None otherwise.\n        \"\"\"\n        if not self.current_trace:\n            return None\n\n        try:\n            span_id = f\"{node_id}_{phase}\"\n\n            # Create a child span using v2 API\n            span = self.current_trace.span(\n                name=f\"{node_name}.{phase}\",\n                metadata={\n                    \"node_type\": node_name,\n                    \"node_id\": node_id,\n                    \"phase\": phase,\n                    \"start_timestamp\": datetime.now().isoformat(),\n                },\n            )\n\n            self.spans[span_id] = span\n\n            if self.config.debug:\n                print(f\"\u2713 Started span: {span_id}\")\n\n            return span_id\n\n        except Exception as e:\n            if self.config.debug:\n                print(f\"\u2717 Failed to start span: {e}\")\n            return None\n\n    def end_node_span(\n        self,\n        span_id: str,\n        input_data: Any = None,\n        output_data: Any = None,\n        error: Exception = None,\n    ) -> None:\n        \"\"\"\n        End a node execution span.\n\n        Args:\n            span_id: ID of the span to end.\n            input_data: Input data for the phase.\n            output_data: Output data from the phase.\n            error: Exception if the phase failed.\n        \"\"\"\n        if span_id not in self.spans:\n            return\n\n        try:\n            span = self.spans[span_id]\n\n            # Prepare update data\n            update_data = {}\n\n            if input_data is not None and self.config.trace_inputs:\n                update_data[\"input\"] = self._serialize_data(input_data)\n            if output_data is not None and self.config.trace_outputs:\n                update_data[\"output\"] = self._serialize_data(output_data)\n\n            if error and self.config.trace_errors:\n                update_data.update(\n                    {\n                        \"level\": \"ERROR\",\n                        \"status_message\": str(error),\n                        \"metadata\": {\n                            \"error_type\": type(error).__name__,\n                            \"error_message\": str(error),\n                            \"end_timestamp\": datetime.now().isoformat(),\n                        },\n                    }\n                )\n            else:\n                update_data.update(\n                    {\n                        \"level\": \"DEFAULT\",\n                        \"metadata\": {\"end_timestamp\": datetime.now().isoformat()},\n                    }\n                )\n\n            # Update the span with all data at once\n            span.update(**update_data)\n\n            # End the span\n            span.end()\n\n            if self.config.debug:\n                status = \"ERROR\" if error else \"SUCCESS\"\n                print(f\"\u2713 Ended span: {span_id} with status: {status}\")\n\n        except Exception as e:\n            if self.config.debug:\n                print(f\"\u2717 Failed to end span: {e}\")\n        finally:\n            if span_id in self.spans:\n                del self.spans[span_id]\n\n    def _serialize_data(self, data: Any) -> Any:\n        \"\"\"\n        Safely serialize data for Langfuse.\n\n        Args:\n            data: Data to serialize.\n\n        Returns:\n            Serialized data that can be sent to Langfuse.\n        \"\"\"\n        try:\n            # Handle common PocketFlow data types\n            if hasattr(data, \"__dict__\"):\n                # Convert objects to dict representation\n                return {\"_type\": type(data).__name__, \"_data\": str(data)}\n            elif isinstance(data, (dict, list, str, int, float, bool, type(None))):\n                # JSON-serializable types\n                return data\n            else:\n                # Fallback to string representation\n                return {\"_type\": type(data).__name__, \"_data\": str(data)}\n        except Exception:\n            # Ultimate fallback\n            return {\"_type\": \"unknown\", \"_data\": \"<serialization_failed>\"}\n\n    def flush(self) -> None:\n        \"\"\"Flush any pending traces to Langfuse.\"\"\"\n        if self.client:\n            try:\n                self.client.flush()\n                if self.config.debug:\n                    print(\"\u2713 Flushed traces to Langfuse\")\n            except Exception as e:\n                if self.config.debug:\n                    print(f\"\u2717 Failed to flush traces: {e}\")\n",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-tracing/tracing/core.py` (unit: core.py):**\n\n1. **Primary Purpose:**\n   - This code provides core tracing functionality for PocketFlow and integrates with the Langfuse tracing platform. It enables tracing of flows and node executio ns, including capturing inputs, outputs, errors, and timestamps in a structured way for observability, debugging, and analytics.\n\n2. **Brief Description of Parameters:**\n   - The main class is `LangfuseTracer`, which is initialized with a `TracingConfig` object (contains Langfuse keys, host, user/session IDs, and debugging options).\n   - Most methods take as parameters flow names, node names/IDs, input/output data, error information, and execution phase names.\n\n3. **Brief Description of Return Values:**\n   - `start_trace`: Returns the trace ID string if the trace is started successfully, else `None`.\n   - `start_node_span`: Returns the span ID string if the span is started successfully, else `None`.\n   - Most other methods return `None`.\n\n4. **List of Other Functions or Methods Called Internally:**\n   - `config.validate()` \u2013 checks tracing configuration validity.\n   - `Langfuse(...)` \u2013 initializes the Langfuse client.\n   - `self.client.trace(...)` \u2013 starts a trace in Langfuse.\n   - `self.current_trace.update(...)` \u2013 updates trace metadata/output.\n   - `self.current_trace.span(...)` \u2013 creates a child span in a trace.\n   - `span.update(...)` \u2013 updates span data.\n   - `span.end()` \u2013 marks a span as ended.\n   - `self.client.flush()` \u2013 flushes pending traces to Langfuse.\n   - `self._serialize_data(...)` \u2013 safely serializes inputs/outputs/errors for logging.\n   - Python built-ins: `print`, `str`, and `hasattr`.\n   - Standard library: `datetime.now().isoformat()` and type-checking functions (`isinstance`/`type`)."
    },
    "166": {
      "unit_name": "config.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/tracing/config.py",
      "code": "\"\"\"\nConfiguration module for PocketFlow tracing with Langfuse.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom dotenv import load_dotenv\n\n\n@dataclass\nclass TracingConfig:\n    \"\"\"Configuration class for PocketFlow tracing with Langfuse.\"\"\"\n    \n    # Langfuse configuration\n    langfuse_secret_key: Optional[str] = None\n    langfuse_public_key: Optional[str] = None\n    langfuse_host: Optional[str] = None\n    \n    # PocketFlow tracing configuration\n    debug: bool = False\n    trace_inputs: bool = True\n    trace_outputs: bool = True\n    trace_prep: bool = True\n    trace_exec: bool = True\n    trace_post: bool = True\n    trace_errors: bool = True\n    \n    # Session configuration\n    session_id: Optional[str] = None\n    user_id: Optional[str] = None\n    \n    @classmethod\n    def from_env(cls, env_file: Optional[str] = None) -> \"TracingConfig\":\n        \"\"\"\n        Create TracingConfig from environment variables.\n        \n        Args:\n            env_file: Optional path to .env file. If None, looks for .env in current directory.\n            \n        Returns:\n            TracingConfig instance with values from environment variables.\n        \"\"\"\n        # Load environment variables from .env file if it exists\n        if env_file:\n            load_dotenv(env_file)\n        else:\n            # Try to find .env file in current directory or parent directories\n            load_dotenv()\n        \n        return cls(\n            langfuse_secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n            langfuse_public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n            langfuse_host=os.getenv(\"LANGFUSE_HOST\"),\n            debug=os.getenv(\"POCKETFLOW_TRACING_DEBUG\", \"false\").lower() == \"true\",\n            trace_inputs=os.getenv(\"POCKETFLOW_TRACE_INPUTS\", \"true\").lower() == \"true\",\n            trace_outputs=os.getenv(\"POCKETFLOW_TRACE_OUTPUTS\", \"true\").lower() == \"true\",\n            trace_prep=os.getenv(\"POCKETFLOW_TRACE_PREP\", \"true\").lower() == \"true\",\n            trace_exec=os.getenv(\"POCKETFLOW_TRACE_EXEC\", \"true\").lower() == \"true\",\n            trace_post=os.getenv(\"POCKETFLOW_TRACE_POST\", \"true\").lower() == \"true\",\n            trace_errors=os.getenv(\"POCKETFLOW_TRACE_ERRORS\", \"true\").lower() == \"true\",\n            session_id=os.getenv(\"POCKETFLOW_SESSION_ID\"),\n            user_id=os.getenv(\"POCKETFLOW_USER_ID\"),\n        )\n    \n    def validate(self) -> bool:\n        \"\"\"\n        Validate that required configuration is present.\n        \n        Returns:\n            True if configuration is valid, False otherwise.\n        \"\"\"\n        if not self.langfuse_secret_key:\n            if self.debug:\n                print(\"Warning: LANGFUSE_SECRET_KEY not set\")\n            return False\n            \n        if not self.langfuse_public_key:\n            if self.debug:\n                print(\"Warning: LANGFUSE_PUBLIC_KEY not set\")\n            return False\n            \n        if not self.langfuse_host:\n            if self.debug:\n                print(\"Warning: LANGFUSE_HOST not set\")\n            return False\n            \n        return True\n    \n    def to_langfuse_kwargs(self) -> dict:\n        \"\"\"\n        Convert configuration to kwargs for Langfuse client initialization.\n        \n        Returns:\n            Dictionary of kwargs for Langfuse client.\n        \"\"\"\n        kwargs = {}\n        \n        if self.langfuse_secret_key:\n            kwargs[\"secret_key\"] = self.langfuse_secret_key\n            \n        if self.langfuse_public_key:\n            kwargs[\"public_key\"] = self.langfuse_public_key\n            \n        if self.langfuse_host:\n            kwargs[\"host\"] = self.langfuse_host\n            \n        if self.debug:\n            kwargs[\"debug\"] = True\n            \n        return kwargs\n",
      "summary": "**Summary for `config.py`**\n\n1. **Primary Purpose:**  \n   This code unit defines the configuration management for PocketFlow's tracing integration with Langfuse. It provides a dataclass to encapsulate tracing and Langfuse setup, supports loading configuration from environment variables (including `.env` files), validates required fields, and prepares configuration for Langfuse client initialization.\n\n2. **Description of Parameters:**\n   - **TracingConfig fields:**  \n     - `langfuse_secret_key`, `langfuse_public_key`, `langfuse_host`: Credentials and endpoint for Langfuse.\n     - `debug`: Toggles debug mode (default: False).\n     - Tracing toggles: `trace_inputs`, `trace_outputs`, `trace_prep`, `trace_exec`, `trace_post`, `trace_errors` (all default to True).\n     - `session_id`, `user_id`: Optional session and user identifiers.\n   - **from_env(env_file=None):**  \n     - `env_file`: Optional path to a .env file to load environment variables from; if omitted, it tries the default .env in the current (or parent) directory.\n\n3. **Description of Return Values:**\n   - **from_env:** Returns a new `TracingConfig` instance populated from environment variables.\n   - **validate:** Returns a `True` or `False` indicating if the required Langfuse config values are present.\n   - **to_langfuse_kwargs:** Returns a dictionary suitable for initializing the Langfuse client.\n\n4. **Other Functions/Methods Called:**\n   - `os.getenv`\n   - `dotenv.load_dotenv`\n   - Standard Python print function (for debug warnings)\n\nNo external API calls or complex dependencies are present beyond the Python standard library and `dotenv` for environment management."
    },
    "167": {
      "unit_name": "basic_example.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/examples/basic_example.py",
      "code": "#!/usr/bin/env python3\n\"\"\"\nBasic example demonstrating PocketFlow tracing with Langfuse.\n\nThis example shows how to use the @trace_flow decorator to automatically\ntrace a simple PocketFlow workflow.\n\"\"\"\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Add parent directory to path to import pocketflow and tracing\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\"))\nsys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))\n\nfrom pocketflow import Node, Flow\nfrom tracing import trace_flow, TracingConfig\n\n\nclass GreetingNode(Node):\n    \"\"\"A simple node that creates a greeting message.\"\"\"\n\n    def prep(self, shared):\n        \"\"\"Extract the name from shared data.\"\"\"\n        name = shared.get(\"name\", \"World\")\n        return name\n\n    def exec(self, name):\n        \"\"\"Create a greeting message.\"\"\"\n        greeting = f\"Hello, {name}!\"\n        return greeting\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the greeting in shared data.\"\"\"\n        shared[\"greeting\"] = exec_res\n        return \"default\"\n\n\nclass UppercaseNode(Node):\n    \"\"\"A node that converts the greeting to uppercase.\"\"\"\n\n    def prep(self, shared):\n        \"\"\"Get the greeting from shared data.\"\"\"\n        return shared.get(\"greeting\", \"\")\n\n    def exec(self, greeting):\n        \"\"\"Convert to uppercase.\"\"\"\n        return greeting.upper()\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the uppercase greeting.\"\"\"\n        shared[\"uppercase_greeting\"] = exec_res\n        return \"default\"\n\n\n@trace_flow(flow_name=\"BasicGreetingFlow\")\nclass BasicGreetingFlow(Flow):\n    \"\"\"A simple flow that creates and processes a greeting.\"\"\"\n\n    def __init__(self):\n        # Create nodes\n        greeting_node = GreetingNode()\n        uppercase_node = UppercaseNode()\n\n        # Connect nodes\n        greeting_node >> uppercase_node\n\n        # Initialize flow\n        super().__init__(start=greeting_node)\n\n\ndef main():\n    \"\"\"Run the basic tracing example.\"\"\"\n    print(\"\ud83d\ude80 Starting PocketFlow Tracing Basic Example\")\n    print(\"=\" * 50)\n\n    # Create the flow\n    flow = BasicGreetingFlow()\n\n    # Prepare shared data\n    shared = {\"name\": \"PocketFlow User\"}\n\n    print(f\"\ud83d\udce5 Input: {shared}\")\n\n    # Run the flow (this will be automatically traced)\n    try:\n        result = flow.run(shared)\n        print(f\"\ud83d\udce4 Output: {shared}\")\n        print(f\"\ud83c\udfaf Result: {result}\")\n        print(\"\u2705 Flow completed successfully!\")\n\n        # Print the final greeting\n        if \"uppercase_greeting\" in shared:\n            print(f\"\ud83c\udf89 Final greeting: {shared['uppercase_greeting']}\")\n\n    except Exception as e:\n        print(f\"\u274c Flow failed with error: {e}\")\n        raise\n\n    print(\"\\n\ud83d\udcca Check your Langfuse dashboard to see the trace!\")\n    langfuse_host = os.getenv(\"LANGFUSE_HOST\", \"your-langfuse-host\")\n    print(f\"   Dashboard URL: {langfuse_host}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "summary": "**Summary of `basic_example.py`:**\n\n1. **Primary purpose of the code unit:**\n   \n   The file demonstrates how to use PocketFlow's tracing capabilities (via the Langfuse integration) by implementing and tracing a simple greeting workflow. It provides a minimal example of defining custom nodes, composing them into a flow, and automatically tracing the flow execution with the `@trace_flow` decorator.\n\n2. **Parameters:**\n   \n   - The main function (`main`) accepts no parameters.\n   - The flow itself operates on a `shared` dictionary, which initially contains the key `\"name\"` and is used to pass data between nodes.\n\n3. **Return value:**\n   \n   - `main` does not return a value; it prints output to the console.\n   - When the flow is run (`flow.run(shared)`), it returns a result value (the output from the flow, typically the last node\u2019s post-execution result). This is displayed by the script but not returned from `main`.\n\n4. **Internal function/method calls:**\n\n   - External API:\n     - `load_dotenv()` (from `dotenv`)\n     - `os.getenv()`\n     - `print()`\n   \n   - Class methods and flow control:\n     - `GreetingNode.prep(shared)`\n     - `GreetingNode.exec(name)`\n     - `GreetingNode.post(shared, prep_res, exec_res)`\n     - `UppercaseNode.prep(shared)`\n     - `UppercaseNode.exec(greeting)`\n     - `UppercaseNode.post(shared, prep_res, exec_res)`\n     - `BasicGreetingFlow.__init__()`\n     - `Flow.__init__(start=...)`\n     - `greeting_node >> uppercase_node` (probably operator overloading for flow connection)\n     - `flow.run(shared)`\n   \n   - Tracing and decorators:\n     - `trace_flow(flow_name=\"BasicGreetingFlow\")` (decorator wrapping `BasicGreetingFlow`)\n     - Langfuse tracing occurs implicitly via the decorator.\n\n   - Error handling:\n     - `try` / `except Exception as e` / `raise`\n\n   - Environment setup:\n     - `sys.path.insert()`\n     - `os.path.dirname()`\n     - `os.path.join()`\n\n**In essence:**  \nThe script defines a traceable workflow that creates and transforms a greeting, while exhibiting how to trace flows using PocketFlow and Langfuse, operating on a mutable `shared` dictionary and primarily utilizing custom node logic and flow infrastructure provided by the PocketFlow library."
    },
    "168": {
      "unit_name": "async_example.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-tracing/examples/async_example.py",
      "code": "#!/usr/bin/env python3\n\"\"\"\nAsync example demonstrating PocketFlow tracing with Langfuse.\n\nThis example shows how to use the @trace_flow decorator with AsyncFlow\nand AsyncNode to trace asynchronous workflows.\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Add parent directory to path to import pocketflow and tracing\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\"))\nsys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))\n\nfrom pocketflow import AsyncNode, AsyncFlow\nfrom tracing import trace_flow, TracingConfig\n\n\nclass AsyncDataFetchNode(AsyncNode):\n    \"\"\"An async node that simulates fetching data.\"\"\"\n\n    async def prep_async(self, shared):\n        \"\"\"Extract the query from shared data.\"\"\"\n        query = shared.get(\"query\", \"default\")\n        return query\n\n    async def exec_async(self, query):\n        \"\"\"Simulate async data fetching.\"\"\"\n        print(f\"\ud83d\udd0d Fetching data for query: {query}\")\n\n        # Simulate async operation\n        await asyncio.sleep(1)\n\n        # Return mock data\n        data = {\n            \"query\": query,\n            \"results\": [f\"Result {i} for {query}\" for i in range(3)],\n            \"timestamp\": \"2024-01-01T00:00:00Z\",\n        }\n        return data\n\n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Store the fetched data.\"\"\"\n        shared[\"fetched_data\"] = exec_res\n        return \"process\"\n\n\nclass AsyncDataProcessNode(AsyncNode):\n    \"\"\"An async node that processes the fetched data.\"\"\"\n\n    async def prep_async(self, shared):\n        \"\"\"Get the fetched data.\"\"\"\n        return shared.get(\"fetched_data\", {})\n\n    async def exec_async(self, data):\n        \"\"\"Process the data asynchronously.\"\"\"\n        print(\"\u2699\ufe0f Processing fetched data...\")\n\n        # Simulate async processing\n        await asyncio.sleep(0.5)\n\n        # Process the results\n        processed_results = []\n        for result in data.get(\"results\", []):\n            processed_results.append(f\"PROCESSED: {result}\")\n\n        return {\n            \"original_query\": data.get(\"query\"),\n            \"processed_results\": processed_results,\n            \"result_count\": len(processed_results),\n        }\n\n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Store the processed data.\"\"\"\n        shared[\"processed_data\"] = exec_res\n        return \"default\"\n\n\n@trace_flow(flow_name=\"AsyncDataProcessingFlow\")\nclass AsyncDataProcessingFlow(AsyncFlow):\n    \"\"\"An async flow that fetches and processes data.\"\"\"\n\n    def __init__(self):\n        # Create async nodes\n        fetch_node = AsyncDataFetchNode()\n        process_node = AsyncDataProcessNode()\n\n        # Connect nodes\n        fetch_node - \"process\" >> process_node\n\n        # Initialize async flow\n        super().__init__(start=fetch_node)\n\n\nasync def main():\n    \"\"\"Run the async tracing example.\"\"\"\n    print(\"\ud83d\ude80 Starting PocketFlow Async Tracing Example\")\n    print(\"=\" * 50)\n\n    # Create the async flow\n    flow = AsyncDataProcessingFlow()\n\n    # Prepare shared data\n    shared = {\"query\": \"machine learning tutorials\"}\n\n    print(f\"\ud83d\udce5 Input: {shared}\")\n\n    # Run the async flow (this will be automatically traced)\n    try:\n        result = await flow.run_async(shared)\n        print(f\"\ud83d\udce4 Output: {shared}\")\n        print(f\"\ud83c\udfaf Result: {result}\")\n        print(\"\u2705 Async flow completed successfully!\")\n\n        # Print the processed data\n        if \"processed_data\" in shared:\n            processed = shared[\"processed_data\"]\n            print(\n                f\"\ud83c\udf89 Processed {processed['result_count']} results for query: {processed['original_query']}\"\n            )\n            for result in processed[\"processed_results\"]:\n                print(f\"   - {result}\")\n\n    except Exception as e:\n        print(f\"\u274c Async flow failed with error: {e}\")\n        raise\n\n    print(\"\\n\ud83d\udcca Check your Langfuse dashboard to see the async trace!\")\n    langfuse_host = os.getenv(\"LANGFUSE_HOST\", \"your-langfuse-host\")\n    print(f\"   Dashboard URL: {langfuse_host}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "summary": "**Summary of async_example.py**\n\n1. **Primary Purpose:**\n   - The code unit demonstrates how to trace asynchronous workflows using the PocketFlow library integrated with Langfuse. It showcases tracing of an async data fetching and processing pipeline with the `@trace_flow` decorator, suitable for monitoring and debugging async operations.\n\n2. **Parameters:**\n   - The main execution (`main()` and the async flow) uses a `shared` dictionary as input, containing a `\"query\"` key (e.g., `\"machine learning tutorials\"`).\n   - Nodes (`AsyncDataFetchNode`, `AsyncDataProcessNode`) and methods take parameters relevant to their stage, such as `shared`, `query`, or `data`.\n\n3. **Return Value:**\n   - The top-level `main()` function does not explicitly return a value; instead, it prints outputs to the console and modifies the shared dictionary in-place.\n   - Individual node methods may return result data, and the async flow\u2019s `run_async()` method produces a result stored in the shared object.\n\n4. **Functions or Methods Called Internally:**\n   - `load_dotenv()` (loads environment variables)\n   - `sys.path.insert()` (modifies import path)\n   - Classes imported: `AsyncNode`, `AsyncFlow` (from pocketflow)\n   - Decorator: `trace_flow` (from tracing)\n   - `os.path` and `os.getenv()`\n   - Various `asyncio` functions:\n     - `asyncio.sleep()`\n     - `asyncio.run()`\n   - Methods implemented and called on the async flow/nodes:\n     - `prep_async()`, `exec_async()`, `post_async()` (for each node)\n     - `run_async()` (on the async flow)\n   - Python print statements for illustration and result reporting.\n\n**In summary:**  \nThis script sets up an asynchronous, traceable pipeline for fetching and processing data, making use of PocketFlow and Langfuse tracing. It exemplifies the construction and execution of such a pipeline, includes sample input, and provides detailed output and error handling for demonstration purposes."
    },
    "169": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-async-basic/nodes.py",
      "code": "from pocketflow import AsyncNode\nfrom utils import fetch_recipes, call_llm_async, get_user_input\n\nclass FetchRecipes(AsyncNode):\n    \"\"\"AsyncNode that fetches recipes.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get ingredient from user.\"\"\"\n        ingredient = await get_user_input(\"Enter ingredient: \")\n        return ingredient\n    \n    async def exec_async(self, ingredient):\n        \"\"\"Fetch recipes asynchronously.\"\"\"\n        recipes = await fetch_recipes(ingredient)\n        return recipes\n    \n    async def post_async(self, shared, prep_res, recipes):\n        \"\"\"Store recipes and continue.\"\"\"\n        shared[\"recipes\"] = recipes\n        shared[\"ingredient\"] = prep_res\n        return \"suggest\"\n\nclass SuggestRecipe(AsyncNode):\n    \"\"\"AsyncNode that suggests a recipe using LLM.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get recipes from shared store.\"\"\"\n        return shared[\"recipes\"]\n    \n    async def exec_async(self, recipes):\n        \"\"\"Get suggestion from LLM.\"\"\"\n        suggestion = await call_llm_async(\n            f\"Choose best recipe from: {', '.join(recipes)}\"\n        )\n        return suggestion\n    \n    async def post_async(self, shared, prep_res, suggestion):\n        \"\"\"Store suggestion and continue.\"\"\"\n        shared[\"suggestion\"] = suggestion\n        return \"approve\"\n\nclass GetApproval(AsyncNode):\n    \"\"\"AsyncNode that gets user approval.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get current suggestion.\"\"\"\n        return shared[\"suggestion\"]\n    \n    async def exec_async(self, suggestion):\n        \"\"\"Ask for user approval.\"\"\"\n        answer = await get_user_input(f\"\\nAccept this recipe? (y/n): \")\n        return answer\n    \n    async def post_async(self, shared, prep_res, answer):\n        \"\"\"Handle user's decision.\"\"\"\n        if answer == \"y\":\n            print(\"\\nGreat choice! Here's your recipe...\")\n            print(f\"Recipe: {shared['suggestion']}\")\n            print(f\"Ingredient: {shared['ingredient']}\")\n            return \"accept\"\n        else:\n            print(\"\\nLet's try another recipe...\")\n            return \"retry\" ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-async-basic/nodes.py`:**\n\n1. **Primary Purpose**  \nThis code defines three asynchronous workflow nodes for a recipe recommendation process, to be used with a framework called PocketFlow. Each node is responsible for a step in an async pipeline: fetching recipes by ingredient, suggesting a recipe using an LLM, and obtaining user approval for the suggestion.\n\n2. **Parameters**  \nEach class (`FetchRecipes`, `SuggestRecipe`, `GetApproval`) inherits from `AsyncNode` and implements three async methods with the following parameters:\n- `prep_async(self, shared)`: `shared` is a mutable dictionary for passing data between nodes.\n- `exec_async(self, arg)`: `arg` is the result from `prep_async` (could be user input or recipes list).\n- `post_async(self, shared, prep_res, exec_res)`: receives the shared dictionary, the result of `prep_async`, and the result of `exec_async`.\n\n3. **Return Value**  \n- Each node\u2019s `prep_async` and `exec_async` methods return data to be passed to the next step.\n- Each `post_async` method stores results in the shared dictionary and returns a string (e.g., `\"suggest\"`, `\"approve\"`, `\"accept\"`, or `\"retry\"`) to control flow in the pipeline.\n\n4. **Internally Called Functions or Methods**\n- `get_user_input(...)` (from `utils`): async function for receiving user input.\n- `fetch_recipes(ingredient)` (from `utils`): async function for fetching recipes using an ingredient.\n- `call_llm_async(...)` (from `utils`): async function that calls a language model (likely LLM) to pick a recipe.\n- `print(...)`: prints information to the console.\n\n**In summary:**  \nThe code models a stepwise, asynchronous recipe suggestion and approval workflow using custom async nodes, coordinating user input, external API calls, and shared state across the steps."
    },
    "170": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-async-basic/flow.py",
      "code": "\"\"\"AsyncFlow implementation for recipe finder.\"\"\"\n\nfrom pocketflow import AsyncFlow, Node\nfrom nodes import FetchRecipes, SuggestRecipe, GetApproval\n\nclass NoOp(Node):\n    \"\"\"Node that does nothing, used to properly end the flow.\"\"\"\n    pass\n\ndef create_flow():\n    \"\"\"Create and connect nodes into a flow.\"\"\"\n    \n    # Create nodes\n    fetch = FetchRecipes()\n    suggest = SuggestRecipe()\n    approve = GetApproval()\n    end = NoOp()\n    \n    # Connect nodes\n    fetch - \"suggest\" >> suggest\n    suggest - \"approve\" >> approve\n    approve - \"retry\" >> suggest  # Loop back for another suggestion\n    approve - \"accept\" >> end     # Properly end the flow\n    \n    # Create flow starting with fetch\n    flow = AsyncFlow(start=fetch)\n    return flow ",
      "summary": "**Summary of code unit `flow.py`:**\n\n1. **Primary purpose:**  \n   This code defines the asynchronous control flow for a recipe finder application using the `AsyncFlow` framework. It organizes how various tasks and user interactions (fetching, suggesting, and approving recipes) are sequenced and interconnected.\n\n2. **Parameters:**  \n   - The `create_flow()` function does **not** take any parameters.\n\n3. **Return value:**  \n   - The `create_flow()` function returns an `AsyncFlow` object representing the constructed flow, starting with the `FetchRecipes` node and moving through suggestion, approval, and completion steps.\n\n4. **Internally called functions/methods:**  \n   - `FetchRecipes()` (class constructor from `nodes`)\n   - `SuggestRecipe()` (class constructor from `nodes`)\n   - `GetApproval()` (class constructor from `nodes`)\n   - `NoOp()` (local class constructor)\n   - `AsyncFlow(start=...)` (from `pocketflow`)  \n   - Overloaded operators on node objects for connecting flow:  \n     - `- \"suggest\" >> suggest`\n     - `- \"approve\" >> approve`\n     - `- \"retry\" >> suggest`\n     - `- \"accept\" >> end`"
    },
    "171": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-async-basic/main.py",
      "code": "import asyncio\nfrom flow import create_flow\n\nasync def main():\n    \"\"\"Run the recipe finder flow.\"\"\"\n    # Create flow\n    flow = create_flow()\n    \n    # Create shared store\n    shared = {}\n    \n    # Run flow\n    print(\"\\nWelcome to Recipe Finder!\")\n    print(\"------------------------\")\n    await flow.run_async(shared)\n    print(\"\\nThanks for using Recipe Finder!\")\n\nif __name__ == \"__main__\":\n    # Run the async main function\n    asyncio.run(main()) ",
      "summary": "**Summary of main.py**\n\n1. **Primary Purpose:**\n   - This code serves as the entry point for an asynchronous \"Recipe Finder\" application. It initializes a flow (workflow), creates a shared data store, and orchestrates the execution of the recipe-finding process.\n\n2. **Parameters:**\n   - The `main()` function and the script as a whole do not take any external parameters.\n\n3. **Return Value:**\n   - The `main()` function does not return any value; it is designed for its side effects (console output and running the flow).\n\n4. **Internally Called Functions and Methods:**\n   - `create_flow()` (imported from the `flow` module): Used to set up the application's main flow.\n   - `flow.run_async(shared)`: Asynchronously runs the flow with a shared store.\n   - `asyncio.run(main())`: Runs the asynchronous `main()` function when the script is executed directly.\n   - Built-in functions: `print()` is used to display messages to the user."
    },
    "172": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-async-basic/utils.py",
      "code": "import asyncio\nimport aiohttp\nfrom openai import AsyncOpenAI\n\nasync def fetch_recipes(ingredient):\n    \"\"\"Fetch recipes from an API asynchronously.\"\"\"\n    print(f\"Fetching recipes for {ingredient}...\")\n    \n    # Simulate API call with delay\n    await asyncio.sleep(1)\n    \n    # Mock recipes (in real app, would fetch from API)\n    recipes = [\n        f\"{ingredient} Stir Fry\",\n        f\"Grilled {ingredient} with Herbs\",\n        f\"Baked {ingredient} with Vegetables\"\n    ]\n    \n    print(f\"Found {len(recipes)} recipes.\")\n    \n    return recipes\n\nasync def call_llm_async(prompt):\n    \"\"\"Make async LLM call.\"\"\"\n    print(\"\\nSuggesting best recipe...\")\n    \n    # Simulate LLM call with delay\n    await asyncio.sleep(1)\n    \n    # Mock LLM response (in real app, would call OpenAI)\n    recipes = prompt.split(\": \")[1].split(\", \")\n    suggestion = recipes[1]  # Always suggest second recipe\n    \n    print(f\"How about: {suggestion}\")\n    return suggestion\n\nasync def get_user_input(prompt):\n    \"\"\"Get user input asynchronously.\"\"\"\n    # Create event loop to handle async input\n    loop = asyncio.get_event_loop()\n    \n    # Get input in a non-blocking way\n    answer = await loop.run_in_executor(None, input, prompt)\n\n    return answer.lower() ",
      "summary": "**Summary of `utils.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides asynchronous utility functions for fetching recipe suggestions based on a user-supplied ingredient, making mock calls to a language model for recipe recommendations, and obtaining user input asynchronously.\n\n2. **Parameters:**\n   - `fetch_recipes(ingredient)`:  \n     - `ingredient` (str): The main ingredient to search for recipes.\n   - `call_llm_async(prompt)`:  \n     - `prompt` (str): A prompt string containing a list of recipes.\n   - `get_user_input(prompt)`:  \n     - `prompt` (str): The question or message to display when asking for user input.\n\n3. **Return Values:**\n   - `fetch_recipes`: Returns a list of recipe names (list of strings) featuring the provided ingredient.\n   - `call_llm_async`: Returns a single recipe suggestion (str) from the provided prompt.\n   - `get_user_input`: Returns the user's input as a lowercase string.\n\n4. **Other Functions/Methods Called Internally:**\n   - `asyncio.sleep()` (in `fetch_recipes` and `call_llm_async`)\n   - `print()` (in all functions)\n   - `asyncio.get_event_loop()` and `run_in_executor()` (in `get_user_input`)\n   - `input()` (within `run_in_executor` in `get_user_input`)\n   - Basic string methods like `split()`, `lower()` (in `call_llm_async` and `get_user_input`)"
    },
    "173": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-code-generator/nodes.py",
      "code": "import yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.call_llm import call_llm\nfrom utils.code_executor import execute_python\n\nclass GenerateTestCases(Node):\n    def prep(self, shared):\n        return shared[\"problem\"]\n\n    def exec(self, problem):\n        prompt = f\"\"\"Generate 5-7 test cases for this coding problem:\n\n{problem}\n\nOutput in this YAML format with reasoning:\n```yaml\nreasoning: |\n    The input parameters should be: param1 as a string, and param2 as a number.\n    To test the function, I will consider basic cases, edge cases, and corner cases.\n    For this problem, I need to test...\ntest_cases:\n  - name: \"Basic case\"\n    input: {{param1: value1, param2: value2}}\n    expected: result1\n  - name: \"Edge case - empty\"\n    input: {{param1: value3, param2: value4}}\n    expected: result2\n```\"\"\"\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        result = yaml.safe_load(yaml_str)\n        \n        # Validation asserts\n        assert \"test_cases\" in result, \"Result must have 'test_cases' field\"\n        assert isinstance(result[\"test_cases\"], list), \"test_cases must be a list\"\n        \n        for i, test_case in enumerate(result[\"test_cases\"]):\n            assert \"name\" in test_case, f\"Test case {i} missing 'name' field\"\n            assert isinstance(test_case[\"name\"], str), f\"Test case {i} 'name' must be string\"\n            assert \"input\" in test_case, f\"Test case {i} missing 'input' field\"\n            assert isinstance(test_case[\"input\"], dict), f\"Test case {i} 'input' must be dict\"\n            assert \"expected\" in test_case, f\"Test case {i} missing 'expected' field\"\n        \n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"test_cases\"] = exec_res[\"test_cases\"]\n        \n        # Print all generated test cases\n        print(f\"\\n=== Generated {len(exec_res['test_cases'])} Test Cases ===\")\n        for i, test_case in enumerate(exec_res[\"test_cases\"], 1):\n            print(f\"{i}. {test_case['name']}\")\n            print(f\"   input: {test_case['input']}\")\n            print(f\"   expected: {test_case['expected']}\")\n\nclass ImplementFunction(Node):\n    def prep(self, shared):\n        return shared[\"problem\"], shared[\"test_cases\"]\n\n    def exec(self, inputs):\n        problem, test_cases = inputs\n        \n        # Format test cases nicely for the prompt\n        formatted_tests = \"\"\n        for i, test in enumerate(test_cases, 1):\n            formatted_tests += f\"{i}. {test['name']}\\n\"\n            formatted_tests += f\"   input: {test['input']}\\n\"\n            formatted_tests += f\"   expected: {test['expected']}\\n\\n\"\n        \n        prompt = f\"\"\"Implement a solution for this problem:\n\n{problem}\n\nTest cases to consider:\n{formatted_tests}\n\nIMPORTANT: The function name must be exactly \"run_code\"\n\nOutput in this YAML format:\n```yaml\nreasoning: |\n    To implement this function, I will...\n    My approach is...\nfunction_code: |\n    def run_code(...):\n        # your implementation\n        return result\n```\"\"\"\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        result = yaml.safe_load(yaml_str)\n        \n        # Validation asserts\n        assert \"function_code\" in result, \"Result must have 'function_code' field\"\n        assert isinstance(result[\"function_code\"], str), \"function_code must be string\"\n        assert \"def run_code\" in result[\"function_code\"], \"Function must be named 'run_code'\"\n        \n        return result[\"function_code\"]\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"function_code\"] = exec_res\n        \n        # Print the implemented function\n        print(f\"\\n=== Implemented Function ===\")\n        print(exec_res)\n\nclass RunTests(BatchNode):\n    def prep(self, shared):\n        function_code = shared[\"function_code\"]\n        test_cases = shared[\"test_cases\"]\n        # Return list of tuples (function_code, test_case)\n        return [(function_code, test_case) for test_case in test_cases]\n\n    def exec(self, test_data):\n        function_code, test_case = test_data\n        output, error = execute_python(function_code, test_case[\"input\"])\n        \n        if error:\n            return {\n                \"test_case\": test_case,\n                \"passed\": False,\n                \"actual\": None,\n                \"expected\": test_case[\"expected\"],\n                \"error\": error\n            }\n        \n        passed = output == test_case[\"expected\"]\n        return {\n            \"test_case\": test_case,\n            \"passed\": passed,\n            \"actual\": output,\n            \"expected\": test_case[\"expected\"],\n            \"error\": None if passed else f\"Expected {test_case['expected']}, got {output}\"\n        }\n\n    def post(self, shared, prep_res, exec_res_list):\n        shared[\"test_results\"] = exec_res_list\n        all_passed = all(result[\"passed\"] for result in exec_res_list)\n        shared[\"iteration_count\"] = shared.get(\"iteration_count\", 0) + 1\n        \n        # Print test results\n        passed_count = len([r for r in exec_res_list if r[\"passed\"]])\n        total_count = len(exec_res_list)\n        print(f\"\\n=== Test Results: {passed_count}/{total_count} Passed ===\")\n        \n        failed_tests = [r for r in exec_res_list if not r[\"passed\"]]\n        if failed_tests:\n            print(\"Failed tests:\")\n            for i, result in enumerate(failed_tests, 1):\n                test_case = result['test_case']\n                print(f\"{i}. {test_case['name']}:\")\n                if result['error']:\n                    print(f\"   error: {result['error']}\")\n                else:\n                    print(f\"   output: {result['actual']}\")\n                print(f\"   expected: {result['expected']}\")\n        \n        if all_passed:\n            return \"success\"\n        elif shared[\"iteration_count\"] >= shared.get(\"max_iterations\", 5):\n            return \"max_iterations\"\n        else:\n            return \"failure\"\n\nclass Revise(Node):\n    def prep(self, shared):\n        failed_tests = [r for r in shared[\"test_results\"] if not r[\"passed\"]]\n        return {\n            \"problem\": shared[\"problem\"],\n            \"test_cases\": shared[\"test_cases\"],\n            \"function_code\": shared[\"function_code\"],\n            \"failed_tests\": failed_tests\n        }\n\n    def exec(self, inputs):\n        # Format current test cases nicely\n        formatted_tests = \"\"\n        for i, test in enumerate(inputs['test_cases'], 1):\n            formatted_tests += f\"{i}. {test['name']}\\n\"\n            formatted_tests += f\"   input: {test['input']}\\n\"\n            formatted_tests += f\"   expected: {test['expected']}\\n\\n\"\n        \n        # Format failed tests nicely\n        formatted_failures = \"\"\n        for i, result in enumerate(inputs['failed_tests'], 1):\n            test_case = result['test_case']\n            formatted_failures += f\"{i}. {test_case['name']}:\\n\"\n            if result['error']:\n                formatted_failures += f\"   error: {result['error']}\\n\"\n            else:\n                formatted_failures += f\"   output: {result['actual']}\\n\"\n            formatted_failures += f\"   expected: {result['expected']}\\n\\n\"\n\n        prompt = f\"\"\"Problem: {inputs['problem']}\n\nCurrent test cases:\n{formatted_tests}\n\nCurrent function:\n```python\n{inputs['function_code']}\n```\n\nFailed tests:\n{formatted_failures}\n\nAnalyze the failures and output revisions in YAML. You can revise test cases, function code, or both:\n\n```yaml\nreasoning: |\n    Looking at the failures, I see that...\n    The issue appears to be...\n    I will revise...\ntest_cases:  # Dictionary mapping test case index (1-based) to revised test case\n  1:\n    name: \"Revised test name\"\n    input: {{...}}\n    expected: ...\nfunction_code: |  # Include this if revising function\n  def run_code(...):\n    return ...\n```\"\"\"\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        result = yaml.safe_load(yaml_str)\n        \n        # Validation asserts\n        if \"test_cases\" in result:\n            assert isinstance(result[\"test_cases\"], dict), \"test_cases must be a dictionary\"\n            for index_str, test_case in result[\"test_cases\"].items():\n                assert isinstance(index_str, (str, int)), \"test_cases keys must be strings or ints\"\n                assert \"name\" in test_case, f\"Revised test case {index_str} missing 'name' field\"\n                assert \"input\" in test_case, f\"Revised test case {index_str} missing 'input' field\"\n                assert \"expected\" in test_case, f\"Revised test case {index_str} missing 'expected' field\"\n        \n        if \"function_code\" in result:\n            assert isinstance(result[\"function_code\"], str), \"function_code must be string\"\n            assert \"def run_code\" in result[\"function_code\"], \"Function must be named 'run_code'\"\n        \n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        # Print what is being revised\n        print(f\"\\n=== Revisions (Iteration {shared['iteration_count']}) ===\")\n        \n        # Handle test case revisions - map indices to actual test cases\n        if \"test_cases\" in exec_res:\n            current_tests = shared[\"test_cases\"].copy()\n            print(\"Revising test cases:\")\n            for index_str, revised_test in exec_res[\"test_cases\"].items():\n                index = int(index_str) - 1  # Convert to 0-based\n                if 0 <= index < len(current_tests):\n                    old_test = current_tests[index]\n                    print(f\"  Test {index_str}: '{old_test['name']}' -> '{revised_test['name']}'\")\n                    print(f\"    old input: {old_test['input']}\")\n                    print(f\"    new input: {revised_test['input']}\")\n                    print(f\"    old expected: {old_test['expected']}\")\n                    print(f\"    new expected: {revised_test['expected']}\")\n                    current_tests[index] = revised_test\n            shared[\"test_cases\"] = current_tests\n            \n        if \"function_code\" in exec_res:\n            print(\"Revising function code:\")\n            print(\"New function:\")\n            print(exec_res[\"function_code\"])\n            shared[\"function_code\"] = exec_res[\"function_code\"] ",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary purpose:**  \nThe code defines four modular nodes for an automated code generation and iterative improvement pipeline, likely for AI-driven code assistants. Each node encapsulates a process in solving code problems: generating test cases, implementing a solution, running tests, and revising based on feedback.\n\n2. **Parameters description:**  \nEach node operates on a `shared` state dictionary that contains problem descriptions, test cases, function code, and results. Nodes communicate by reading from and writing to this shared dictionary, rather than through traditional parameters. The nodes' `prep` and `exec` methods accept and process relevant subsets of this shared data (e.g., the problem statement, test cases, or function code).\n\n3. **Return value description:**  \n- `GenerateTestCases.exec` returns a dictionary loaded from YAML, which includes the reasoning and a list of test cases.\n- `ImplementFunction.exec` returns a string of Python code for the function implementation.\n- `RunTests.exec` returns a dictionary with the result for one test case (including pass/fail, outputs, errors).\n- `Revise.exec` returns a dictionary from YAML, potentially including updated test cases and/or revised function code.\n- The nodes\u2019 `post` methods update the `shared` dictionary and print summaries; the `RunTests.post` method returns \"success\", \"failure\", or \"max_iterations\" strings, depending on the test outcomes and process iteration.\n\n4. **List of internally called functions/methods:**\n- `call_llm`: Invokes a large language model to generate test cases, implement code, or provide revisions.\n- `yaml.safe_load`: Parses YAML-formatted strings generated by the LLM.\n- `execute_python`: Dynamically executes Python function code on input data for test case validation.\n- Various standard Python built-in functions for assertions, printing, iteration, etc.\n\n**Classes Defined:**\n- `GenerateTestCases`: Prompts the LLM for test case generation, validates them, and stores them.\n- `ImplementFunction`: Prompts the LLM for a solution function using the problem and test cases, validates, and stores code.\n- `RunTests`: Executes the generated function against test cases, records and summarizes pass/fail results.\n- `Revise`: Prompts the LLM to analyze and revise either test cases or the function code based on failed tests; applies and logs these revisions.\n\n**Overall**, the code implements a modular, AI-driven, test-driven development workflow, managing problem state and iterative improvement via node classes and shared data."
    },
    "174": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-code-generator/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import GenerateTestCases, ImplementFunction, RunTests, Revise\n\ndef create_code_generator_flow():\n    \"\"\"Creates and returns the code generator flow.\"\"\"\n    # Create nodes\n    generate_tests = GenerateTestCases()\n    implement_function = ImplementFunction()\n    run_tests = RunTests()\n    revise = Revise()\n\n    # Define transitions\n    generate_tests >> implement_function\n    implement_function >> run_tests\n    run_tests - \"failure\" >> revise\n    revise >> run_tests\n\n    # Create flow starting with test generation\n    flow = Flow(start=generate_tests)\n    return flow ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-code-generator/flow.py`:**\n\n1. **Primary Purpose:**  \n   This code unit defines a function to create and return a workflow (or \"flow\") for an automated code generation pipeline. The pipeline consists of nodes for generating test cases, implementing functions, running tests, and revising code based on test results.\n\n2. **Parameters:**  \n   The `create_code_generator_flow()` function does **not** take any parameters.\n\n3. **Return Value:**  \n   The function returns a `Flow` object, with the entire code generation process starting at the test generation node and following a defined sequence of steps.\n\n4. **Internal Function/Method Calls:**\n   - `GenerateTestCases()`\n   - `ImplementFunction()`\n   - `RunTests()`\n   - `Revise()`\n   - The `>>` operator is used for defining transitions between nodes.\n   - The `- \"failure\" >>` syntax defines a conditional transition.\n   - `Flow(start=generate_tests)` (constructor call)\n\n**In summary:**  \nThe code creates a `Flow` object that represents a sequence of automated steps (test generation \u2192 implementation \u2192 testing \u2192 possible revision) for code generation, using node and flow constructs. It neither takes parameters nor requires input, and returns the constructed workflow."
    },
    "175": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-code-generator/main.py",
      "code": "import sys\nfrom flow import create_code_generator_flow\n\ndef main():\n    \"\"\"Runs the PocketFlow Code Generator application.\"\"\"\n    print(\"Starting PocketFlow Code Generator...\")\n    \n    # Check if problem is provided as argument\n    if len(sys.argv) > 1:\n        problem = \" \".join(sys.argv[1:])\n    else:\n        # Default Two Sum problem\n        problem = \"\"\"Two Sum\n\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\n\nExample 1:\nInput: nums = [2,7,11,15], target = 9\nOutput: [0,1]\n\nExample 2:\nInput: nums = [3,2,4], target = 6\nOutput: [1,2]\n\nExample 3:\nInput: nums = [3,3], target = 6\nOutput: [0,1]\"\"\"\n\n    shared = {\n        \"problem\": problem,\n        \"test_cases\": [],  # Will be populated with [{name, input, expected}, ...]\n        \"function_code\": \"\",\n        \"test_results\": [],\n        \"iteration_count\": 0,\n        \"max_iterations\": 5\n    }\n\n    # Create and run the flow\n    flow = create_code_generator_flow()\n    flow.run(shared)\n    \n    print(\"\\n=== Final Results ===\")\n    print(f\"Problem: {shared['problem'][:50]}...\")\n    print(f\"Iterations: {shared['iteration_count']}\")\n    print(f\"Function:\\n{shared['function_code']}\")\n    print(f\"Test Results: {len([r for r in shared['test_results'] if r['passed']])}/{len(shared['test_results'])} passed\")\n\nif __name__ == \"__main__\":\n    main() ",
      "summary": "**Summary of Code Unit: main.py**\n\n1. **Primary Purpose:**  \n   This code unit serves as the entry point for the PocketFlow Code Generator application. It initializes the problem statement (either from command-line arguments or a default \"Two Sum\" problem), sets up a shared state for the code generation process, and invokes the main code generation workflow.\n\n2. **Parameters:**  \n   The `main` function does not take any parameters. The script as a whole can receive command-line arguments via `sys.argv`, which are used to specify a custom problem statement.\n\n3. **Return Value:**  \n   Neither the `main` function nor the script returns any value. All output is handled via standard output (print statements), and state is managed via the `shared` dictionary.\n\n4. **Functions or Methods Called Internally:**\n   - `print` (built-in): For displaying status and results.\n   - `flow = create_code_generator_flow()` (from the local `flow` module): Instantiates the code generation workflow.\n   - `flow.run(shared)`: Runs the code generation flow with the shared state.\n   - `sys.argv`: Reads command-line arguments.\n\nOverall, the script prepares the environment and parameters for an iterative code generation process, then delegates the main computation to the workflow defined elsewhere."
    },
    "176": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-code-generator/utils/__init__.py",
      "code": "",
      "summary": "It appears that the provided code unit from /tmp/PocketFlow/cookbook/pocketflow-code-generator/utils/__init__.py is empty\u2014no code or definitions are present.\n\nSummary:\n1. Primary purpose: This __init__.py file currently serves as a package initializer for the utils module but does not contain any logic or code.\n2. Parameters: None.\n3. Return value: None.\n4. Other functions or methods called: None.\n\nIf you intended to analyze actual code, please provide the code content from the file."
    },
    "177": {
      "unit_name": "call_llm.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-code-generator/utils/call_llm.py",
      "code": "from anthropic import Anthropic\nimport os\n\ndef call_llm(prompt):\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = client.messages.create(\n        model=\"claude-sonnet-4-20250514\",\n        max_tokens=6000,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[0].text\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")",
      "summary": "**Summary of call_llm.py:**\n\n1. **Primary purpose:**  \n   The code unit provides a function to send a prompt to the Anthropic Claude LLM (Language Learning Model) API and retrieve its generated response.\n\n2. **Parameters:**  \n   - The `call_llm` function takes a single parameter:\n     - `prompt`: a string containing the input text or question to be sent to the language model.\n\n3. **Return value:**  \n   - The `call_llm` function returns the generated response text from the language model as a string.\n\n4. **Internal functions/methods called:**  \n   - `Anthropic(...)` (constructor for Anthropic API client)\n   - `os.environ.get(...)` (to retrieve the API key from environment variables)\n   - `client.messages.create(...)` (to send the prompt and receive a response from the Claude model)\n   - `response.content[0].text` (to extract the response text from the API response)\n   - In the test block: `print(...)` (for output)\n\n**Note:**  \nThe code expects the environment variable `ANTHROPIC_API_KEY` to be set with a valid API key, otherwise it defaults to `\"your-api-key\"`."
    },
    "178": {
      "unit_name": "code_executor.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-code-generator/utils/code_executor.py",
      "code": "import sys\nimport io\nimport traceback\nfrom contextlib import redirect_stdout, redirect_stderr\n\ndef execute_python(function_code, input):\n    try:\n        namespace = {\"__builtins__\": __builtins__}\n        stdout_capture = io.StringIO()\n        stderr_capture = io.StringIO()\n        \n        with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):\n            exec(function_code, namespace)\n            \n            if \"run_code\" not in namespace:\n                return None, \"Function 'run_code' not found\"\n            \n            run_code = namespace[\"run_code\"]\n            \n            if isinstance(input, dict):\n                result = run_code(**input)\n            elif isinstance(input, (list, tuple)):\n                result = run_code(*input)\n            else:\n                result = run_code(input)\n            \n            return result, None\n                \n    except Exception as e:\n        return None, f\"{type(e).__name__}: {str(e)}\"\n\nif __name__ == \"__main__\":\n    # Test 1: Working function\n    function_code = \"\"\"\ndef run_code(nums, target):\n    for i in range(len(nums)):\n        for j in range(i + 1, len(nums)):\n            if nums[i] + nums[j] == target:\n                return [i, j]\n    return []\n\"\"\"\n    \n    input = {\"nums\": [2, 7, 11, 15], \"target\": 9}\n    output, error = execute_python(function_code, input)\n    print(f\"Output: {output}\")\n    print(f\"Error: {error}\")\n    \n    # Test 2: Function with error\n    broken_function_code = \"\"\"\ndef run_code(nums, target):\n    return nums[100]  # Index error\n\"\"\"\n    \n    output2, error2 = execute_python(broken_function_code, input)\n    print(f\"Output: {output2}\")\n    print(f\"Error: {error2}\") ",
      "summary": "**Summary for `code_executor.py`:**\n\n1. **Primary Purpose:**  \n   The main purpose of this code unit is to safely execute dynamically provided Python code that defines a function `run_code`, and invoke this function with given input arguments while capturing any output or errors.\n\n2. **Parameters:**  \n   - The `execute_python` function has two parameters:\n     - `function_code`: A string containing Python code that defines a function named `run_code`.\n     - `input`: The input arguments to be passed to `run_code`, which can be a dictionary (for keyword arguments), a list/tuple (for positional arguments), or a single value.\n\n3. **Return Value:**  \n   - Returns a tuple `(result, error)`:\n     - `result`: The output produced by the `run_code` function (or `None` if there's an error).\n     - `error`: A string describing any exception that occurred (or `None` if there was no error).\n\n4. **Internal Functions and Methods Called:**  \n   - `exec()`\n   - `isinstance()`\n   - `redirect_stdout()` and `redirect_stderr()` from `contextlib`\n   - `io.StringIO()`\n   - Standard dictionary operations for namespace access\n   - Exception handling (`try`/`except`)\n   - Standard output and error printing (`print()`) in the `__main__` block (for testing)"
    },
    "179": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch/main.py",
      "code": "import os\nimport time\nfrom pocketflow import BatchNode, Flow\nfrom utils import call_llm\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \n                              \"Russian\", \"Portuguese\", \"French\", \"Korean\"])\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        \n        prompt = f\"\"\"\nPlease translate the following markdown file into {language}. \nBut keep the original markdown format, links and code blocks.\nDirectly return the translated text, without any other text or comments.\n\nOriginal: \n{text}\n\nTranslated:\"\"\"\n        \n        result = call_llm(prompt)\n        print(f\"Translated {language} text\")\n        return {\"language\": language, \"translation\": result}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Create output directory if it doesn't exist\n        output_dir = shared.get(\"output_dir\", \"translations\")\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Write each translation to a file\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            \n            # Write to file\n            filename = os.path.join(output_dir, f\"README_{language.upper()}.md\")\n            with open(filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(translation)\n            \n            print(f\"Saved translation to {filename}\")\n\nif __name__ == \"__main__\":\n    # read the text from ../../README.md\n    with open(\"../../README.md\", \"r\") as f:\n        text = f.read()\n    \n    # Default settings\n    shared = {\n        \"text\": text,\n        \"languages\": [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \"Russian\", \"Portuguese\", \"French\", \"Korean\"],\n        \"output_dir\": \"translations\"\n    }\n\n    # --- Time Measurement Start ---\n    print(f\"Starting sequential translation into {len(shared['languages'])} languages...\")\n    start_time = time.perf_counter()\n\n    # Run the translation flow\n    translate_node = TranslateTextNode(max_retries=3)\n    flow = Flow(start=translate_node)\n    flow.run(shared)\n\n    # --- Time Measurement End ---\n    end_time = time.perf_counter()\n    duration = end_time - start_time\n\n    print(f\"\\nTotal sequential translation time: {duration:.4f} seconds\") # Print duration\n    print(\"\\n=== Translation Complete ===\")\n    print(f\"Translations saved to: {shared['output_dir']}\")\n    print(\"============================\")",
      "summary": "**Summary of `main.py` in `/tmp/PocketFlow/cookbook/pocketflow-batch/`:**\n\n1. **Primary Purpose:**  \n   The code\u2019s primary purpose is to batch-translate the contents of a `README.md` file into multiple languages using an LLM (Large Language Model), preserving markdown formatting. It saves each translation as a separate file in a designated output directory.\n\n2. **Parameters (Inputs):**  \n   - **`shared` dictionary:**  \n     - `text`: The markdown text to translate (read from `../../README.md` by default).\n     - `languages`: A list of target languages (defaults to: Chinese, Spanish, Japanese, German, Russian, Portuguese, French, Korean).\n     - `output_dir`: The directory where output files will be saved (default: `translations`).\n\n3. **Return Value (Outputs):**  \n   - The program does not return a value from the main function.  \n   - The translation results are saved as separate markdown files in the specified output directory, named according to their language (e.g., `README_CHINESE.md`).\n\n4. **Other Functions or Methods Called Internally:**  \n   - **Internal methods:**  \n     - `TranslateTextNode.prep`: Prepares batches for translation.\n     - `TranslateTextNode.exec`: Performs the actual translation using an LLM.\n     - `TranslateTextNode.post`: Saves translations to individual files.\n   - **Library/framework calls:**  \n     - `call_llm`: Sends the prompt for translation to an LLM.\n     - `os.makedirs`: Creates the output directory.\n     - Standard file operations: `open`, `f.read()`, and `f.write()`.\n     - Timing: `time.perf_counter` (to measure execution duration).\n     - Print statements for progress and results.\n   - **Batch processing framework:**  \n     - `BatchNode`, `Flow` (from the `pocketflow` library) to manage the batching and workflow."
    },
    "180": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-batch/utils.py",
      "code": "from anthropic import Anthropic\nimport os\n\ndef call_llm(prompt):\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=20000,\n        thinking={\n            \"type\": \"enabled\",\n            \"budget_tokens\": 16000\n        },\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[1].text\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")",
      "summary": "**Summary of utils.py:**\n\n1. **Primary Purpose:**  \n   The code provides a utility function, `call_llm`, which sends a user-supplied prompt to Anthropic's Claude 3 language model and returns the model's generated response.\n\n2. **Parameters:**  \n   - The `call_llm` function takes a single parameter:\n     - `prompt`: a string containing the user's prompt to be sent to the language model.\n\n3. **Return Value:**  \n   - The `call_llm` function returns the text of the model's response as a string, specifically extracted from `response.content[1].text`.\n\n4. **Functions/Methods Called Internally:**  \n   - `os.environ.get` (to fetch the API key from environment variables)\n   - `Anthropic` constructor (from the `anthropic` library, to create an API client)\n   - `client.messages.create` (to send the prompt and receive the response from the model)\n\n**Note:** There appears to be a potential indexing bug in `response.content[1].text`, as typically one would expect model content at `[0]`. Also, the use of the `thinking` parameter is atypical for the standard Anthropic API and may cause errors unless using a custom extension."
    },
    "181": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/nodes.py",
      "code": "from pocketflow import Node\nfrom utils.generate_image import generate_image\n\nclass GenerateImageNode(Node):\n    \"\"\"Generates image from text prompt using OpenAI API.\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"task_input\", \"\")\n\n    def exec(self, prompt):\n        return generate_image(prompt)\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"input_used_by_process\"] = prep_res\n        shared[\"generated_image\"] = exec_res\n        shared[\"stage\"] = \"user_feedback\"\n        return \"default\"",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/nodes.py` (Unit: nodes.py):**\n\n1. **Primary Purpose:**  \n   The code defines a node class (`GenerateImageNode`) intended for use in a flow or pipeline. Its main function is to generate an image based on a text prompt, utilizing the OpenAI API through a helper function.\n\n2. **Parameters:**  \n   - `prep(self, shared)`: Takes a shared dictionary (likely shared state/context), retrieves the `\"task_input\"` key as the text prompt.\n   - `exec(self, prompt)`: Takes a text prompt (string).\n   - `post(self, shared, prep_res, exec_res)`: Accepts the shared dict, the prompt used (`prep_res`), and the generated image (`exec_res`).\n\n3. **Return Value:**  \n   - `prep` returns the extracted text prompt (string).\n   - `exec` returns the generated image (type depends on implementation of `generate_image`).\n   - `post` updates the shared dict with results and returns the string `\"default\"`.\n\n4. **Internal Function or Method Calls:**  \n   - `shared.get()` (dictionary method in `prep`)\n   - `generate_image()` (from `utils.generate_image`)"
    },
    "182": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/flow.py",
      "code": "from pocketflow import Flow\nfrom nodes import GenerateImageNode\n\ndef create_generation_flow():\n    \"\"\"Creates a flow for image generation (initial or regeneration).\"\"\"\n    generate_image_node = GenerateImageNode()\n    return Flow(start=generate_image_node)\n\n\n",
      "summary": "**Summary of `flow.py`:**\n\n1. **Primary purpose:**  \n   The code defines a function to create and return an image generation flow, leveraging a `GenerateImageNode` as the starting point. It is used to set up a process for generating (or regenerating) images within a flow-based architecture.\n\n2. **Parameters:**  \n   The `create_generation_flow()` function does **not** take any parameters.\n\n3. **Return value:**  \n   The function returns a `Flow` object whose starting node is an instance of `GenerateImageNode`.\n\n4. **Other functions or methods called internally:**  \n   - `GenerateImageNode()` (constructor, presumably from the local `nodes` module)\n   - `Flow(start=...)` (constructor, imported from `pocketflow`)"
    },
    "183": {
      "unit_name": "app.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/app.py",
      "code": "import streamlit as st\nimport base64\nfrom flow import create_generation_flow\n\nst.title(\"PocketFlow Image Generation HITL\")\n\n# Initialize session state for shared store\nif 'stage' not in st.session_state:\n    st.session_state.stage = \"initial_input\"\n    st.session_state.task_input = \"\"\n    st.session_state.generated_image = \"\"\n    st.session_state.final_result = \"\"\n    st.session_state.error_message = \"\"\n\n# Debug info\nwith st.expander(\"Session State\"):\n    st.json({k: v for k, v in st.session_state.items() if not k.startswith(\"_\")})\n\n# State-based UI\nif st.session_state.stage == \"initial_input\":\n    st.header(\"1. Generate Image\")\n    \n    prompt = st.text_area(\"Enter image prompt:\", value=st.session_state.task_input, height=100)\n    \n    if st.button(\"Generate Image\"):\n        if prompt.strip():\n            st.session_state.task_input = prompt\n            st.session_state.error_message = \"\"\n            \n            try:\n                with st.spinner(\"Generating image...\"):\n                    flow = create_generation_flow()\n                    flow.run(st.session_state)\n                st.rerun()\n            except Exception as e:\n                st.session_state.error_message = str(e)\n        else:\n            st.error(\"Please enter a prompt\")\n\nelif st.session_state.stage == \"user_feedback\":\n    st.header(\"2. Review Generated Image\")\n    \n    if st.session_state.generated_image:\n        # Display image\n        image_bytes = base64.b64decode(st.session_state.generated_image)\n        st.image(image_bytes, caption=f\"Prompt: {st.session_state.task_input}\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            if st.button(\"Approve\", use_container_width=True):\n                st.session_state.final_result = st.session_state.generated_image\n                st.session_state.stage = \"final\"\n                st.rerun()\n        \n        with col2:\n            if st.button(\"Regenerate\", use_container_width=True):\n                try:\n                    with st.spinner(\"Regenerating image...\"):\n                        flow = create_generation_flow()\n                        flow.run(st.session_state)\n                    st.rerun()\n                except Exception as e:\n                    st.session_state.error_message = str(e)\n\nelif st.session_state.stage == \"final\":\n    st.header(\"3. Final Result\")\n    st.success(\"Image approved!\")\n    \n    if st.session_state.final_result:\n        image_bytes = base64.b64decode(st.session_state.final_result)\n        st.image(image_bytes, caption=f\"Final approved image: {st.session_state.task_input}\")\n    \n    if st.button(\"Start Over\", use_container_width=True):\n        st.session_state.stage = \"initial_input\"\n        st.session_state.task_input = \"\"\n        st.session_state.generated_image = \"\"\n        st.session_state.final_result = \"\"\n        st.session_state.error_message = \"\"\n        st.rerun()\n\n# Show errors\nif st.session_state.error_message:\n    st.error(st.session_state.error_message)\n\n",
      "summary": "**Summary of `app.py` from `/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/app.py`:**\n\n1. **Primary Purpose:**  \n   This code implements a Streamlit web application for human-in-the-loop (HITL) image generation. The app guides users through entering an image prompt, generating an image based on that prompt, reviewing the generated result, and approving or regenerating until satisfied.\n\n2. **Parameters:**  \n   The main script does not define any traditional function parameters; instead, it uses Streamlit\u2019s session state to manage and persist application-wide variables (such as the current stage, input prompt, generated image, and error messages).\n\n3. **Return Value:**  \n   The script does not have a return value, as it is designed to run and render a user interface in the browser via Streamlit.\n\n4. **Other Functions or Methods Called Internally:**\n   - `streamlit.st.title`, `st.header`, `st.text_area`, `st.button`, `st.image`, `st.columns`, `st.error`, `st.success`, `st.expander`, `st.json`, `st.spinner`, `st.rerun`: Various Streamlit APIs for building the UI and managing the session.\n   - `base64.b64decode`: Decodes base64-encoded images for display.\n   - `flow.create_generation_flow`: Imported function that creates a flow object responsible for running the image generation process.\n   - `flow.run`: Method called on the flow object to execute image generation or regeneration with the current session state.\n\n**In Summary:**  \nThis code is the main UI entry point for a HITL image generation workflow using Streamlit. It manages user input and feedback in several stages, leverages a flow abstraction for generating images, and allows users to approve or iterate on results."
    },
    "184": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/utils/__init__.py",
      "code": "",
      "summary": "The code unit '/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/utils/__init__.py' is empty\u2014there is no code provided in your prompt.\n\nSummary:\n\n1. **Primary purpose**:  \n   - (None evident, as no code is present.) Typically, an __init__.py file marks the directory as a Python package, allowing its modules to be imported. Sometimes, such files include package-level imports or initialization code.\n\n2. **Parameters**:  \n   - None (no functions or classes are defined).\n\n3. **Return value**:  \n   - None (no functions or operations are present).\n\n4. **Functions/methods called internally**:  \n   - None.\n\nIf there is code in the file that you intended to share, please re-submit it for a more detailed analysis."
    },
    "185": {
      "unit_name": "generate_image.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/utils/generate_image.py",
      "code": "from openai import OpenAI\nimport os\nimport base64\n\ndef generate_image(prompt: str) -> str:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    response = client.images.generate(\n        model=\"gpt-image-1\",\n        prompt=prompt,\n        n=1,\n        size=\"1024x1024\"\n    )\n    \n    image_b64 = response.data[0].b64_json\n    print(f\"Generated image ({len(image_b64)} chars)\")\n    return image_b64\n\nif __name__ == \"__main__\":\n    test_prompt = \"A gray tabby cat hugging an otter with an orange scarf\"\n    print(f\"Generating image for prompt: {test_prompt[:50]}...\")\n    \n    image_base64 = generate_image(test_prompt)\n    print(f\"Success! Generated {len(image_base64)} characters of base64 data\")\n    \n    # Write image to local file for testing\n    image_bytes = base64.b64decode(image_base64)\n    with open(\"test_generated_image.png\", \"wb\") as f:\n        f.write(image_bytes)\n    print(\"Test image saved as test_generated_image.png\") ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-streamlit-fsm/utils/generate_image.py`:**\n\n1. **Primary Purpose:**  \n   The primary purpose of this code unit is to generate an AI-created image using the OpenAI API, given a text prompt, and return the result as a base64-encoded string.\n\n2. **Parameters:**  \n   - The main function, `generate_image`, accepts one parameter:\n     - `prompt` (str): A text string describing the image to be generated by the OpenAI model.\n\n3. **Return Value:**  \n   - `generate_image` returns a base64-encoded string representing the generated image.\n\n4. **Internally Called Functions and Methods:**  \n   - `os.getenv` (to get the OpenAI API key)\n   - `OpenAI` (instantiates the OpenAI API client)\n   - `client.images.generate` (calls the OpenAI API to generate the image)\n   - `base64.b64decode` (decodes the base64 string for saving/testing)\n   - File handling methods `open`/`write` (for saving an image if run as a script)\n   - `print` (for logging output to the console)"
    },
    "186": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-google-calendar/nodes.py",
      "code": "from pocketflow import Node\nfrom utils.google_calendar import create_event, list_events, list_calendar_lists\nfrom datetime import datetime, timedelta\n\nclass CreateCalendarEventNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the necessary data to create an event.\"\"\"\n        return {\n            'summary': shared.get('event_summary'),\n            'description': shared.get('event_description'),\n            'start_time': shared.get('event_start_time'),\n            'end_time': shared.get('event_end_time')\n        }\n    \n    def exec(self, event_data):\n        \"\"\"Creates a new calendar event.\"\"\"\n        try:\n            event = create_event(\n                summary=event_data['summary'],\n                description=event_data['description'],\n                start_time=event_data['start_time'],\n                end_time=event_data['end_time']\n            )\n            return {'success': True, 'event': event}\n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the event creation result.\"\"\"\n        if exec_res['success']:\n            shared['last_created_event'] = exec_res['event']\n            return 'success'\n        else:\n            shared['error'] = exec_res['error']\n            return 'error'\n\nclass ListCalendarEventsNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepares parameters to list events.\"\"\"\n        return {\n            'days': shared.get('days_to_list', 7)\n        }\n    \n    def exec(self, params):\n        \"\"\"Lists calendar events.\"\"\"\n        try:\n            events = list_events(days=params['days'])\n            return {'success': True, 'events': events}\n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the list of events.\"\"\"\n        if exec_res['success']:\n            shared['calendar_events'] = exec_res['events']\n            return 'success'\n        else:\n            shared['error'] = exec_res['error']\n            return 'error'\n\nclass ListCalendarsNode(Node):\n    def prep(self, shared):\n        \"\"\"No special preparation needed to list calendars.\"\"\"\n        return {}\n\n    def exec(self, params):\n        \"\"\"Lists all available calendars for the user.\"\"\"\n        try:\n            calendars = list_calendar_lists()\n            return {'success': True, 'calendars': calendars}\n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the list of calendars in the shared store.\"\"\"\n        if exec_res['success']:\n            shared['available_calendars'] = exec_res['calendars']\n            return 'success'\n        else:\n            shared['error'] = exec_res['error']\n            return 'error' ",
      "summary": "**Summary of `nodes.py`**\n\n1. **Primary Purpose:**  \n   This code defines a set of Node classes for integrating Google Calendar operations into a PocketFlow workflow. These Node classes encapsulate the logic for creating a calendar event, listing calendar events, and listing available calendars for a user, to be used as modular workflow steps.\n\n2. **Parameters:**  \n   - Each Node class has methods (`prep`, `exec`, and `post`) that work with dictionaries:\n     - The `prep` method receives a `shared` dictionary containing workflow state or inputs, and prepares the necessary parameters for the operation.\n     - The `exec` method receives parameters prepared by `prep` (such as event details or listing parameters).\n     - The `post` method receives `shared`, the preparation result, and the execution result to update the workflow state or capture errors.\n   - Specific expected keys in `shared` include:  \n     - `'event_summary'`, `'event_description'`, `'event_start_time'`, `'event_end_time'` for event creation.  \n     - `'days_to_list'` for listing events.\n\n3. **Return Values:**  \n   - The `prep` and `exec` methods return dictionaries with relevant data for the step (such as event parameters, a list of events/calendars, or error info).\n   - The `post` methods update the `shared` state in place and return a status string: either `'success'` or `'error'`.\n\n4. **Other Functions/Methods Called Internally:**  \n   - `create_event` from `utils.google_calendar`\n   - `list_events` from `utils.google_calendar`\n   - `list_calendar_lists` from `utils.google_calendar`\n\nThese classes enable adding Google Calendar operations to workflows in a structured, error-tolerant way, relying on helper functions for the actual Google API interactions."
    },
    "187": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-google-calendar/main.py",
      "code": "from pocketflow import Flow\nfrom nodes import CreateCalendarEventNode, ListCalendarEventsNode, ListCalendarsNode\nfrom datetime import datetime, timedelta\n\ndef create_calendar_flow():\n    \"\"\"Creates a flow to manage calendar events.\"\"\"\n    # Create nodes\n    create_event_node = CreateCalendarEventNode()\n    list_events_node = ListCalendarEventsNode()\n    \n    # Connect nodes\n    create_event_node - \"success\" >> list_events_node\n    create_event_node - \"error\" >> None\n    \n    # Create flow\n    return Flow(start=create_event_node)\n\ndef list_calendars_flow():\n    \"\"\"Creates a flow to list all user calendars.\"\"\"\n    list_calendars_node = ListCalendarsNode()\n    return Flow(start=list_calendars_node)\n\ndef main():\n    # Example: List all calendars\n    print(\"=== Listing your calendars ===\")\n    flow = list_calendars_flow()\n    shared = {}\n    flow.run(shared)\n    \n    if 'available_calendars' in shared:\n        for cal in shared['available_calendars']:\n            print(f\"- {cal.get('summary')}\")\n\n    # Example: Create a simple event\n    print(\"\\n=== Creating an example event ===\")\n    flow = create_calendar_flow()\n\n    shared = {\n        'event_summary': 'Example Meeting',\n        'event_description': 'An example meeting created by PocketFlow',\n        'event_start_time': datetime.now() + timedelta(days=1),\n        'event_end_time': datetime.now() + timedelta(days=1, hours=1),\n        'days_to_list': 7\n    }\n\n    flow.run(shared)\n    \n    if 'last_created_event' in shared:\n        print(\"Event created successfully!\")\n        print(f\"Event ID: {shared['last_created_event']['id']}\")\n\nif __name__ == \"__main__\":\n    main()",
      "summary": "**Summary of main.py**\n\n1. **Primary Purpose:**  \n   The code unit serves as a demonstration script for managing Google Calendar events using the PocketFlow framework. It provides flows to list all user calendars and to create a new calendar event, showcasing how to orchestrate different nodes (task units) within PocketFlow to interact with Google Calendar.\n\n2. **Parameters:**  \n   - The primary functions in this file (`main`, `create_calendar_flow`, `list_calendars_flow`) do not accept input parameters.  \n   - The `shared` dictionary acts as an implicit input/output data structure, carrying context and results between node executions within a flow.\n\n3. **Return Value:**  \n   - `create_calendar_flow()` and `list_calendars_flow()` each return a `Flow` object (from PocketFlow), preconfigured with relevant nodes as the starting point.  \n   - `main()` does not return a value; it is the script's entry point, managing the demonstration's execution flow.\n\n4. **Internally Called Functions / Methods:**\n   - **From this file:**  \n     - `list_calendars_flow()`  \n     - `create_calendar_flow()`\n   - **From external sources / libraries:**  \n     - `Flow.run(shared):` Executes the defined flow with the shared context.  \n     - Node constructors:\n       - `CreateCalendarEventNode()`\n       - `ListCalendarEventsNode()`\n       - `ListCalendarsNode()`\n     - Standard library:\n       - `datetime.now()`\n       - `timedelta(days=...)`\n   - **PocketFlow Node Connections:**  \n     - Node connection and routing logic using operators like `- \"success\" >>` and `- \"error\" >>`.\n   \n**Summary Statement:**  \nThis script demonstrates how to create and execute flows for listing calendars and creating events via Google Calendar using PocketFlow, with execution details managed via a shared data dictionary and modular, chainable node objects."
    },
    "188": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-google-calendar/utils/__init__.py",
      "code": "",
      "summary": "There is no code provided in '/tmp/PocketFlow/cookbook/pocketflow-google-calendar/utils/__init__.py'\u2014the code unit appears to be empty.\n\nSummary:\n\n1. Primary Purpose:  \nThe code unit does not contain any code, so it serves as an empty __init__.py file. Its main role is likely to indicate that the 'utils' directory is a Python package.\n\n2. Parameters:  \nThere are no parameters, as there are no functions or classes defined.\n\n3. Return Value:  \nThere are no return values, as the file is empty.\n\n4. Internal Calls:  \nNo functions or methods are called internally.\n\nIn summary, this empty __init__.py file simply marks the directory as a Python package and contains no implementation."
    },
    "189": {
      "unit_name": "google_calendar.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-google-calendar/utils/google_calendar.py",
      "code": "from google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom google.auth.transport.requests import Request\nfrom googleapiclient.discovery import build\nimport os.path\nimport os\nimport pickle\nfrom datetime import datetime, timedelta\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nCALENDAR_ID = os.getenv('GOOGLE_CALENDAR_ID')\nGOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\nTIMEZONE = os.getenv('TIMEZONE')\n\nSCOPES = ['https://www.googleapis.com/auth/calendar']\n\ndef get_calendar_service():\n    \"\"\"Gets the authenticated Google Calendar service.\"\"\"\n    creds = None\n    if os.path.exists('token.pickle'):\n        with open('token.pickle', 'rb') as token:\n            creds = pickle.load(token)\n    \n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                GOOGLE_APPLICATION_CREDENTIALS, SCOPES)\n            creds = flow.run_local_server(port=0)\n        with open('token.pickle', 'wb') as token:\n            pickle.dump(creds, token)\n\n    return build('calendar', 'v3', credentials=creds)\n\ndef create_event(summary, description, start_time, end_time, timezone=TIMEZONE):\n    \"\"\"Creates a new event in Google Calendar.\"\"\"\n    service = get_calendar_service()\n    \n    event = {\n        'summary': summary,\n        'description': description,\n        'start': {\n            'dateTime': start_time.isoformat(),\n            'timeZone': timezone,\n        },\n        'end': {\n            'dateTime': end_time.isoformat(),\n            'timeZone': timezone,\n        },\n    }\n\n    event = service.events().insert(calendarId=CALENDAR_ID, body=event).execute()\n    return event\n\ndef list_events(days=7):\n    \"\"\"Lists events for the next X days.\"\"\"\n    service = get_calendar_service()\n    \n    now = datetime.utcnow()\n    time_min = now.isoformat() + 'Z'\n    time_max = (now + timedelta(days=days)).isoformat() + 'Z'\n\n    events_result = service.events().list(\n        calendarId=CALENDAR_ID,\n        timeMin=time_min,\n        timeMax=time_max,\n        singleEvents=True,\n        orderBy='startTime'\n    ).execute()\n    \n    return events_result.get('items', [])\n\ndef create_custom_calendar(calendar_name, description=\"\"):\n    \"\"\"Creates a new custom calendar in Google Calendar.\"\"\"\n    service = get_calendar_service()\n    \n    calendar = {\n        'summary': calendar_name,\n        'description': description,\n        'timeZone': TIMEZONE\n    }\n\n    created_calendar = service.calendars().insert(body=calendar).execute()\n    return created_calendar\n\ndef list_calendar_lists():\n    \"\"\"Lists all available calendars for the user.\"\"\"\n    service = get_calendar_service()\n    \n    calendar_list = service.calendarList().list().execute()\n    return calendar_list.get('items', []) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-google-calendar/utils/google_calendar.py`:**\n\n1. **Primary Purpose:**  \n   This code unit provides utility functions to interact with the Google Calendar API. It handles authentication (OAuth2), allows creating events, listing events, creating custom calendars, and listing all accessible calendars for the authenticated user.\n\n2. **Brief Description of Parameters:**\n   - `create_event(summary, description, start_time, end_time, timezone=TIMEZONE)`:  \n     - `summary`: Title of the calendar event.\n     - `description`: Details or body of the event.\n     - `start_time`: Start time as a `datetime` object.\n     - `end_time`: End time as a `datetime` object.\n     - `timezone`: (optional) Time zone string; defaults to environment variable `TIMEZONE`.\n   - `list_events(days=7)`:  \n     - `days`: Number of days ahead from now to fetch events for (default is 7).\n   - `create_custom_calendar(calendar_name, description=\"\")`:  \n     - `calendar_name`: Name of the new calendar.\n     - `description`: (optional) Description for the new calendar.\n   - None of the other functions take parameters.\n\n3. **Brief Description of Return Value:**\n   - `get_calendar_service()`: Returns an authenticated Google Calendar API service object.\n   - `create_event(...)`: Returns the details of the created event (as a dictionary).\n   - `list_events(...)`: Returns a list of the retrieved events (list of dictionaries).\n   - `create_custom_calendar(...)`: Returns the details of the created custom calendar (as a dictionary).\n   - `list_calendar_lists()`: Returns a list of all calendar list entries (as dictionaries) for the user.\n\n4. **Other Functions or Methods Called Internally:**\n   - Functions from the [Google API Python Client](https://github.com/googleapis/google-api-python-client):\n     - `build()`\n     - Service methods: `events().insert()`, `events().list()`, `calendars().insert()`, `calendarList().list()`\n   - OAuth handling:\n     - `InstalledAppFlow.from_client_secrets_file()`\n     - `creds.refresh(Request())`\n   - File I/O: opens and reads/writes `token.pickle`\n   - `load_dotenv()` (from `python-dotenv`)\n   - Standard library:  \n     - `os.getenv()`, `os.path.exists()`, `pickle.load()`, `pickle.dump()`\n     - `datetime`, `timedelta` for date/time calculations\n\n**In summary:**  \nThis module is a wrapper around the Google Calendar API providing easy-to-use functions for authentication, event and calendar management, and information retrieval. It leverages Google-supplied libraries and manages tokens and credentials automatically using environment variables and local storage."
    },
    "190": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-parallel-batch-flow/nodes.py",
      "code": "\"\"\"AsyncNode implementations for image processing.\"\"\"\nimport os\nimport asyncio\nfrom PIL import Image, ImageFilter\nimport numpy as np\nfrom pocketflow import AsyncNode\n\nclass LoadImage(AsyncNode):\n    \"\"\"Node that loads an image from file.\"\"\"\n    async def prep_async(self, shared):\n        \"\"\"Get image path from parameters.\"\"\"\n        image_path = self.params[\"image_path\"]\n        print(f\"Loading image: {image_path}\")\n        return image_path\n    \n    async def exec_async(self, image_path):\n        \"\"\"Load image using PIL.\"\"\"\n        # Simulate I/O delay\n        await asyncio.sleep(0.5)\n        return Image.open(image_path)\n    \n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Store image in shared store.\"\"\"\n        shared[\"image\"] = exec_res\n        return \"apply_filter\"\n\nclass ApplyFilter(AsyncNode):\n    \"\"\"Node that applies a filter to an image.\"\"\"\n    async def prep_async(self, shared):\n        \"\"\"Get image and filter type.\"\"\"\n        image = shared[\"image\"]\n        filter_type = self.params[\"filter\"]\n        print(f\"Applying {filter_type} filter...\")\n        return image, filter_type\n    \n    async def exec_async(self, inputs):\n        \"\"\"Apply the specified filter.\"\"\"\n        image, filter_type = inputs\n        \n        # Simulate processing delay\n        await asyncio.sleep(0.5)\n        \n        if filter_type == \"grayscale\":\n            return image.convert(\"L\")\n        elif filter_type == \"blur\":\n            return image.filter(ImageFilter.BLUR)\n        elif filter_type == \"sepia\":\n            # Convert to array for sepia calculation\n            img_array = np.array(image)\n            sepia_matrix = np.array([\n                [0.393, 0.769, 0.189],\n                [0.349, 0.686, 0.168],\n                [0.272, 0.534, 0.131]\n            ])\n            sepia_array = img_array.dot(sepia_matrix.T)\n            sepia_array = np.clip(sepia_array, 0, 255).astype(np.uint8)\n            return Image.fromarray(sepia_array)\n        else:\n            raise ValueError(f\"Unknown filter: {filter_type}\")\n    \n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Store filtered image.\"\"\"\n        shared[\"filtered_image\"] = exec_res\n        return \"save\"\n\nclass SaveImage(AsyncNode):\n    \"\"\"Node that saves the processed image.\"\"\"\n    async def prep_async(self, shared):\n        \"\"\"Prepare output path.\"\"\"\n        image = shared[\"filtered_image\"]\n        base_name = os.path.splitext(os.path.basename(self.params[\"image_path\"]))[0]\n        filter_type = self.params[\"filter\"]\n        output_path = f\"output/{base_name}_{filter_type}.jpg\"\n        \n        # Create output directory if needed\n        os.makedirs(\"output\", exist_ok=True)\n        \n        return image, output_path\n    \n    async def exec_async(self, inputs):\n        \"\"\"Save the image.\"\"\"\n        image, output_path = inputs\n        \n        # Simulate I/O delay\n        await asyncio.sleep(0.5)\n        \n        image.save(output_path)\n        return output_path\n    \n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Print success message.\"\"\"\n        print(f\"Saved: {exec_res}\")\n        return \"default\" ",
      "summary": "**Summary of nodes.py**\n\n1. **Primary Purpose:**  \n   This code defines asynchronous processing nodes for an image transformation pipeline, where each node loads, filters, or saves an image as part of a batch-processing workflow. It enables stepwise, asynchronous image manipulation using the PocketFlow framework.\n\n2. **Parameters:**  \n   - `self.params[\"image_path\"]`: File path to the input image.\n   - `self.params[\"filter\"]`: Name of the filter to apply (\"grayscale\", \"blur\", or \"sepia\").\n   - Other parameters may be passed to `AsyncNode` base class but aren't explicitly listed here.\n\n3. **Return Value:**  \n   - Each node\u2019s `post_async` method returns a string indicating the next workflow node (\"apply_filter\", \"save\", or \"default\").\n   - Intermediate data such as loaded images and filtered images are stored in the shared context (`shared` dictionary).\n\n4. **List of Other Functions or Methods Called Internally:**\n   - `Image.open()` (from PIL): Loads an image from file.\n   - `Image.convert()`: Converts an image to grayscale.\n   - `Image.filter()`: Applies a blur filter.\n   - `Image.fromarray()`: Converts a NumPy array back to a PIL Image.\n   - `os.path.splitext()`, `os.path.basename()`: For path manipulations.\n   - `os.makedirs()`: Ensures the output directory exists.\n   - `Image.save()`: Saves an image to disk.\n   - `asyncio.sleep()`: Simulates I/O or processing delays.\n   - `np.array()`, `np.clip()`, and matrix operations: For sepia filter calculation.\n   - Standard Python dictionary operations for sharing data among nodes.\n\n**In summary:**  \nThe code provides a modular set of asynchronous nodes for loading an image, applying a specified filter, and saving the result, each step managing its own inputs, outputs, and next step in the pipeline, leveraging core PIL, NumPy, OS, and asyncio functionalities."
    },
    "191": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-parallel-batch-flow/flow.py",
      "code": "\"\"\"Flow definitions for parallel image processing.\"\"\"\n\nfrom pocketflow import AsyncParallelBatchFlow, AsyncBatchFlow\nfrom nodes import LoadImage, ApplyFilter, SaveImage\n\ndef create_base_flow():\n    \"\"\"Create flow for processing a single image with one filter.\"\"\"\n    # Create nodes\n    load = LoadImage()\n    apply_filter = ApplyFilter()\n    save = SaveImage()\n    \n    # Connect nodes\n    load - \"apply_filter\" >> apply_filter\n    apply_filter - \"save\" >> save\n    \n    # Create flow\n    return load\n\nclass ImageBatchFlow(AsyncBatchFlow):\n    \"\"\"Flow that processes multiple images with multiple filters in batch.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Generate parameters for each image-filter combination.\"\"\"\n        # Get list of images and filters\n        images = shared.get(\"images\", [])\n        filters = [\"grayscale\", \"blur\", \"sepia\"]\n        \n        # Create parameter combinations\n        params = []\n        for image_path in images:\n            for filter_type in filters:\n                params.append({\n                    \"image_path\": image_path,\n                    \"filter\": filter_type\n                })\n        \n        print(f\"Processing {len(images)} images with {len(filters)} filters...\")\n        print(f\"Total combinations: {len(params)}\")\n        return params\n\nclass ImageParallelBatchFlow(AsyncParallelBatchFlow):\n    \"\"\"Flow that processes multiple images with multiple filters in parallel.\"\"\"\n\n    async def prep_async(self, shared):\n        \"\"\"Generate parameters for each image-filter combination.\"\"\"\n        # Get list of images and filters\n        images = shared.get(\"images\", [])\n        filters = [\"grayscale\", \"blur\", \"sepia\"]\n        \n        # Create parameter combinations\n        params = []\n        for image_path in images:\n            for filter_type in filters:\n                params.append({\n                    \"image_path\": image_path,\n                    \"filter\": filter_type\n                })\n        \n        print(f\"Processing {len(images)} images with {len(filters)} filters...\")\n        print(f\"Total combinations: {len(params)}\")\n        return params\n\ndef create_flows():\n    \"\"\"Create the complete parallel processing flow.\"\"\"\n    # Create base flow for single image processing\n    base_flow = create_base_flow()\n    \n    # Wrap in parallel batch flow\n    return ImageBatchFlow(start=base_flow), ImageParallelBatchFlow(start=base_flow)",
      "summary": "**Summary of flow.py**\n\n1. **Primary Purpose:**  \n   The code defines image processing flows that handle multiple images and apply multiple filters, supporting both batch and parallel batch execution using the PocketFlow framework. It sets up reusable processing pipelines for image loading, filtering, and saving.\n\n2. **Parameters:**  \n   - The main functions and classes do not take direct parameters, except for methods like `prep_async(self, shared)`, which receives a dictionary called `shared` that is expected to contain an \"images\" key (a list of image file paths).\n   - The `create_flows()` function does not accept any parameters.\n\n3. **Return Value:**  \n   - `create_base_flow()`: Returns the starting node (`load`, representing the entry point of a single image processing flow).\n   - `prep_async()`: Returns a list of dictionaries, each containing an \"image_path\" and \"filter\" key, representing all combinations of input images and filters.\n   - `create_flows()`: Returns a tuple containing instances of `ImageBatchFlow` and `ImageParallelBatchFlow`, both initialized to start from the base flow.\n\n4. **Internal Function/Method Calls:**  \n   - Calls to the following node constructors: `LoadImage()`, `ApplyFilter()`, `SaveImage()`\n   - Node connection methods/operators: `- \"apply_filter\" >>` and `- \"save\" >>`\n   - Internal usage of class constructors: `ImageBatchFlow()`, `ImageParallelBatchFlow()`\n   - Dictionary methods: `shared.get()`\n   - Built-in `print()` function\n\n**Overall**, the code sets up modular image processing flows that can batch-process or parallel-process images with multiple filters, supporting scalable and automated image manipulations."
    },
    "192": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-parallel-batch-flow/main.py",
      "code": "import os\nimport asyncio\nimport time\nfrom flow import create_flows\n\ndef get_image_paths():\n    \"\"\"Get paths of existing images in the images directory.\"\"\"\n    images_dir = \"images\"\n    if not os.path.exists(images_dir):\n        raise ValueError(f\"Directory '{images_dir}' not found!\")\n    \n    # List all jpg files in the images directory\n    image_paths = []\n    for filename in os.listdir(images_dir):\n        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n            image_paths.append(os.path.join(images_dir, filename))\n    \n    if not image_paths:\n        raise ValueError(f\"No images found in '{images_dir}' directory!\")\n    \n    print(f\"Found {len(image_paths)} images:\")\n    for path in image_paths:\n        print(f\"- {path}\")\n    \n    return image_paths\n\nasync def main():\n    \"\"\"Run the parallel image processing example.\"\"\"\n    print(\"Parallel Image Processor\")\n    print(\"-\" * 30)\n    \n    # Get existing image paths\n    image_paths = get_image_paths()\n    \n    # Create shared store with image paths\n    shared = {\"images\": image_paths}\n    \n    # Create both flows\n    batch_flow, parallel_batch_flow = create_flows()\n    \n    # Run and time batch flow\n    start_time = time.time()\n    print(\"\\nRunning sequential batch flow...\")\n    await batch_flow.run_async(shared)\n    batch_time = time.time() - start_time\n    \n    # Run and time parallel batch flow\n    start_time = time.time()\n    print(\"\\nRunning parallel batch flow...\")\n    await parallel_batch_flow.run_async(shared)\n    parallel_time = time.time() - start_time\n    \n    # Print timing results\n    print(\"\\nTiming Results:\")\n    print(f\"Sequential batch processing: {batch_time:.2f} seconds\")\n    print(f\"Parallel batch processing: {parallel_time:.2f} seconds\")\n    print(f\"Speedup: {batch_time/parallel_time:.2f}x\")\n    \n    print(\"\\nProcessing complete! Check the output/ directory for results.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main()) ",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-parallel-batch-flow/main.py`:**\n\n1. **Primary Purpose:**  \n   This code serves as the main entry point for a parallel image processing demonstration. It loads image paths from a directory, then benchmarks the sequential (batch) and parallel batch processing flows, comparing their performance on the same set of images.\n\n2. **Parameters:**  \n   - The main functions in this script do **not accept external parameters**. The only function with arguments is `get_image_paths()`, which takes no parameters.\n   - The image processing itself is based on images found in the `images` directory, and uses flows created via the `create_flows()` method.\n\n3. **Return Value:**  \n   - `get_image_paths()` returns a list of image file paths.\n   - The main async function `main()` does not return a value (its return is `None`).  \n   - The script, when run as a module, has no return value\u2014it performs side effects (console output, running flows).\n\n4. **Internally Called Functions/Methods:**  \n   - `os.path.exists()`, `os.listdir()`, `os.path.join()` (from the `os` module)\n   - `time.time()` (from the `time` module for timing)\n   - `asyncio.run()` (for running the async loop)\n   - `get_image_paths()` (defined within this file)\n   - `create_flows()` (imported from the `flow` module)\n   - `batch_flow.run_async(shared)` and `parallel_batch_flow.run_async(shared)` (methods on flow objects created by `create_flows()`)\n\n**Summary:**  \nIn short, this script loads all images from an `images` directory, creates two processing flows (sequential and parallel), runs both, measures the elapsed times, prints out the speedup achieved by parallel processing, and guides the user to check output files. The heavy lifting is delegated to the imported `flow` module and its asynchronous processing routines."
    },
    "193": {
      "unit_name": "simple_server.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-mcp/simple_server.py",
      "code": "from fastmcp import FastMCP\n\n# Create a named server\nmcp = FastMCP(\"Math Operations Server\")\n\n# Define mathematical operation tools\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers together\"\"\"\n    return a + b\n\n@mcp.tool()\ndef subtract(a: int, b: int) -> int:\n    \"\"\"Subtract b from a\"\"\"\n    return a - b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers together\"\"\"\n    return a * b\n\n@mcp.tool()\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide a by b\"\"\"\n    if b == 0:\n        raise ValueError(\"Division by zero is not allowed\")\n    return a / b\n\n# Start the server\nif __name__ == \"__main__\":\n    mcp.run()",
      "summary": "**Summary of 'simple_server.py':**\n\n1. **Primary purpose:**  \n   The code defines and runs a simple server for basic mathematical operations (addition, subtraction, multiplication, division) using the FastMCP framework. This server exposes these operations as API-accessible tools.\n\n2. **Parameters:**  \n   - Each tool function takes two parameters:  \n     - `a` (int): The first operand.\n     - `b` (int): The second operand.\n\n3. **Return value:**  \n   - Each function returns the result of the respective operation:  \n     - `add`, `subtract`, and `multiply` return an integer.\n     - `divide` returns a float (quotient of a divided by b).\n\n4. **Other functions or methods called internally:**  \n   - `FastMCP()` constructor to create the server instance.\n   - `@mcp.tool()` decorator to register function as server tools.\n   - Each tool uses Python's arithmetic operators and, for division, the `ValueError` exception is raised if dividing by zero.\n   - `mcp.run()` to start the server if run as a script."
    },
    "194": {
      "unit_name": "main.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-mcp/main.py",
      "code": "from pocketflow import Node, Flow\nfrom utils import call_llm, get_tools, call_tool\nimport yaml\nimport sys\n\nclass GetToolsNode(Node):\n    def prep(self, shared):\n        \"\"\"Initialize and get tools\"\"\"\n        # The question is now passed from main via shared\n        print(\"\ud83d\udd0d Getting available tools...\")\n        return \"simple_server.py\"\n\n    def exec(self, server_path):\n        \"\"\"Retrieve tools from the MCP server\"\"\"\n        tools = get_tools(server_path)\n        return tools\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store tools and process to decision node\"\"\"\n        tools = exec_res\n        shared[\"tools\"] = tools\n        \n        # Format tool information for later use\n        tool_info = []\n        for i, tool in enumerate(tools, 1):\n            properties = tool.inputSchema.get('properties', {})\n            required = tool.inputSchema.get('required', [])\n            \n            params = []\n            for param_name, param_info in properties.items():\n                param_type = param_info.get('type', 'unknown')\n                req_status = \"(Required)\" if param_name in required else \"(Optional)\"\n                params.append(f\"    - {param_name} ({param_type}): {req_status}\")\n            \n            tool_info.append(f\"[{i}] {tool.name}\\n  Description: {tool.description}\\n  Parameters:\\n\" + \"\\n\".join(params))\n        \n        shared[\"tool_info\"] = \"\\n\".join(tool_info)\n        return \"decide\"\n\nclass DecideToolNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the prompt for LLM to process the question\"\"\"\n        tool_info = shared[\"tool_info\"]\n        question = shared[\"question\"]\n        \n        prompt = f\"\"\"\n### CONTEXT\nYou are an assistant that can use tools via Model Context Protocol (MCP).\n\n### ACTION SPACE\n{tool_info}\n\n### TASK\nAnswer this question: \"{question}\"\n\n## NEXT ACTION\nAnalyze the question, extract any numbers or parameters, and decide which tool to use.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning about what the question is asking and what numbers to extract>\ntool: <name of the tool to use>\nreason: <why you chose this tool>\nparameters:\n    <parameter_name>: <parameter_value>\n    <parameter_name>: <parameter_value>\n```\nIMPORTANT: \n1. Extract numbers from the question properly\n2. Use proper indentation (4 spaces) for multi-line fields\n3. Use the | character for multi-line text fields\n\"\"\"\n        return prompt\n\n    def exec(self, prompt):\n        \"\"\"Call LLM to process the question and decide which tool to use\"\"\"\n        print(\"\ud83e\udd14 Analyzing question and deciding which tool to use...\")\n        response = call_llm(prompt)\n        return response\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Extract decision from YAML and save to shared context\"\"\"\n        try:\n            yaml_str = exec_res.split(\"```yaml\")[1].split(\"```\")[0].strip()\n            decision = yaml.safe_load(yaml_str)\n            \n            shared[\"tool_name\"] = decision[\"tool\"]\n            shared[\"parameters\"] = decision[\"parameters\"]\n            shared[\"thinking\"] = decision.get(\"thinking\", \"\")\n            \n            print(f\"\ud83d\udca1 Selected tool: {decision['tool']}\")\n            print(f\"\ud83d\udd22 Extracted parameters: {decision['parameters']}\")\n            \n            return \"execute\"\n        except Exception as e:\n            print(f\"\u274c Error parsing LLM response: {e}\")\n            print(\"Raw response:\", exec_res)\n            return None\n\nclass ExecuteToolNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare tool execution parameters\"\"\"\n        return shared[\"tool_name\"], shared[\"parameters\"]\n\n    def exec(self, inputs):\n        \"\"\"Execute the chosen tool\"\"\"\n        tool_name, parameters = inputs\n        print(f\"\ud83d\udd27 Executing tool '{tool_name}' with parameters: {parameters}\")\n        result = call_tool(\"simple_server.py\", tool_name, parameters)\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        print(f\"\\n\u2705 Final Answer: {exec_res}\")\n        return \"done\"\n\n\nif __name__ == \"__main__\":\n    # Default question\n    default_question = \"What is 982713504867129384651 plus 73916582047365810293746529?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    print(f\"\ud83e\udd14 Processing question: {question}\")\n    \n    # Create nodes\n    get_tools_node = GetToolsNode()\n    decide_node = DecideToolNode()\n    execute_node = ExecuteToolNode()\n    \n    # Connect nodes\n    get_tools_node - \"decide\" >> decide_node\n    decide_node - \"execute\" >> execute_node\n    \n    # Create and run flow\n    flow = Flow(start=get_tools_node)\n    shared = {\"question\": question}\n    flow.run(shared)",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-mcp/main.py`:**\n\n1. **Primary Purpose:**  \n   This code defines a workflow that receives a user question, determines which available tool is best suited to answer it using an LLM (Large Language Model), and automatically executes the selected tool with the extracted parameters to produce an answer. It demonstrates an automated agent loop using PocketFlow nodes, integrating tool discovery, LLM reasoning, and tool execution.\n\n2. **Parameters:**  \n   - The main workflow accepts a user question, which can be passed via the command line (as an argument prefixed by `--`). If no argument is provided, a default math question is used.\n   - The workflow also expects access to a tool server script (hardcoded as `\"simple_server.py\"`).\n\n3. **Return Value:**  \n   - There is no explicit return value from the main script; instead, the result is printed to the console (\"\u2705 Final Answer: ...\").  \n   - The workflow stores intermediate state information (such as available tools, LLM analysis, and parameters) in a shared dictionary.\n\n4. **Functions and Methods Called Internally:**  \n   - **Imported Functions:**\n     - `call_llm(prompt)`: Sends a prompt to the LLM and gets back its response.\n     - `get_tools(server_path)`: Connects to the tool server to retrieve a list of available tool objects.\n     - `call_tool(server, tool_name, parameters)`: Invokes the selected tool on the server with given parameters.\n   - **PocketFlow Node Methods:**  \n     - `prep(self, shared)`\n     - `exec(self, ...)`\n     - `post(self, shared, prep_res, exec_res)`\n   - **YAML Handling:**\n     - `yaml.safe_load()`: To parse YAML-formatted LLM responses.\n\n**Overall,** the script acts as an intelligent agent pipeline, automating the process of selecting and invoking tools to answer complex questions, leveraging both structured tool discovery and LLM-based reasoning."
    },
    "195": {
      "unit_name": "utils.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-mcp/utils.py",
      "code": "from openai import OpenAI\nimport os\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Global flag to control whether to use MCP or local implementation\nMCP = False\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef get_tools(server_script_path=None):\n    \"\"\"Get available tools, either from MCP server or locally based on MCP global setting.\"\"\"\n    if MCP:\n        return mcp_get_tools(server_script_path)\n    else:\n        return local_get_tools(server_script_path)\n    \ndef mcp_get_tools(server_script_path):\n    \"\"\"Get available tools from an MCP server.\n    \"\"\"\n    async def _get_tools():\n        server_params = StdioServerParameters(\n            command=\"python\",\n            args=[server_script_path]\n        )\n        \n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n                tools_response = await session.list_tools()\n                return tools_response.tools\n    \n    return asyncio.run(_get_tools())\n\ndef local_get_tools(server_script_path=None):\n    \"\"\"A simple dummy implementation of get_tools without MCP.\"\"\"\n    tools = [\n        {\n            \"name\": \"add\",\n            \"description\": \"Add two numbers together\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        },\n        {\n            \"name\": \"subtract\",\n            \"description\": \"Subtract b from a\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        },\n        {\n            \"name\": \"multiply\",\n            \"description\": \"Multiply two numbers together\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        },\n        {\n            \"name\": \"divide\",\n            \"description\": \"Divide a by b\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        }\n    ]\n\n    class DictObject(dict):\n        \"\"\"A simple class that behaves both as a dictionary and as an object with attributes.\"\"\"\n        def __init__(self, data):\n            super().__init__(data)\n            for key, value in data.items():\n                if isinstance(value, dict):\n                    self[key] = DictObject(value)\n                elif isinstance(value, list) and value and isinstance(value[0], dict):\n                    self[key] = [DictObject(item) for item in value]\n        \n        def __getattr__(self, key):\n            try:\n                return self[key]\n            except KeyError:\n                raise AttributeError(f\"'DictObject' object has no attribute '{key}'\")\n\n    return [DictObject(tool) for tool in tools]\n\ndef call_tool(server_script_path=None, tool_name=None, arguments=None):\n    \"\"\"Call a tool, either from MCP server or locally based on MCP global setting.\"\"\"\n    if MCP:\n        return mcp_call_tool(server_script_path, tool_name, arguments)\n    else:\n        return local_call_tool(server_script_path, tool_name, arguments)\n    \ndef mcp_call_tool(server_script_path=None, tool_name=None, arguments=None):\n    \"\"\"Call a tool on an MCP server.\n    \"\"\"\n    async def _call_tool():\n        server_params = StdioServerParameters(\n            command=\"python\",\n            args=[server_script_path]\n        )\n        \n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n                result = await session.call_tool(tool_name, arguments)\n                return result.content[0].text\n    \n    return asyncio.run(_call_tool())\n\ndef local_call_tool(server_script_path=None, tool_name=None, arguments=None):\n    \"\"\"A simple dummy implementation of call_tool without MCP.\"\"\"\n    # Simple implementation of tools\n    if tool_name == \"add\":\n        if \"a\" in arguments and \"b\" in arguments:\n            return arguments[\"a\"] + arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    elif tool_name == \"subtract\":\n        if \"a\" in arguments and \"b\" in arguments:\n            return arguments[\"a\"] - arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    elif tool_name == \"multiply\":\n        if \"a\" in arguments and \"b\" in arguments:\n            return arguments[\"a\"] * arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    elif tool_name == \"divide\":\n        if \"a\" in arguments and \"b\" in arguments:\n            if arguments[\"b\"] == 0:\n                return \"Error: Division by zero is not allowed\"\n            return arguments[\"a\"] / arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    else:\n        return f\"Error: Unknown tool '{tool_name}'\"\n\nif __name__ == \"__main__\":\n    print(\"=== Testing call_llm ===\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"Response: {response}\")\n\n        # Find available tools\n    print(\"=== Finding available tools ===\")\n    tools = get_tools(\"simple_server.py\")\n    \n    # Print tool information nicely formatted\n    for i, tool in enumerate(tools, 1):\n        print(f\"\\nTool {i}: {tool.name}\")\n        print(\"=\" * (len(tool.name) + 8))\n        print(f\"Description: {tool.description}\")\n        \n        # Parameters section\n        print(\"Parameters:\")\n        properties = tool.inputSchema.get('properties', {})\n        required = tool.inputSchema.get('required', [])\n        \n        # No parameters case\n        if not properties:\n            print(\"  None\")\n        \n        # Print each parameter with its details\n        for param_name, param_info in properties.items():\n            param_type = param_info.get('type', 'unknown')\n            req_status = \"(Required)\" if param_name in required else \"(Optional)\"\n            print(f\"  \u2022 {param_name}: {param_type} {req_status}\")\n    \n    # Call a tool\n    print(\"\\n=== Calling the add tool ===\")\n    a, b = 5, 3\n    result = call_tool(\"simple_server.py\", \"add\", {\"a\": a, \"b\": b})\n    print(f\"Result of {a} + {b} = {result}\")\n    \n    # You can easily call with different parameters\n    a, b = 10, 20\n    result = call_tool(\"simple_server.py\", \"add\", {\"a\": a, \"b\": b})\n    print(f\"Result of {a} + {b} = {result}\")\n\n",
      "summary": "**Summary of `utils.py`**\n\n1. **Primary Purpose:**  \n   The code provides utility functions to interact with either a local set of mathematical tools or an external MCP (Machine Control Protocol) server for tool management and invocation. It also allows interaction with the OpenAI GPT model through an LLM API call. The module is designed to support both local (mock) and remote (MCP server-based) tool execution for use cases such as testing or distributed computation.\n\n2. **Brief Description of Parameters:**\n\n   - **call_llm(prompt):**  \n     - `prompt` (str): The user prompt/question to send to the LLM (GPT).\n     \n   - **get_tools(server_script_path=None):**  \n     - `server_script_path` (str, optional): Path to the server script for MCP server.\n     \n   - **mcp_get_tools(server_script_path):**  \n     - `server_script_path` (str): Path to the server script to run the MCP server.\n     \n   - **local_get_tools(server_script_path=None):**  \n     - `server_script_path` (not used): Present for API symmetry.\n     \n   - **call_tool(server_script_path=None, tool_name=None, arguments=None):**  \n     - `server_script_path` (str, optional): Path to server script for MCP server (ignored locally).\n     - `tool_name` (str): The name of the tool to call (e.g., \"add\", \"subtract\").\n     - `arguments` (dict): A dictionary of arguments for the tool (keys depend on the tool, e.g., `{\"a\": 5, \"b\": 3}`).\n   \n   - **mcp_call_tool(server_script_path, tool_name, arguments):**  \n     - As above.\n   \n   - **local_call_tool(server_script_path, tool_name, arguments):**  \n     - As above.\n\n3. **Brief Description of Return Value:**\n\n   - **call_llm:**  \n     - (str): The LLM's response text to the prompt.\n   \n   - **get_tools / mcp_get_tools / local_get_tools:**  \n     - (list): A list of tool objects, each describing a tool's name, description, and input schema.\n   \n   - **call_tool / mcp_call_tool / local_call_tool:**  \n     - (varies): The result of calling the specified tool:\n         - For local tools: the numerical output (e.g., sum or difference) or error message (str).\n         - For MCP tools: the text result returned by the tool (str).\n   \n4. **List of Other Functions or Methods Called Internally:**\n\n   - **call_llm:**  \n     - `OpenAI`\n     - `client.chat.completions.create()`\n   \n   - **get_tools:**  \n     - `mcp_get_tools` (if MCP enabled)\n     - `local_get_tools` (if MCP not enabled)\n   \n   - **mcp_get_tools:**  \n     - `StdioServerParameters`\n     - `stdio_client`\n     - `ClientSession` (methods: `initialize()`, `list_tools()`)\n     - `asyncio.run()`\n   \n   - **local_get_tools:**  \n     - `DictObject` (nested class)\n   \n   - **call_tool:**  \n     - `mcp_call_tool` (if MCP enabled)\n     - `local_call_tool` (if MCP not enabled)\n   \n   - **mcp_call_tool:**  \n     - `StdioServerParameters`\n     - `stdio_client`\n     - `ClientSession` (methods: `initialize()`, `call_tool()`)\n     - `asyncio.run()`\n   \n   - **local_call_tool:**  \n     - (internal logic \u2013 no external function calls)\n\n**In summary:**  \nThis file offers a flexible interface for calling LLMs and simple math tools either locally or via a remote MCP server, switching modes via the global `MCP` flag. Functions accept parameters related to tool names, arguments, and script paths, returning responses from LLMs or tool execution results. It leverages OpenAI's API, asyncio, and an MCP client library."
    },
    "196": {
      "unit_name": "nodes.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-hitl/nodes.py",
      "code": "from pocketflow import Node, AsyncNode\nfrom utils.process_task import process_task\n\nclass ProcessNode(Node):\n    def prep(self, shared):\n        task_input = shared.get(\"task_input\", \"No input\")\n        print(\"ProcessNode Prep\")\n        return task_input\n\n    def exec(self, prep_res):\n        return process_task(prep_res)\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"processed_output\"] = exec_res\n        print(\"ProcessNode Post: Output stored.\")\n        return \"default\" # Go to ReviewNode\n\nclass ReviewNode(AsyncNode):\n    async def prep_async(self, shared):\n        review_event = shared.get(\"review_event\")\n        queue = shared.get(\"sse_queue\") # Expect queue in shared\n        processed_output = shared.get(\"processed_output\", \"N/A\")\n\n        if not review_event or not queue:\n            print(\"ERROR: ReviewNode Prep - Missing review_event or sse_queue in shared store!\")\n            return None # Signal failure\n\n        # Push status update to SSE queue\n        status_update = {\n            \"status\": \"waiting_for_review\",\n            \"output_to_review\": processed_output\n        }\n        await queue.put(status_update)\n        print(\"ReviewNode Prep: Put 'waiting_for_review' on SSE queue.\")\n\n        return review_event # Return event for exec_async\n\n    async def exec_async(self, prep_res):\n        review_event = prep_res\n        if not review_event:\n            print(\"ReviewNode Exec: Skipping wait (no event from prep).\")\n            return\n        print(\"ReviewNode Exec: Waiting on review_event...\")\n        await review_event.wait()\n        print(\"ReviewNode Exec: review_event set.\")\n\n    async def post_async(self, shared, prep_res, exec_res):\n        feedback = shared.get(\"feedback\")\n        print(f\"ReviewNode Post: Processing feedback '{feedback}'\")\n\n        # Clear the event for potential loops\n        review_event = shared.get(\"review_event\")\n        if review_event:\n            review_event.clear()\n        shared[\"feedback\"] = None # Reset feedback\n\n        if feedback == \"approved\":\n            shared[\"final_result\"] = shared.get(\"processed_output\")\n            print(\"ReviewNode Post: Action=approved\")\n            return \"approved\"\n        else:\n            print(\"ReviewNode Post: Action=rejected\")\n            return \"rejected\"\n\nclass ResultNode(Node):\n     def prep(self, shared):\n         print(\"ResultNode Prep\")\n         return shared.get(\"final_result\", \"No final result.\")\n\n     def exec(self, prep_res):\n         print(f\"--- FINAL RESULT ---\")\n         print(prep_res)\n         print(f\"--------------------\")\n         return prep_res\n\n     def post(self, shared, prep_res, exec_res):\n         print(\"ResultNode Post: Flow finished.\")\n         return None # End flow",
      "summary": "**Summary of nodes.py**\n\n1. **Primary Purpose**  \nThis code unit defines nodes for a human-in-the-loop task processing workflow in the PocketFlow framework, designed to process input, allow asynchronous human review (using SSE queues and event signaling), and handle final outputs. It facilitates a multi-step process: automated processing, user (human) review, and final result presentation.\n\n2. **Parameters**  \nThe primary classes (`ProcessNode`, `ReviewNode`, and `ResultNode`) define methods that use these parameters:\n- `shared`: a dictionary passed between nodes, holding the workflow state and data such as task input, output, events, feedback, and queues.\n- `prep_res`, `exec_res`: intermediate results passed between method stages (prep \u2192 exec \u2192 post).\n- `review_event`, `sse_queue`, and `feedback` (within `shared`): handles for review synchronization and user feedback.\n\n3. **Return Values**  \nEach node class's methods return:\n- `prep`/`prep_async`: data needed for the node\u2019s exec/execution function (e.g., input strings, review events).\n- `exec`/`exec_async`: result of task execution (processed data or waits for review).\n- `post`/`post_async`: a string indicating the next node (\"default\", \"approved\", \"rejected\"), or `None` to end the flow.\n\n4. **Internal Function/Method Calls**  \n- `process_task` (from `utils.process_task`): called by `ProcessNode.exec` to process the input.\n- Async queue\u2019s `put` method (`queue.put`): in `ReviewNode.prep_async` for server-sent event (SSE) updates.\n- `review_event.wait()` and `review_event.clear()`: in `ReviewNode.exec_async` and `ReviewNode.post_async` for event-driven coordination.\n- Standard Python `print`: for logging throughout.\n\n---\n\n**In summary:**  \nThe code structures a stepwise, pluggable workflow for preprocessing, async human review, and result management, using state stored in a shared dictionary and synchronization via async events and queues."
    },
    "197": {
      "unit_name": "flow.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-hitl/flow.py",
      "code": "from pocketflow import AsyncFlow\nfrom nodes import ProcessNode, ReviewNode, ResultNode\n\ndef create_feedback_flow():\n    \"\"\"Creates the minimal feedback workflow.\"\"\"\n    process_node = ProcessNode()\n    review_node = ReviewNode()\n    result_node = ResultNode()\n\n    # Define transitions\n    process_node >> review_node\n    review_node - \"approved\" >> result_node\n    review_node - \"rejected\" >> process_node # Loop back\n\n    # Create the AsyncFlow\n    flow = AsyncFlow(start=process_node)\n    print(\"Minimal feedback flow created.\")\n    return flow",
      "summary": "**Summary of `flow.py` Code Unit:**\n\n1. **Primary Purpose:**  \n   The code defines and creates a minimal feedback workflow using a flow-based architecture. It sets up a sequence of processing, review, and result stages, with the ability to loop back for further processing if a review is rejected.\n\n2. **Parameters:**  \n   The main function `create_feedback_flow()` does not take any parameters.\n\n3. **Return Value:**  \n   The function returns an instance of `AsyncFlow` that encapsulates the defined workflow, starting with the processing node.\n\n4. **Internally Called Functions/Methods:**  \n   - `ProcessNode()` (constructor)\n   - `ReviewNode()` (constructor)\n   - `ResultNode()` (constructor)\n   - The transition operators:  \n     - `process_node >> review_node` (custom operator/functionality for node connection)  \n     - `review_node - \"approved\" >> result_node` (custom transition for labeled outcome)  \n     - `review_node - \"rejected\" >> process_node` (loop on rejection)\n   - `AsyncFlow(start=process_node)` (constructor)\n   - `print()` (built-in function)\n\n**In summary:** This code builds a simple, cyclical feedback workflow with defined transitions and returns the assembled workflow as an `AsyncFlow` object."
    },
    "198": {
      "unit_name": "server.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-hitl/server.py",
      "code": "import asyncio\nimport uuid\nimport json\nimport os\nfrom fastapi import FastAPI, Request, HTTPException, status, BackgroundTasks # Import BackgroundTasks\nfrom fastapi.responses import HTMLResponse, StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom pydantic import BaseModel, Field # Import Pydantic for request/response models\nfrom typing import Dict, Any, Literal # For type hinting\n\nfrom flow import create_feedback_flow # PocketFlow imports\n\n# --- Configuration ---\napp = FastAPI(title=\"Minimal Feedback Loop API\")\n\nstatic_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'static'))\nif os.path.isdir(static_dir):\n    app.mount(\"/static\", StaticFiles(directory=static_dir), name=\"static\")\nelse:\n    print(f\"Warning: Static directory '{static_dir}' not found.\")\n\ntemplate_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'templates'))\nif os.path.isdir(template_dir):\n    templates = Jinja2Templates(directory=template_dir)\nelse:\n    print(f\"Warning: Template directory '{template_dir}' not found.\")\n    templates = None\n\n# --- State Management (In-Memory - NOT FOR PRODUCTION) ---\n# Global dictionary to store task state. In production, use Redis, DB, etc.\ntasks: Dict[str, Dict[str, Any]] = {}\n# Structure: task_id -> {\"shared\": dict, \"status\": str, \"task_obj\": asyncio.Task | None}\n\n\n# --- Background Flow Runner ---\n# This function remains mostly the same, as it defines the work to be done.\n# It will be scheduled by FastAPI's BackgroundTasks now.\nasync def run_flow_background(task_id: str, flow, shared: Dict[str, Any]):\n    \"\"\"Runs the flow in background, uses queue in shared for SSE.\"\"\"\n    # Check if task exists (might have been cancelled/deleted)\n    if task_id not in tasks:\n        print(f\"Background task {task_id}: Task not found, aborting.\")\n        return\n    queue = shared.get(\"sse_queue\")\n    if not queue:\n        print(f\"ERROR: Task {task_id} missing sse_queue in shared store!\")\n        tasks[task_id][\"status\"] = \"failed\"\n        # Cannot report failure via SSE if queue is missing\n        return\n\n    tasks[task_id][\"status\"] = \"running\"\n    await queue.put({\"status\": \"running\"})\n    print(f\"Task {task_id}: Background flow starting.\")\n\n    final_status = \"unknown\"\n    error_message = None\n    try:\n        # Execute the potentially long-running PocketFlow\n        await flow.run_async(shared)\n\n        # Determine final status based on shared state after flow completion\n        if shared.get(\"final_result\") is not None:\n            final_status = \"completed\"\n        else:\n            # If flow ends without setting final_result\n            final_status = \"finished_incomplete\"\n        print(f\"Task {task_id}: Flow finished with status: {final_status}\")\n\n    except Exception as e:\n        final_status = \"failed\"\n        error_message = str(e)\n        print(f\"Task {task_id}: Flow execution failed: {e}\")\n        # Consider logging traceback here in production\n    finally:\n        # Ensure task still exists before updating state\n        if task_id in tasks:\n            tasks[task_id][\"status\"] = final_status\n            final_update = {\"status\": final_status}\n            if final_status == \"completed\":\n                final_update[\"final_result\"] = shared.get(\"final_result\")\n            elif error_message:\n                final_update[\"error\"] = error_message\n            # Put final status update onto the queue\n            await queue.put(final_update)\n\n        # Signal the end of the SSE stream by putting None\n        # Must happen regardless of whether task was deleted mid-run\n        if queue:\n           await queue.put(None)\n        print(f\"Task {task_id}: Background task ended. Final update sentinel put on queue.\")\n        # Remove the reference to the completed/failed asyncio Task object\n        if task_id in tasks:\n            tasks[task_id][\"task_obj\"] = None\n\n# --- Pydantic Models for Request/Response Validation ---\nclass SubmitRequest(BaseModel):\n    data: str = Field(..., min_length=1, description=\"Input data for the task\")\n\nclass SubmitResponse(BaseModel):\n    message: str = \"Task submitted\"\n    task_id: str\n\nclass FeedbackRequest(BaseModel):\n    feedback: Literal[\"approved\", \"rejected\"] # Use Literal for specific choices\n\nclass FeedbackResponse(BaseModel):\n    message: str\n\n# --- FastAPI Routes ---\n@app.get(\"/\", response_class=HTMLResponse, include_in_schema=False)\nasync def get_index(request: Request):\n    \"\"\"Serves the main HTML frontend.\"\"\"\n    if templates is None:\n        raise HTTPException(status_code=500, detail=\"Templates directory not configured.\")\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/submit\", response_model=SubmitResponse, status_code=status.HTTP_202_ACCEPTED)\nasync def submit_task(\n    submit_request: SubmitRequest, # Use Pydantic model for validation\n    background_tasks: BackgroundTasks # Inject BackgroundTasks instance\n):\n    \"\"\"\n    Submits a new task. The actual processing runs in the background.\n    Returns immediately with the task ID.\n    \"\"\"\n    task_id = str(uuid.uuid4())\n    feedback_event = asyncio.Event()\n    status_queue = asyncio.Queue()\n\n    shared = {\n        \"task_input\": submit_request.data,\n        \"processed_output\": None,\n        \"feedback\": None,\n        \"review_event\": feedback_event,\n        \"sse_queue\": status_queue,\n        \"final_result\": None,\n        \"task_id\": task_id\n    }\n\n    flow = create_feedback_flow()\n\n    # Store task state BEFORE scheduling background task\n    tasks[task_id] = {\n        \"shared\": shared,\n        \"status\": \"pending\",\n        \"task_obj\": None # Placeholder for the asyncio Task created by BackgroundTasks\n    }\n\n    await status_queue.put({\"status\": \"pending\", \"task_id\": task_id})\n\n    # Schedule the flow execution using FastAPI's BackgroundTasks\n    # This runs AFTER the response has been sent\n    background_tasks.add_task(run_flow_background, task_id, flow, shared)\n    # Note: We don't get a direct reference to the asyncio Task object this way,\n    # which is fine for this minimal example. If cancellation were needed,\n    # managing asyncio.create_task manually would be necessary.\n\n    print(f\"Task {task_id}: Submitted, scheduled for background execution.\")\n    return SubmitResponse(task_id=task_id)\n\n\n@app.post(\"/feedback/{task_id}\", response_model=FeedbackResponse)\nasync def provide_feedback(task_id: str, feedback_request: FeedbackRequest):\n    \"\"\"Provides feedback (approved/rejected) to potentially unblock a waiting task.\"\"\"\n    if task_id not in tasks:\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"Task not found\")\n\n    task_info = tasks[task_id]\n    shared = task_info[\"shared\"]\n    queue = shared.get(\"sse_queue\")\n    review_event = shared.get(\"review_event\")\n\n    async def report_error(message, status_code=status.HTTP_400_BAD_REQUEST):\n        # Helper to log, put status on queue, and raise HTTP exception\n        print(f\"Task {task_id}: Feedback error - {message}\")\n        if queue: await queue.put({\"status\": \"feedback_error\", \"error\": message})\n        raise HTTPException(status_code=status_code, detail=message)\n\n    if not review_event:\n        # This indicates an internal setup error if the task exists but has no event\n        await report_error(\"Task not configured for feedback\", status.HTTP_500_INTERNAL_SERVER_ERROR)\n    if review_event.is_set():\n        # Prevent processing feedback multiple times or if the task isn't waiting\n        await report_error(\"Task not awaiting feedback or feedback already sent\", status.HTTP_409_CONFLICT)\n\n    feedback = feedback_request.feedback # Already validated by Pydantic\n    print(f\"Task {task_id}: Received feedback via POST: {feedback}\")\n\n    # Update status *before* setting the event, so client sees 'processing' first\n    if queue: await queue.put({\"status\": \"processing_feedback\", \"feedback_value\": feedback})\n    tasks[task_id][\"status\"] = \"processing_feedback\" # Update central status tracker\n\n    # Store feedback and signal the waiting ReviewNode\n    shared[\"feedback\"] = feedback\n    review_event.set()\n\n    return FeedbackResponse(message=f\"Feedback '{feedback}' received\")\n\n\n# --- SSE Endpoint ---\n@app.get(\"/stream/{task_id}\")\nasync def stream_status(task_id: str):\n    \"\"\"Streams status updates for a given task using Server-Sent Events.\"\"\"\n    if task_id not in tasks or \"sse_queue\" not in tasks[task_id][\"shared\"]:\n         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"Task or queue not found\")\n\n    queue = tasks[task_id][\"shared\"][\"sse_queue\"]\n\n    async def event_generator():\n        \"\"\"Yields SSE messages from the task's queue.\"\"\"\n        print(f\"SSE Stream: Client connected for {task_id}\")\n        try:\n            while True:\n                # Wait for the next status update from the queue\n                update = await queue.get()\n                if update is None: # Sentinel value indicates end of stream\n                    print(f\"SSE Stream: Sentinel received for {task_id}, closing stream.\")\n                    yield f\"data: {json.dumps({'status': 'stream_closed'})}\\n\\n\"\n                    break\n\n                sse_data = json.dumps(update)\n                print(f\"SSE Stream: Sending for {task_id}: {sse_data}\")\n                yield f\"data: {sse_data}\\n\\n\" # SSE format: \"data: <json>\\n\\n\"\n                queue.task_done() # Acknowledge processing the queue item\n\n        except asyncio.CancelledError:\n            # This happens if the client disconnects\n            print(f\"SSE Stream: Client disconnected for {task_id}.\")\n        except Exception as e:\n            # Log unexpected errors during streaming\n            print(f\"SSE Stream: Error in generator for {task_id}: {e}\")\n            # Optionally send an error message to the client if possible\n            try:\n                yield f\"data: {json.dumps({'status': 'stream_error', 'error': str(e)})}\\n\\n\"\n            except Exception: # Catch errors if yield fails (e.g., connection already closed)\n                pass\n        finally:\n            print(f\"SSE Stream: Generator finished for {task_id}.\")\n            # Consider cleanup here (e.g., removing task if no longer needed)\n            # if task_id in tasks: del tasks[task_id]\n\n    # Use FastAPI/Starlette's StreamingResponse for SSE\n    headers = {'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}\n    return StreamingResponse(event_generator(), media_type=\"text/event-stream\", headers=headers)\n\n# --- Main Execution Guard (for running with uvicorn) ---\nif __name__ == \"__main__\":\n    print(\"Starting FastAPI server using Uvicorn is recommended:\")\n    print(\"uvicorn server:app --reload --host 0.0.0.0 --port 8000\")\n    # Example using uvicorn programmatically (less common than CLI)\n    # import uvicorn\n    # uvicorn.run(app, host=\"0.0.0.0\", port=8000)",
      "summary": "**Summary of `/tmp/PocketFlow/cookbook/pocketflow-fastapi-hitl/server.py`:**\n\n1. **Primary Purpose:**\n   - This code implements a minimal FastAPI backend server to orchestrate human-in-the-loop (HITL) feedback flows using PocketFlow. It provides endpoints to submit processing tasks, send feedback, and stream real-time status updates to clients via Server-Sent Events (SSE). The application manages asynchronous task execution and communicates progress and results back to frontend clients, supporting a review/feedback step within the main flow.\n\n2. **Brief Description of Parameters:**\n   - Most routes are FastAPI endpoints using Pydantic models and URL parameters:\n     - `/submit`: Accepts a JSON payload (SubmitRequest) with a `data` string to submit a new processing task.\n     - `/feedback/{task_id}`: Accepts a `task_id` in the URL path and a JSON payload (FeedbackRequest) with a `feedback` value (\u201capproved\u201d or \u201crejected\u201d).\n     - `/stream/{task_id}`: Accepts a `task_id` as a URL path parameter to stream status events for a specific task.\n\n3. **Brief Description of Return Values:**\n   - `/submit`: Returns JSON (SubmitResponse) with a task ID and a message confirming submission.\n   - `/feedback/{task_id}`: Returns JSON (FeedbackResponse) confirming receipt of the feedback.\n   - `/stream/{task_id}`: Returns an SSE stream (StreamingResponse) that emits JSON status updates as the task is processed.\n   - `/`: Returns an HTML page (the main frontend), rendered with Jinja2 templates.\n\n4. **List of Other Functions/Methods Called Internally:**\n   - `create_feedback_flow()` (from PocketFlow, user-defined)\n   - `run_flow_background` (defined in this module, called as a background task)\n   - FastAPI/Starlette functions:\n     - `FastAPI`\n     - `BackgroundTasks.add_task`\n     - `StreamingResponse`\n     - `Jinja2Templates.TemplateResponse`\n     - `StaticFiles`\n   - Pydantic model instantiation/validation\n   - Python standard library:\n     - `asyncio.Event`\n     - `asyncio.Queue`\n     - `uuid.uuid4`\n     - `os.path` functions\n     - `json.dumps`\n   - Asynchronous queue and event methods:\n     - `Event.set`\n     - `Queue.put`, `Queue.get`, `Queue.task_done`\n   - Miscellaneous:\n     - `print` for logging\n     - `HTTPException`\n     - Status codes from `fastapi.status`"
    },
    "199": {
      "unit_name": "__init__.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-hitl/utils/__init__.py",
      "code": "",
      "summary": "The provided code from /tmp/PocketFlow/cookbook/pocketflow-fastapi-hitl/utils/__init__.py is empty\u2014there is no code, class, function, or variable definition present in the given unit.\n\nSummary:\n\n1. Primary purpose of the code unit:  \n   As it stands, the __init__.py file is empty. In Python packages, an empty __init__.py is typically used to mark a directory as a package, allowing modules inside it to be imported.\n\n2. Brief description of its parameters:  \n   Not applicable; there are no parameters.\n\n3. Brief description of its return value:  \n   Not applicable; there are no return values.\n\n4. List of other functions or methods it calls internally:  \n   Not applicable; there are no internal function or method calls.\n\nIn summary, the file currently serves only as a namespace initializer for the utils package."
    },
    "200": {
      "unit_name": "process_task.py",
      "file_path": "/tmp/PocketFlow/cookbook/pocketflow-fastapi-hitl/utils/process_task.py",
      "code": "import time\n\ndef process_task(input_data):\n    \"\"\"Minimal simulation of processing the input data.\"\"\"\n    print(f\"Processing: '{input_data[:50]}...'\")\n    \n    # Simulate work\n    time.sleep(2)\n\n    processed_result = f\"Processed: {input_data}\"\n    print(f\"Finished processing.\")\n    return processed_result\n\n# We don't need a separate utils/call_llm.py for this minimal example,\n# but you would add it here if ProcessNode used an LLM.\n\n",
      "summary": "**Summary of `process_task.py`:**\n\n1. **Primary purpose:**  \n   The code provides a minimal example of simulating a task processing function, presumably as a placeholder for more complex logic.\n\n2. **Parameters:**  \n   - `input_data`: A single argument passed into `process_task`, expected to be data (likely a string) that will be \"processed.\"\n\n3. **Return value:**  \n   - Returns a string in the format `Processed: {input_data}`, indicating the provided input has been processed.\n\n4. **Internal function/method calls:**  \n   - `print()`: Called twice to log the start and end of the processing.\n   - `time.sleep(2)`: Called to simulate the task processing time by pausing execution for 2 seconds."
    },
    "201": {
      "unit_name": "test_async_batch_flow.py",
      "file_path": "/tmp/PocketFlow/tests/test_async_batch_flow.py",
      "code": "import unittest\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import AsyncNode, AsyncBatchFlow\n\nclass AsyncDataProcessNode(AsyncNode):\n    async def prep_async(self, shared_storage):\n        key = self.params.get('key')\n        data = shared_storage['input_data'][key]\n        if 'results' not in shared_storage:\n            shared_storage['results'] = {}\n        shared_storage['results'][key] = data\n        return data\n\n    async def post_async(self, shared_storage, prep_result, proc_result):\n        await asyncio.sleep(0.01)  # Simulate async work\n        key = self.params.get('key')\n        shared_storage['results'][key] = prep_result * 2  # Double the value\n        return \"processed\"\n\nclass AsyncErrorNode(AsyncNode):\n    async def post_async(self, shared_storage, prep_result, proc_result):\n        key = self.params.get('key')\n        if key == 'error_key':\n            raise ValueError(f\"Async error processing key: {key}\")\n        return \"processed\"\n\nclass TestAsyncBatchFlow(unittest.TestCase):\n    def setUp(self):\n        self.process_node = AsyncDataProcessNode()\n\n    def test_basic_async_batch_processing(self):\n        \"\"\"Test basic async batch processing with multiple keys\"\"\"\n        class SimpleTestAsyncBatchFlow(AsyncBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        shared_storage = {\n            'input_data': {\n                'a': 1,\n                'b': 2,\n                'c': 3\n            }\n        }\n\n        flow = SimpleTestAsyncBatchFlow(start=self.process_node)\n        asyncio.run(flow.run_async(shared_storage))\n\n        expected_results = {\n            'a': 2,  # 1 * 2\n            'b': 4,  # 2 * 2\n            'c': 6   # 3 * 2\n        }\n        self.assertEqual(shared_storage['results'], expected_results)\n\n    def test_empty_async_batch(self):\n        \"\"\"Test async batch processing with empty input\"\"\"\n        class EmptyTestAsyncBatchFlow(AsyncBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        shared_storage = {\n            'input_data': {}\n        }\n\n        flow = EmptyTestAsyncBatchFlow(start=self.process_node)\n        asyncio.run(flow.run_async(shared_storage))\n\n        self.assertEqual(shared_storage.get('results', {}), {})\n\n    def test_async_error_handling(self):\n        \"\"\"Test error handling during async batch processing\"\"\"\n        class ErrorTestAsyncBatchFlow(AsyncBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        shared_storage = {\n            'input_data': {\n                'normal_key': 1,\n                'error_key': 2,\n                'another_key': 3\n            }\n        }\n\n        flow = ErrorTestAsyncBatchFlow(start=AsyncErrorNode())\n        \n        with self.assertRaises(ValueError):\n            asyncio.run(flow.run_async(shared_storage))\n\n    def test_nested_async_flow(self):\n        \"\"\"Test async batch processing with nested flows\"\"\"\n        class AsyncInnerNode(AsyncNode):\n            async def post_async(self, shared_storage, prep_result, proc_result):\n                key = self.params.get('key')\n                if 'intermediate_results' not in shared_storage:\n                    shared_storage['intermediate_results'] = {}\n                shared_storage['intermediate_results'][key] = shared_storage['input_data'][key] + 1\n                await asyncio.sleep(0.01)\n                return \"next\"\n\n        class AsyncOuterNode(AsyncNode):\n            async def post_async(self, shared_storage, prep_result, proc_result):\n                key = self.params.get('key')\n                if 'results' not in shared_storage:\n                    shared_storage['results'] = {}\n                shared_storage['results'][key] = shared_storage['intermediate_results'][key] * 2\n                await asyncio.sleep(0.01)\n                return \"done\"\n\n        class NestedAsyncBatchFlow(AsyncBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        # Create inner flow\n        inner_node = AsyncInnerNode()\n        outer_node = AsyncOuterNode()\n        inner_node - \"next\" >> outer_node\n\n        shared_storage = {\n            'input_data': {\n                'x': 1,\n                'y': 2\n            }\n        }\n\n        flow = NestedAsyncBatchFlow(start=inner_node)\n        asyncio.run(flow.run_async(shared_storage))\n\n        expected_results = {\n            'x': 4,  # (1 + 1) * 2\n            'y': 6   # (2 + 1) * 2\n        }\n        self.assertEqual(shared_storage['results'], expected_results)\n\n    def test_custom_async_parameters(self):\n        \"\"\"Test async batch processing with additional custom parameters\"\"\"\n        class CustomParamAsyncNode(AsyncNode):\n            async def post_async(self, shared_storage, prep_result, proc_result):\n                key = self.params.get('key')\n                multiplier = self.params.get('multiplier', 1)\n                await asyncio.sleep(0.01)\n                if 'results' not in shared_storage:\n                    shared_storage['results'] = {}\n                shared_storage['results'][key] = shared_storage['input_data'][key] * multiplier\n                return \"done\"\n\n        class CustomParamAsyncBatchFlow(AsyncBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{\n                    'key': k,\n                    'multiplier': i + 1\n                } for i, k in enumerate(shared_storage['input_data'].keys())]\n\n        shared_storage = {\n            'input_data': {\n                'a': 1,\n                'b': 2,\n                'c': 3\n            }\n        }\n\n        flow = CustomParamAsyncBatchFlow(start=CustomParamAsyncNode())\n        asyncio.run(flow.run_async(shared_storage))\n\n        expected_results = {\n            'a': 1 * 1,  # first item, multiplier = 1\n            'b': 2 * 2,  # second item, multiplier = 2\n            'c': 3 * 3   # third item, multiplier = 3\n        }\n        self.assertEqual(shared_storage['results'], expected_results)\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary for `/tmp/PocketFlow/tests/test_async_batch_flow.py`**\n\n1. **Primary purpose of the code unit:**  \n   The code defines a set of unit tests for verifying the behavior of asynchronous batch processing flows using the `AsyncBatchFlow` and `AsyncNode` classes from the `pocketflow` library. The tests cover core async processing, error handling, custom parameters, empty batches, and nested async batch flows.\n\n2. **Brief description of its parameters:**  \n   - The main test class, `TestAsyncBatchFlow`, does not take parameters directly.\n   - Each test method prepares a `shared_storage` dictionary that simulates input data for batch processing and holds results.\n   - Some node and flow classes accept parameter dictionaries with keys such as `'key'` and `'multiplier'`.\n\n3. **Brief description of its return value:**  \n   - The test methods themselves return `None` (as per unittest conventions).\n   - Intermediate custom nodes' methods (`prep_async`, `post_async`) typically return processed results, e.g., processed data, the string `\"processed\"`/`\"next\"`/`\"done\"`, or raise exceptions for testing error behaviors.\n   - Results of batch computations are stored in the `shared_storage` dictionary.\n\n4. **List of other functions or methods it calls internally:**  \n   - **From `unittest.TestCase`:**\n     - `self.assertEqual()`\n     - `self.assertRaises()`\n   - **From `asyncio`:**\n     - `asyncio.run()`\n     - `asyncio.sleep()`\n   - **From `pocketflow`:**\n     - `AsyncBatchFlow.run_async()`\n     - `AsyncNode.__init__()`\n   - **Other internal methods/classes:**\n     - Custom `prep_async()` and `post_async()` methods defined in test node/flow classes.\n   - **Python built-ins:**\n     - `dict.get()`\n     - Dictionary/list comprehensions\n\n**In summary:**  \nThis file tests asynchronous batch processing flows in `pocketflow` by simulating various scenarios with custom nodes and confirming correct results, error handling, and use of custom parameters via Python's `unittest` and `asyncio` libraries. Inputs and results are passed via a shared storage dictionary (`shared_storage`), and the async flows are constructed to exercise typical and edge-case behaviors of the `AsyncBatchFlow` abstraction."
    },
    "202": {
      "unit_name": "test_fall_back.py",
      "file_path": "/tmp/PocketFlow/tests/test_fall_back.py",
      "code": "import unittest\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import Node, AsyncNode, Flow, AsyncFlow\n\nclass FallbackNode(Node):\n    def __init__(self, should_fail=True, max_retries=1):\n        super().__init__(max_retries=max_retries)\n        self.should_fail = should_fail\n        self.attempt_count = 0\n    \n    def prep(self, shared_storage):\n        if 'results' not in shared_storage:\n            shared_storage['results'] = []\n        return None\n    \n    def exec(self, prep_result):\n        self.attempt_count += 1\n        if self.should_fail:\n            raise ValueError(\"Intentional failure\")\n        return \"success\"\n    \n    def exec_fallback(self, prep_result, exc):\n        return \"fallback\"\n    \n    def post(self, shared_storage, prep_result, exec_result):\n        shared_storage['results'].append({\n            'attempts': self.attempt_count,\n            'result': exec_result\n        })\n\nclass AsyncFallbackNode(AsyncNode):\n    def __init__(self, should_fail=True, max_retries=1):\n        super().__init__(max_retries=max_retries)\n        self.should_fail = should_fail\n        self.attempt_count = 0\n    \n    async def prep_async(self, shared_storage):\n        if 'results' not in shared_storage:\n            shared_storage['results'] = []\n        return None\n    \n    async def exec_async(self, prep_result):\n        self.attempt_count += 1\n        if self.should_fail:\n            raise ValueError(\"Intentional async failure\")\n        return \"success\"\n    \n    async def exec_fallback_async(self, prep_result, exc):\n        await asyncio.sleep(0.01)  # Simulate async work\n        return \"async_fallback\"\n    \n    async def post_async(self, shared_storage, prep_result, exec_result):\n        shared_storage['results'].append({\n            'attempts': self.attempt_count,\n            'result': exec_result\n        })\n\nclass TestExecFallback(unittest.TestCase):\n    def test_successful_execution(self):\n        \"\"\"Test that exec_fallback is not called when execution succeeds\"\"\"\n        shared_storage = {}\n        node = FallbackNode(should_fail=False)\n        result = node.run(shared_storage)\n        \n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['attempts'], 1)\n        self.assertEqual(shared_storage['results'][0]['result'], \"success\")\n\n    def test_fallback_after_failure(self):\n        \"\"\"Test that exec_fallback is called after all retries are exhausted\"\"\"\n        shared_storage = {}\n        node = FallbackNode(should_fail=True, max_retries=2)\n        result = node.run(shared_storage)\n        \n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['attempts'], 2)\n        self.assertEqual(shared_storage['results'][0]['result'], \"fallback\")\n\n    def test_fallback_in_flow(self):\n        \"\"\"Test that fallback works within a Flow\"\"\"\n        class ResultNode(Node):\n            def prep(self, shared_storage):\n                return shared_storage.get('results', [])\n                \n            def exec(self, prep_result):\n                return prep_result\n                \n            def post(self, shared_storage, prep_result, exec_result):\n                shared_storage['final_result'] = exec_result\n                return None\n        \n        shared_storage = {}\n        fallback_node = FallbackNode(should_fail=True)\n        result_node = ResultNode()\n        fallback_node >> result_node\n        \n        flow = Flow(start=fallback_node)\n        flow.run(shared_storage)\n        \n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['result'], \"fallback\")\n        self.assertEqual(shared_storage['final_result'], [{'attempts': 1, 'result': 'fallback'}] )\n\n    def test_no_fallback_implementation(self):\n        \"\"\"Test that default fallback behavior raises the exception\"\"\"\n        class NoFallbackNode(Node):\n            def prep(self, shared_storage):\n                if 'results' not in shared_storage:\n                    shared_storage['results'] = []\n                return None\n            \n            def exec(self, prep_result):\n                raise ValueError(\"Test error\")\n            \n            def post(self, shared_storage, prep_result, exec_result):\n                shared_storage['results'].append({'result': exec_result})\n                return exec_result\n        \n        shared_storage = {}\n        node = NoFallbackNode()\n        with self.assertRaises(ValueError):\n            node.run(shared_storage)\n\n    def test_retry_before_fallback(self):\n        \"\"\"Test that retries are attempted before calling fallback\"\"\"\n        shared_storage = {}\n        node = FallbackNode(should_fail=True, max_retries=3)\n        node.run(shared_storage)\n        \n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['attempts'], 3)\n        self.assertEqual(shared_storage['results'][0]['result'], \"fallback\")\n\nclass TestAsyncExecFallback(unittest.TestCase):\n    def setUp(self):\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n    \n    def tearDown(self):\n        self.loop.close()\n\n    def test_async_successful_execution(self):\n        \"\"\"Test that async exec_fallback is not called when execution succeeds\"\"\"\n        async def run_test():\n            shared_storage = {}\n            node = AsyncFallbackNode(should_fail=False)\n            await node.run_async(shared_storage)\n            return shared_storage\n        \n        shared_storage = self.loop.run_until_complete(run_test())\n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['attempts'], 1)\n        self.assertEqual(shared_storage['results'][0]['result'], \"success\")\n\n    def test_async_fallback_after_failure(self):\n        \"\"\"Test that async exec_fallback is called after all retries are exhausted\"\"\"\n        async def run_test():\n            shared_storage = {}\n            node = AsyncFallbackNode(should_fail=True, max_retries=2)\n            await node.run_async(shared_storage)\n            return shared_storage\n        \n        shared_storage = self.loop.run_until_complete(run_test())\n        \n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['attempts'], 2)\n        self.assertEqual(shared_storage['results'][0]['result'], \"async_fallback\")\n\n    def test_async_fallback_in_flow(self):\n        \"\"\"Test that async fallback works within an AsyncFlow\"\"\"\n        class AsyncResultNode(AsyncNode):\n            async def prep_async(self, shared_storage):\n                return shared_storage['results'][-1]['result']  # Get last result\n                \n            async def exec_async(self, prep_result):\n                return prep_result\n                \n            async def post_async(self, shared_storage, prep_result, exec_result):\n                shared_storage['final_result'] = exec_result\n                return \"done\"\n        \n        async def run_test():\n            shared_storage = {}\n            fallback_node = AsyncFallbackNode(should_fail=True)\n            result_node = AsyncResultNode()\n            fallback_node >> result_node\n            \n            flow = AsyncFlow(start=fallback_node)\n            await flow.run_async(shared_storage)\n            return shared_storage\n        \n        shared_storage = self.loop.run_until_complete(run_test())\n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['result'], \"async_fallback\")\n        self.assertEqual(shared_storage['final_result'], \"async_fallback\")\n\n    def test_async_no_fallback_implementation(self):\n        \"\"\"Test that default async fallback behavior raises the exception\"\"\"\n        class NoFallbackAsyncNode(AsyncNode):\n            async def prep_async(self, shared_storage):\n                if 'results' not in shared_storage:\n                    shared_storage['results'] = []\n                return None\n            \n            async def exec_async(self, prep_result):\n                raise ValueError(\"Test async error\")\n            \n            async def post_async(self, shared_storage, prep_result, exec_result):\n                shared_storage['results'].append({'result': exec_result})\n                return exec_result\n        \n        async def run_test():\n            shared_storage = {}\n            node = NoFallbackAsyncNode()\n            await node.run_async(shared_storage)\n        \n        with self.assertRaises(ValueError):\n            self.loop.run_until_complete(run_test())\n\n    def test_async_retry_before_fallback(self):\n        \"\"\"Test that retries are attempted before calling async fallback\"\"\"\n        async def run_test():\n            shared_storage = {}\n            node = AsyncFallbackNode(should_fail=True, max_retries=3)\n            result = await node.run_async(shared_storage)\n            return result, shared_storage\n        \n        result, shared_storage = self.loop.run_until_complete(run_test())\n        self.assertEqual(len(shared_storage['results']), 1)\n        self.assertEqual(shared_storage['results'][0]['attempts'], 3)\n        self.assertEqual(shared_storage['results'][0]['result'], \"async_fallback\")\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_fall_back.py`:**\n\n1. **Primary Purpose of the Code Unit:**\n\n   This file provides unit tests for fallback and retry logic in `Node` and `AsyncNode` classes of the PocketFlow framework. It verifies that fallback handlers are triggered after execution failures, the retry mechanism works as intended, and the behavior is correct both for synchronous and asynchronous node execution, including integration within `Flow` and `AsyncFlow` pipelines.\n\n2. **Parameters:**\n\n   - The main test classes and methods themselves do not take external parameters (they\u2019re unittest test cases).\n   - The custom node classes (`FallbackNode`, `AsyncFallbackNode`) accept:\n     - `should_fail` (bool): Whether to intentionally fail in `exec`/`exec_async`.\n     - `max_retries` (int): How many times execution will retry before fallback.\n   - Test methods internally create and pass a `shared_storage` dictionary to node/flow executions.\n\n3. **Return Value:**\n\n   - The tests primarily use assertions and do not return a value; they verify side effects by checking `shared_storage` for expected results.\n   - Node/flow executions (`run`, `run_async`) may return results used internally in the tests but are not significant as return values at the test method level.\n\n4. **Other Functions or Methods Called Internally:**\n\n   - **From PocketFlow:**\n     - `Node.run(shared_storage)`\n     - `Node.__init__`\n     - `AsyncNode.run_async(shared_storage)`\n     - `AsyncNode.__init__`\n     - `Flow.run(shared_storage)`\n     - `AsyncFlow.run_async(shared_storage)`\n     - Node graph composition using `>>`\n   - **Custom Node Methods (sync & async):**\n     - `prep` / `prep_async`\n     - `exec` / `exec_async`\n     - `exec_fallback` / `exec_fallback_async`\n     - `post` / `post_async`\n   - **Python Standard Library:**\n     - `unittest` test framework methods: `assertEqual`, `assertRaises`\n     - `setUp` and `tearDown` for async event loop control\n     - `asyncio.new_event_loop`, `loop.run_until_complete`, `asyncio.sleep`\n\n**In summary:**  \nThis code unit tests fallback and retry behaviors in both synchronous and asynchronous nodes of PocketFlow, ensuring that failures are handled as expected with or without custom fallback handlers, and validates results written into a shared state dictionary."
    },
    "203": {
      "unit_name": "test_async_batch_node.py",
      "file_path": "/tmp/PocketFlow/tests/test_async_batch_node.py",
      "code": "import unittest\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import AsyncNode, AsyncBatchNode, AsyncFlow\n\nclass AsyncArrayChunkNode(AsyncBatchNode):\n    def __init__(self, chunk_size=10):\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    async def prep_async(self, shared_storage):\n        # Get array from shared storage and split into chunks\n        array = shared_storage.get('input_array', [])\n        chunks = []\n        for start in range(0, len(array), self.chunk_size):\n            end = min(start + self.chunk_size, len(array))\n            chunks.append(array[start:end])\n        return chunks\n    \n    async def exec_async(self, chunk):\n        # Simulate async processing of each chunk\n        await asyncio.sleep(0.01)\n        return sum(chunk)\n        \n    async def post_async(self, shared_storage, prep_result, proc_result):\n        # Store chunk results in shared storage\n        shared_storage['chunk_results'] = proc_result\n        return \"processed\"\n\nclass AsyncSumReduceNode(AsyncNode):\n    async def prep_async(self, shared_storage):\n        # Get chunk results from shared storage\n        chunk_results = shared_storage.get('chunk_results', [])\n        await asyncio.sleep(0.01)  # Simulate async processing\n        total = sum(chunk_results)\n        shared_storage['total'] = total\n        return \"reduced\"\n\nclass TestAsyncBatchNode(unittest.TestCase):\n    def test_array_chunking(self):\n        \"\"\"\n        Test that the array is correctly split into chunks and processed asynchronously\n        \"\"\"\n        shared_storage = {\n            'input_array': list(range(25))  # [0,1,2,...,24]\n        }\n        \n        chunk_node = AsyncArrayChunkNode(chunk_size=10)\n        asyncio.run(chunk_node.run_async(shared_storage))\n        \n        results = shared_storage['chunk_results']\n        self.assertEqual(results, [45, 145, 110])  # Sum of chunks [0-9], [10-19], [20-24]\n        \n    # def test_async_map_reduce_sum(self):\n    #     \"\"\"\n    #     Test a complete async map-reduce pipeline that sums a large array:\n    #     1. Map: Split array into chunks and sum each chunk asynchronously\n    #     2. Reduce: Sum all the chunk sums asynchronously\n    #     \"\"\"\n    #     array = list(range(100))\n    #     expected_sum = sum(array)  # 4950\n        \n    #     shared_storage = {\n    #         'input_array': array\n    #     }\n        \n    #     # Create nodes\n    #     chunk_node = AsyncArrayChunkNode(chunk_size=10)\n    #     reduce_node = AsyncSumReduceNode()\n        \n    #     # Connect nodes\n    #     chunk_node - \"processed\" >> reduce_node\n        \n    #     # Create and run pipeline\n    #     pipeline = AsyncFlow(start=chunk_node)\n    #     asyncio.run(pipeline.run_async(shared_storage))\n        \n    #     self.assertEqual(shared_storage['total'], expected_sum)\n        \n    # def test_uneven_chunks(self):\n    #     \"\"\"\n    #     Test that the async map-reduce works correctly with array lengths\n    #     that don't divide evenly by chunk_size\n    #     \"\"\"\n    #     array = list(range(25))\n    #     expected_sum = sum(array)  # 300\n        \n    #     shared_storage = {\n    #         'input_array': array\n    #     }\n        \n    #     chunk_node = AsyncArrayChunkNode(chunk_size=10)\n    #     reduce_node = AsyncSumReduceNode()\n        \n    #     chunk_node - \"processed\" >> reduce_node\n    #     pipeline = AsyncFlow(start=chunk_node)\n    #     asyncio.run(pipeline.run_async(shared_storage))\n        \n    #     self.assertEqual(shared_storage['total'], expected_sum)\n\n    # def test_custom_chunk_size(self):\n    #     \"\"\"\n    #     Test that the async map-reduce works with different chunk sizes\n    #     \"\"\"\n    #     array = list(range(100))\n    #     expected_sum = sum(array)\n        \n    #     shared_storage = {\n    #         'input_array': array\n    #     }\n        \n    #     # Use chunk_size=15 instead of default 10\n    #     chunk_node = AsyncArrayChunkNode(chunk_size=15)\n    #     reduce_node = AsyncSumReduceNode()\n        \n    #     chunk_node - \"processed\" >> reduce_node\n    #     pipeline = AsyncFlow(start=chunk_node)\n    #     asyncio.run(pipeline.run_async(shared_storage))\n        \n    #     self.assertEqual(shared_storage['total'], expected_sum)\n        \n    # def test_single_element_chunks(self):\n    #     \"\"\"\n    #     Test extreme case where chunk_size=1\n    #     \"\"\"\n    #     array = list(range(5))\n    #     expected_sum = sum(array)\n        \n    #     shared_storage = {\n    #         'input_array': array\n    #     }\n        \n    #     chunk_node = AsyncArrayChunkNode(chunk_size=1)\n    #     reduce_node = AsyncSumReduceNode()\n        \n    #     chunk_node - \"processed\" >> reduce_node\n    #     pipeline = AsyncFlow(start=chunk_node)\n    #     asyncio.run(pipeline.run_async(shared_storage))\n        \n    #     self.assertEqual(shared_storage['total'], expected_sum)\n\n    # def test_empty_array(self):\n    #     \"\"\"\n    #     Test edge case of empty input array\n    #     \"\"\"\n    #     shared_storage = {\n    #         'input_array': []\n    #     }\n        \n    #     chunk_node = AsyncArrayChunkNode(chunk_size=10)\n    #     reduce_node = AsyncSumReduceNode()\n        \n    #     chunk_node - \"processed\" >> reduce_node\n    #     pipeline = AsyncFlow(start=chunk_node)\n    #     asyncio.run(pipeline.run_async(shared_storage))\n        \n    #     self.assertEqual(shared_storage['total'], 0)\n\n    # def test_error_handling(self):\n    #     \"\"\"\n    #     Test error handling in async batch processing\n    #     \"\"\"\n    #     class ErrorAsyncBatchNode(AsyncBatchNode):\n    #         async def exec_async(self, item):\n    #             if item == 2:\n    #                 raise ValueError(\"Error processing item 2\")\n    #             return item\n\n    #     shared_storage = {\n    #         'input_array': [1, 2, 3]\n    #     }\n        \n    #     error_node = ErrorAsyncBatchNode()\n    #     with self.assertRaises(ValueError):\n    #         asyncio.run(error_node.run_async(shared_storage))\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_async_batch_node.py`**\n\n1. **Primary purpose:**  \n   This code unit is a set of unit tests designed to validate the asynchronous batch processing capabilities of the `PocketFlow` framework, specifically testing the logic of splitting arrays into chunks, processing them asynchronously, and performing reductions (e.g., sum) using custom async nodes (`AsyncBatchNode` and `AsyncNode`). It ensures that asynchronous data processing with batching and reduction works as expected, including in edge and error cases.\n\n2. **Parameters:**  \n   - The primary class under test, `AsyncArrayChunkNode`, can be constructed with a `chunk_size` parameter (default 10) to specify how large each data chunk should be.\n   - The test methods themselves accept `self` as per standard unittest `TestCase` signatures.\n   - The test logic typically uses `shared_storage`, a dictionary simulating state shared among async nodes.\n\n3. **Return value:**  \n   - The `test_array_chunking` test (and others, though commented-out) does not return a value but uses assertions to validate correct behavior.\n   - The `prep_async`, `exec_async`, and `post_async` methods of nodes return chunked lists, computations, or status strings, used as part of the async processing flow.\n\n4. **Other functions or methods called internally:**  \n   - **PocketFlow API:**\n     - `AsyncNode`, `AsyncBatchNode`, `AsyncFlow` (base classes and pipeline orchestration\u2014imported, not shown, presumably core to the framework)\n     - `run_async`: Executes the asynchronous workflow for a node or flow (`chunk_node.run_async(shared_storage)`, etc.)\n   - **Python Standard Library:**\n     - `unittest`: For test structure and assertions.\n     - `asyncio.sleep`: Used to simulate asynchronous delay.\n     - Standard assertion methods, e.g., `self.assertEqual`, `self.assertRaises`.\n   - **Class and method definitions:**\n     - `AsyncArrayChunkNode`\n       - `prep_async` (splits the array into chunks)\n       - `exec_async` (calculates the sum of each chunk)\n       - `post_async` (stores results)\n     - `AsyncSumReduceNode`\n       - `prep_async` (sums chunk results)\n   - (Commented-out tests would also test dataflow between nodes using the custom chaining syntax: `chunk_node - \"processed\" >> reduce_node`.)\n\n**In summary:**  \nThis code tests asynchronous chunking, processing, and reduction operations in a custom async dataflow framework, verifying correct results under various chunking scenarios and error cases. It internally calls both custom async methods for chunking/reduction and standard unittest and asyncio functionality."
    },
    "204": {
      "unit_name": "test_batch_node.py",
      "file_path": "/tmp/PocketFlow/tests/test_batch_node.py",
      "code": "import unittest\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import Node, BatchNode, Flow\n\nclass ArrayChunkNode(BatchNode):\n    def __init__(self, chunk_size=10):\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared_storage):\n        # Get array from shared storage and split into chunks\n        array = shared_storage.get('input_array', [])\n        chunks = []\n        for start in range(0, len(array), self.chunk_size):\n            end = min(start + self.chunk_size, len(array))\n            chunks.append(array[start: end])\n        return chunks\n    \n    def exec(self, chunk):\n        # Process the chunk and return its sum\n        chunk_sum = sum(chunk)\n        return chunk_sum\n        \n    def post(self, shared_storage, prep_result, proc_result):\n        # Store chunk results in shared storage\n        shared_storage['chunk_results'] = proc_result\n        return \"default\"\n\nclass SumReduceNode(Node):\n    def prep(self, shared_storage):\n        # Get chunk results from shared storage and sum them\n        chunk_results = shared_storage.get('chunk_results', [])\n        total = sum(chunk_results)\n        shared_storage['total'] = total\n\nclass TestBatchNode(unittest.TestCase):\n    def test_array_chunking(self):\n        \"\"\"\n        Test that the array is correctly split into chunks\n        \"\"\"\n        shared_storage = {\n            'input_array': list(range(25))  # [0,1,2,...,24]\n        }\n        \n        chunk_node = ArrayChunkNode(chunk_size=10)\n        chunk_node.run(shared_storage)\n        results = shared_storage['chunk_results']\n        self.assertEqual(results, [45, 145, 110])\n        \n    def test_map_reduce_sum(self):\n        \"\"\"\n        Test a complete map-reduce pipeline that sums a large array:\n        1. Map: Split array into chunks and sum each chunk\n        2. Reduce: Sum all the chunk sums\n        \"\"\"\n        # Create test array: [0,1,2,...,99]\n        array = list(range(100))\n        expected_sum = sum(array)  # 4950\n        \n        shared_storage = {\n            'input_array': array\n        }\n        \n        # Create nodes\n        chunk_node = ArrayChunkNode(chunk_size=10)\n        reduce_node = SumReduceNode()\n        \n        # Connect nodes\n        chunk_node >> reduce_node\n        \n        # Create and run pipeline\n        pipeline = Flow(start=chunk_node)\n        pipeline.run(shared_storage)\n        \n        self.assertEqual(shared_storage['total'], expected_sum)\n        \n    def test_uneven_chunks(self):\n        \"\"\"\n        Test that the map-reduce works correctly with array lengths\n        that don't divide evenly by chunk_size\n        \"\"\"\n        array = list(range(25))\n        expected_sum = sum(array)  # 300\n        \n        shared_storage = {\n            'input_array': array\n        }\n        \n        chunk_node = ArrayChunkNode(chunk_size=10)\n        reduce_node = SumReduceNode()\n        \n        chunk_node >> reduce_node\n        pipeline = Flow(start=chunk_node)\n        pipeline.run(shared_storage)\n        \n        self.assertEqual(shared_storage['total'], expected_sum)\n\n    def test_custom_chunk_size(self):\n        \"\"\"\n        Test that the map-reduce works with different chunk sizes\n        \"\"\"\n        array = list(range(100))\n        expected_sum = sum(array)\n        \n        shared_storage = {\n            'input_array': array\n        }\n        \n        # Use chunk_size=15 instead of default 10\n        chunk_node = ArrayChunkNode(chunk_size=15)\n        reduce_node = SumReduceNode()\n        \n        chunk_node >> reduce_node\n        pipeline = Flow(start=chunk_node)\n        pipeline.run(shared_storage)\n        \n        self.assertEqual(shared_storage['total'], expected_sum)\n        \n    def test_single_element_chunks(self):\n        \"\"\"\n        Test extreme case where chunk_size=1\n        \"\"\"\n        array = list(range(5))\n        expected_sum = sum(array)\n        \n        shared_storage = {\n            'input_array': array\n        }\n        \n        chunk_node = ArrayChunkNode(chunk_size=1)\n        reduce_node = SumReduceNode()\n        \n        chunk_node >> reduce_node\n        pipeline = Flow(start=chunk_node)\n        pipeline.run(shared_storage)\n        \n        self.assertEqual(shared_storage['total'], expected_sum)\n\n    def test_empty_array(self):\n        \"\"\"\n        Test edge case of empty input array\n        \"\"\"\n        shared_storage = {\n            'input_array': []\n        }\n        \n        chunk_node = ArrayChunkNode(chunk_size=10)\n        reduce_node = SumReduceNode()\n        \n        chunk_node >> reduce_node\n        pipeline = Flow(start=chunk_node)\n        pipeline.run(shared_storage)\n        \n        self.assertEqual(shared_storage['total'], 0)\n\nif __name__ == '__main__':\n    unittest.main()\n",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_batch_node.py`**\n\n1. **Primary Purpose:**  \n   This code unit is a set of unit tests (using Python\u2019s `unittest` framework) for validating the batch processing and map-reduce capabilities of the `BatchNode` and `Flow` components in the PocketFlow framework. It specifically tests the correct behavior of splitting arrays into chunks, processing each chunk, reducing results, and handling various edge cases (such as empty arrays or single-element chunks).\n\n2. **Brief Description of Parameters:**  \n   - The main test subject is the custom `ArrayChunkNode`, which takes a single parameter `chunk_size` in its constructor to determine the size of each chunk when splitting the input array.\n   - Test methods take no parameters; they construct their own local arrays and configuration via in-method variables.\n   - Shared state is passed and modified via the mutable `shared_storage` dictionary.\n\n3. **Brief Description of Return Values:**  \n   - Individual methods in test classes do not return values (they use assertions).\n   - `ArrayChunkNode.prep` returns a list of array \"chunks\".\n   - `ArrayChunkNode.exec` returns the sum of one chunk (an integer).\n   - `ArrayChunkNode.post` returns the string `\"default\"`, but its main work is to update the `shared_storage` dictionary with the results.\n   - `SumReduceNode.prep` updates `shared_storage` with the final `total`, no explicit return.\n   - The core return value of a full pipeline run is the modification of `shared_storage` (e.g., setting `'chunk_results'` and `'total'`).\n\n4. **Other Functions or Methods Called Internally:**  \n   - `BatchNode.__init__`, `Node.__init__` (via `super().__init__()`).\n   - `BatchNode.prep`, `BatchNode.exec`, `BatchNode.post` (the three classic phases of a batch pipeline node).\n   - `Node.prep` (for reduce node).\n   - `chunk_node.run(shared_storage)` and `pipeline.run(shared_storage)` (methods of `Node`/`Flow` classes).\n   - Standard library: `sum`, `min`, `range`, list methods.\n   - `Flow.__init__` (to initialize a pipeline).\n   - Operator overloading: `chunk_node >> reduce_node` (for connecting nodes).\n   - `unittest` assertion methods (`assertEqual`).\n   - Python introspection: `sys.path.insert`, `Path(__file__).parent.parent`, and relative import of `pocketflow`.\n\n**In summary:**  \nThis is a test suite that constructs custom batch-processing and reduction nodes for arrays, creates small pipelines with different chunk sizes and input arrays, connects and runs them using the `pocketflow` framework, and asserts that the output matches expected results under various scenarios. The key logic tested is the division of work (splitting/summing) and aggregation (reducing/summing chunk sums)."
    },
    "205": {
      "unit_name": "test_batch_flow.py",
      "file_path": "/tmp/PocketFlow/tests/test_batch_flow.py",
      "code": "import unittest\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import Node, BatchFlow, Flow\n\nclass DataProcessNode(Node):\n    def prep(self, shared_storage):\n        key = self.params.get('key')\n        data = shared_storage['input_data'][key]\n        if 'results' not in shared_storage:\n            shared_storage['results'] = {}\n        shared_storage['results'][key] = data * 2\n\nclass ErrorProcessNode(Node):\n    def prep(self, shared_storage):\n        key = self.params.get('key')\n        if key == 'error_key':\n            raise ValueError(f\"Error processing key: {key}\")\n        if 'results' not in shared_storage:\n            shared_storage['results'] = {}\n        shared_storage['results'][key] = True\n\nclass TestBatchFlow(unittest.TestCase):\n    def setUp(self):\n        self.process_node = DataProcessNode()\n        \n    def test_basic_batch_processing(self):\n        \"\"\"Test basic batch processing with multiple keys\"\"\"\n        class SimpleTestBatchFlow(BatchFlow):\n            def prep(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        shared_storage = {\n            'input_data': {\n                'a': 1,\n                'b': 2,\n                'c': 3\n            }\n        }\n\n        flow = SimpleTestBatchFlow(start=self.process_node)\n        flow.run(shared_storage)\n\n        expected_results = {\n            'a': 2,\n            'b': 4,\n            'c': 6\n        }\n        self.assertEqual(shared_storage['results'], expected_results)\n\n    def test_empty_input(self):\n        \"\"\"Test batch processing with empty input dictionary\"\"\"\n        class EmptyTestBatchFlow(BatchFlow):\n            def prep(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        shared_storage = {\n            'input_data': {}\n        }\n\n        flow = EmptyTestBatchFlow(start=self.process_node)\n        flow.run(shared_storage)\n\n        self.assertEqual(shared_storage.get('results', {}), {})\n\n    def test_single_item(self):\n        \"\"\"Test batch processing with single item\"\"\"\n        class SingleItemBatchFlow(BatchFlow):\n            def prep(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        shared_storage = {\n            'input_data': {\n                'single': 5\n            }\n        }\n\n        flow = SingleItemBatchFlow(start=self.process_node)\n        flow.run(shared_storage)\n\n        expected_results = {\n            'single': 10\n        }\n        self.assertEqual(shared_storage['results'], expected_results)\n\n    def test_error_handling(self):\n        \"\"\"Test error handling during batch processing\"\"\"\n        class ErrorTestBatchFlow(BatchFlow):\n            def prep(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        shared_storage = {\n            'input_data': {\n                'normal_key': 1,\n                'error_key': 2,\n                'another_key': 3\n            }\n        }\n\n        flow = ErrorTestBatchFlow(start=ErrorProcessNode())\n        \n        with self.assertRaises(ValueError):\n            flow.run(shared_storage)\n\n    def test_nested_flow(self):\n        \"\"\"Test batch processing with nested flows\"\"\"\n        class InnerNode(Node):\n            def exec(self, prep_result):\n                key = self.params.get('key')\n                if 'intermediate_results' not in shared_storage:\n                    shared_storage['intermediate_results'] = {}\n                shared_storage['intermediate_results'][key] = shared_storage['input_data'][key] + 1\n\n        class OuterNode(Node):\n            def exec(self, prep_result):\n                key = self.params.get('key')\n                if 'results' not in shared_storage:\n                    shared_storage['results'] = {}\n                shared_storage['results'][key] = shared_storage['intermediate_results'][key] * 2\n\n        class NestedBatchFlow(BatchFlow):\n            def prep(self, shared_storage):\n                return [{'key': k} for k in shared_storage['input_data'].keys()]\n\n        # Create inner flow\n        inner_node = InnerNode()\n        outer_node = OuterNode()\n        inner_node >> outer_node\n\n        shared_storage = {\n            'input_data': {\n                'x': 1,\n                'y': 2\n            }\n        }\n\n        flow = NestedBatchFlow(start=inner_node)\n        flow.run(shared_storage)\n\n        expected_results = {\n            'x': 4,  # (1 + 1) * 2\n            'y': 6   # (2 + 1) * 2\n        }\n        self.assertEqual(shared_storage['results'], expected_results)\n\n    def test_custom_parameters(self):\n        \"\"\"Test batch processing with additional custom parameters\"\"\"\n        class CustomParamNode(Node):\n            def exec(self, prep_result):\n                key = self.params.get('key')\n                multiplier = self.params.get('multiplier', 1)\n                if 'results' not in shared_storage:\n                    shared_storage['results'] = {}\n                shared_storage['results'][key] = shared_storage['input_data'][key] * multiplier\n\n        class CustomParamBatchFlow(BatchFlow):\n            def prep(self, shared_storage):\n                return [{\n                    'key': k,\n                    'multiplier': i + 1\n                } for i, k in enumerate(shared_storage['input_data'].keys())]\n\n        shared_storage = {\n            'input_data': {\n                'a': 1,\n                'b': 2,\n                'c': 3\n            }\n        }\n\n        flow = CustomParamBatchFlow(start=CustomParamNode())\n        flow.run(shared_storage)\n\n        expected_results = {\n            'a': 1 * 1,  # first item, multiplier = 1\n            'b': 2 * 2,  # second item, multiplier = 2\n            'c': 3 * 3   # third item, multiplier = 3\n        }\n        self.assertEqual(shared_storage['results'], expected_results)\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_batch_flow.py`**\n\n1. **Primary Purpose:**  \n   This code unit is a test suite that verifies the batch processing capabilities and robustness of the `BatchFlow` class in the PocketFlow framework. It uses the `unittest` framework to define and run a variety of test cases that check different aspects of batch flow processing, including basic operation, edge cases, error handling, nested flows, and the use of custom parameters.\n\n2. **Parameters:**  \n   Most functions within this code are unit test methods or small inner classes for testing, so they don't take external parameters beyond those used for testing purposes (like `shared_storage` dictionaries simulating batch input). The most relevant \"parameters\" are:\n   - `shared_storage`: A dictionary representing input data and the results storage for the batch process.\n   - Node parameters (for example, `'key'`, `'multiplier'`): These are passed in the `params` dictionary to individual nodes for processing.\n\n3. **Return Value:**  \n   The test methods do not return values; instead, they use assertions (like `self.assertEqual`, `self.assertRaises`) to check expected behaviors. The actual methods/classes under test (`BatchFlow`, `Node`, etc.) manipulate `shared_storage` in place, storing results or raising errors as part of their execution.\n\n4. **Functions and Methods Called Internally:**  \n   - **Framework Classes/Methods:**\n     - `BatchFlow.run(shared_storage)`\n     - `Node.exec(prep_result)` and `Node.prep(shared_storage)`\n     - Use of `Node` subclasses for custom processing steps\n     - Custom BatchFlow/Node classes defined in test methods\n     - Operator `>>` for chaining nodes (as seen in nested flow)\n   - **Testing Methods:**\n     - `unittest.TestCase.setUp`\n     - `unittest.TestCase.assertEqual`\n     - `unittest.TestCase.assertRaises`\n     - `unittest.main()`\n   - **Standard Python Built-ins:**\n     - Dictionary operations on `shared_storage`\n     - List/dictionary comprehensions\n     - Exception raising (`raise ValueError`)\n     - Standard class and function definitions\n\n**In summary:**  \nThis file contains unit tests for the batch data flow logic in PocketFlow, ensuring correct handling of input data, parameter passing, results aggregation, exception handling, and chaining/nesting of flows or nodes. The main interaction points are the custom `Node` and `BatchFlow` subclasses and manipulation of the `shared_storage` dict, with all test validation performed using the standard `unittest` assertions."
    },
    "206": {
      "unit_name": "test_flow_basic.py",
      "file_path": "/tmp/PocketFlow/tests/test_flow_basic.py",
      "code": "# tests/test_flow_basic.py\nimport unittest\nimport sys\nfrom pathlib import Path\nimport warnings\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import Node, Flow\n\n# --- Node Definitions ---\n# Nodes intended for default transitions (>>) should NOT return a specific\n# action string from post. Let it return None by default.\n# Nodes intended for conditional transitions (-) MUST return the action string.\n\nclass NumberNode(Node):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n    def prep(self, shared_storage):\n        shared_storage['current'] = self.number\n    # post implicitly returns None - used for default transition\n\nclass AddNode(Node):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n    def prep(self, shared_storage):\n        shared_storage['current'] += self.number\n    # post implicitly returns None - used for default transition\n\nclass MultiplyNode(Node):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n    def prep(self, shared_storage):\n        shared_storage['current'] *= self.number\n    # post implicitly returns None - used for default transition\n\nclass CheckPositiveNode(Node):\n   # This node IS designed for conditional branching\n   def prep(self, shared_storage):\n       pass\n   def post(self, shared_storage, prep_result, proc_result):\n        # MUST return the specific action string for branching\n        if shared_storage['current'] >= 0:\n            return 'positive'\n        else:\n            return 'negative'\n\nclass NoOpNode(Node):\n    # Just a placeholder node\n    pass # post implicitly returns None\n\nclass EndSignalNode(Node):\n    # A node specifically to return a value when it's the end\n    def __init__(self, signal=\"finished\"):\n        super().__init__()\n        self.signal = signal\n    def post(self, shared_storage, prep_result, exec_result):\n        return self.signal # Return a specific signal\n\n# --- Test Class ---\nclass TestFlowBasic(unittest.TestCase):\n\n    def test_start_method_initialization(self):\n        \"\"\"Test initializing flow with start() after creation.\"\"\"\n        shared_storage = {}\n        n1 = NumberNode(5)\n        pipeline = Flow()\n        pipeline.start(n1)\n        last_action = pipeline.run(shared_storage)\n        self.assertEqual(shared_storage['current'], 5)\n        # NumberNode.post returns None (default)\n        self.assertIsNone(last_action)\n\n    def test_start_method_chaining(self):\n        \"\"\"Test fluent chaining using start().next()...\"\"\"\n        shared_storage = {}\n        pipeline = Flow()\n        # Chain: NumberNode -> AddNode -> MultiplyNode\n        # All use default transitions (post returns None)\n        pipeline.start(NumberNode(5)).next(AddNode(3)).next(MultiplyNode(2))\n        last_action = pipeline.run(shared_storage)\n        self.assertEqual(shared_storage['current'], 16)\n        # Last node (MultiplyNode) post returns None\n        self.assertIsNone(last_action)\n\n    def test_sequence_with_rshift(self):\n        \"\"\"Test a simple linear pipeline using >>\"\"\"\n        shared_storage = {}\n        n1 = NumberNode(5)\n        n2 = AddNode(3)\n        n3 = MultiplyNode(2)\n\n        pipeline = Flow()\n        # All default transitions (post returns None)\n        pipeline.start(n1) >> n2 >> n3\n\n        last_action = pipeline.run(shared_storage)\n        self.assertEqual(shared_storage['current'], 16)\n        # Last node (n3: MultiplyNode) post returns None\n        self.assertIsNone(last_action)\n\n    def test_branching_positive(self):\n        \"\"\"Test positive branch: CheckPositiveNode returns 'positive'\"\"\"\n        shared_storage = {}\n        start_node = NumberNode(5)    # post -> None\n        check_node = CheckPositiveNode() # post -> 'positive' or 'negative'\n        add_if_positive = AddNode(10) # post -> None\n        add_if_negative = AddNode(-20) # post -> None (won't run)\n\n        pipeline = Flow()\n        # start -> check (default); check branches on 'positive'/'negative'\n        pipeline.start(start_node) >> check_node\n        check_node - \"positive\" >> add_if_positive\n        check_node - \"negative\" >> add_if_negative\n\n        # Execution: start_node -> check_node -> add_if_positive\n        last_action = pipeline.run(shared_storage)\n        self.assertEqual(shared_storage['current'], 15) # 5 + 10\n        # Last node executed was add_if_positive, its post returns None\n        self.assertIsNone(last_action)\n\n    def test_branching_negative(self):\n        \"\"\"Test negative branch: CheckPositiveNode returns 'negative'\"\"\"\n        shared_storage = {}\n        start_node = NumberNode(-5)   # post -> None\n        check_node = CheckPositiveNode() # post -> 'positive' or 'negative'\n        add_if_positive = AddNode(10) # post -> None (won't run)\n        add_if_negative = AddNode(-20) # post -> None\n\n        pipeline = Flow()\n        pipeline.start(start_node) >> check_node\n        check_node - \"positive\" >> add_if_positive\n        check_node - \"negative\" >> add_if_negative\n\n        # Execution: start_node -> check_node -> add_if_negative\n        last_action = pipeline.run(shared_storage)\n        self.assertEqual(shared_storage['current'], -25) # -5 + -20\n        # Last node executed was add_if_negative, its post returns None\n        self.assertIsNone(last_action)\n\n    def test_cycle_until_negative_ends_with_signal(self):\n        \"\"\"Test cycle, ending on a node that returns a signal\"\"\"\n        shared_storage = {}\n        n1 = NumberNode(10)           # post -> None\n        check = CheckPositiveNode()   # post -> 'positive' or 'negative'\n        subtract3 = AddNode(-3)       # post -> None\n        end_node = EndSignalNode(\"cycle_done\") # post -> \"cycle_done\"\n\n        pipeline = Flow()\n        pipeline.start(n1) >> check\n        # Branching from CheckPositiveNode\n        check - 'positive' >> subtract3\n        check - 'negative' >> end_node # End on negative branch\n        # After subtracting, go back to check (default transition)\n        subtract3 >> check\n\n        # Execution: n1->check->sub3->check->sub3->check->sub3->check->sub3->check->end_node\n        last_action = pipeline.run(shared_storage)\n        self.assertEqual(shared_storage['current'], -2) # 10 -> 7 -> 4 -> 1 -> -2\n        # Last node executed was end_node, its post returns \"cycle_done\"\n        self.assertEqual(last_action, \"cycle_done\")\n\n    def test_flow_ends_warning_default_missing(self):\n        \"\"\"Test warning when default transition is needed but not found\"\"\"\n        shared_storage = {}\n        # Node that returns a specific action from post\n        class ActionNode(Node):\n            def post(self, *args): return \"specific_action\"\n        start_node = ActionNode()\n        next_node = NoOpNode()\n\n        pipeline = Flow()\n        pipeline.start(start_node)\n        # Define successor only for the specific action\n        start_node - \"specific_action\" >> next_node\n\n        # Make start_node return None instead, triggering default search\n        start_node.post = lambda *args: None\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            # Run flow. start_node runs, post returns None.\n            # Flow looks for \"default\", but only \"specific_action\" exists.\n            last_action = pipeline.run(shared_storage)\n\n            self.assertEqual(len(w), 1)\n            self.assertTrue(issubclass(w[-1].category, UserWarning))\n            # Warning message should indicate \"default\" wasn't found\n            self.assertIn(\"Flow ends: 'None' not found in ['specific_action']\", str(w[-1].message))\n        # Last action is from start_node's post\n        self.assertIsNone(last_action)\n\n    def test_flow_ends_warning_specific_missing(self):\n        \"\"\"Test warning when specific action is returned but not found\"\"\"\n        shared_storage = {}\n        # Node that returns a specific action from post\n        class ActionNode(Node):\n            def post(self, *args): return \"specific_action\"\n        start_node = ActionNode()\n        next_node = NoOpNode()\n\n        pipeline = Flow()\n        pipeline.start(start_node)\n        # Define successor only for \"default\"\n        start_node >> next_node # same as start_node.next(next_node, \"default\")\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            # Run flow. start_node runs, post returns \"specific_action\".\n            # Flow looks for \"specific_action\", but only \"default\" exists.\n            last_action = pipeline.run(shared_storage)\n\n            self.assertEqual(len(w), 1)\n            self.assertTrue(issubclass(w[-1].category, UserWarning))\n            # Warning message should indicate \"specific_action\" wasn't found\n            self.assertIn(\"Flow ends: 'specific_action' not found in ['default']\", str(w[-1].message))\n        # Last action is from start_node's post\n        self.assertEqual(last_action, \"specific_action\")\n\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_flow_basic.py`:**\n\n---\n\n**1. Primary purpose:**  \nThis code unit implements a suite of unit tests for the PocketFlow library's `Flow` system, verifying the correct behavior of node sequencing, conditional (branched) transitions, and warning signals when transitions are missing or misconfigured. It tests the core mechanics of constructing and running directed flows of `Node` objects with both default and conditional transitions.\n\n**2. Parameters:**  \nThis is a test file using Python's `unittest` framework. The individual test methods generally take no parameters except for `self` (typical for `unittest.TestCase`). Some inner classes or methods (like custom Node subclasses) accept parameters such as `number` or `signal`, used to initialize node state. The main shared \"parameter\" within tests is the `shared_storage` dictionary, which is passed to the flow execution to store computation state.\n\n**3. Return value:**  \nThe test methods do not return any values (standard for `unittest`), but the various flow executions (`pipeline.run(shared_storage)`) return the last action string generated by the last node's `post` method in the flow. This is checked with assertions, but is not returned from the test methods.\n\n**4. Other functions or methods called internally:**  \nInternal calls within the test code and custom node classes include:\n- `Node.__init__`, `Node.prep`, `Node.post` (via custom subclasses and Flow execution)\n- `Flow()`, `Flow.start()`, `Flow.next()` (via `.next()` or `>>`)\n- Transition linkage operators: `>>` (for default transitions), `-`/`>>` (for conditional branching)\n- `Flow.run(shared_storage)` to execute the constructed flow\n- `unittest.TestCase` methods:\n  - `self.assertEqual`\n  - `self.assertIsNone`\n  - `self.assertTrue`\n  - `self.assertIn`\n- Warnings handling via the `warnings` module:\n  - `warnings.catch_warnings`\n  - `warnings.simplefilter`\n- File and path manipulation via `sys.path` and `pathlib.Path` (for import setup)\n- The test code also uses custom inner classes like `NumberNode`, `AddNode`, `MultiplyNode`, `CheckPositiveNode`, `NoOpNode`, and `EndSignalNode` as stubs/mocks for flow node behaviors.\n\n**In summary:**  \nThis unit is dedicated to testing the correct functioning of PocketFlow's flow composition and execution system. It verifies sequential and branching transitions between nodes, correct update of shared state, action return values, and appropriate warnings when transitions are missing. The test file wraps its checks in methods using the standard unittest framework, with no external parameters or return values beyond those used internally for assertions."
    },
    "207": {
      "unit_name": "test_async_parallel_batch_node.py",
      "file_path": "/tmp/PocketFlow/tests/test_async_parallel_batch_node.py",
      "code": "import unittest\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import AsyncParallelBatchNode, AsyncParallelBatchFlow\n\nclass AsyncParallelNumberProcessor(AsyncParallelBatchNode):\n    def __init__(self, delay=0.1):\n        super().__init__()\n        self.delay = delay\n    \n    async def prep_async(self, shared_storage):\n        numbers = shared_storage.get('input_numbers', [])\n        return numbers\n    \n    async def exec_async(self, number):\n        await asyncio.sleep(self.delay)  # Simulate async processing\n        return number * 2\n        \n    async def post_async(self, shared_storage, prep_result, exec_result):\n        shared_storage['processed_numbers'] = exec_result\n        return \"processed\"\n\nclass TestAsyncParallelBatchNode(unittest.TestCase):\n    def setUp(self):\n        # Reset the event loop for each test\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n    \n    def tearDown(self):\n        self.loop.close()\n    \n    def test_parallel_processing(self):\n        \"\"\"\n        Test that numbers are processed in parallel by measuring execution time\n        \"\"\"\n        shared_storage = {\n            'input_numbers': list(range(5))\n        }\n        \n        processor = AsyncParallelNumberProcessor(delay=0.1)\n        \n        # Run the processor\n        start_time = asyncio.get_event_loop().time()\n        self.loop.run_until_complete(processor.run_async(shared_storage))\n        end_time = asyncio.get_event_loop().time()\n        \n        # Check results\n        expected = [0, 2, 4, 6, 8]  # Each number doubled\n        self.assertEqual(shared_storage['processed_numbers'], expected)\n        \n        # Since processing is parallel, total time should be approximately\n        # equal to the delay of a single operation, not delay * number_of_items\n        execution_time = end_time - start_time\n        self.assertLess(execution_time, 0.2)  # Should be around 0.1s plus minimal overhead\n    \n    def test_empty_input(self):\n        \"\"\"\n        Test processing of empty input\n        \"\"\"\n        shared_storage = {\n            'input_numbers': []\n        }\n        \n        processor = AsyncParallelNumberProcessor()\n        self.loop.run_until_complete(processor.run_async(shared_storage))\n        \n        self.assertEqual(shared_storage['processed_numbers'], [])\n    \n    def test_single_item(self):\n        \"\"\"\n        Test processing of a single item\n        \"\"\"\n        shared_storage = {\n            'input_numbers': [42]\n        }\n        \n        processor = AsyncParallelNumberProcessor()\n        self.loop.run_until_complete(processor.run_async(shared_storage))\n        \n        self.assertEqual(shared_storage['processed_numbers'], [84])\n    \n    def test_large_batch(self):\n        \"\"\"\n        Test processing of a large batch of numbers\n        \"\"\"\n        input_size = 100\n        shared_storage = {\n            'input_numbers': list(range(input_size))\n        }\n        \n        processor = AsyncParallelNumberProcessor(delay=0.01)\n        self.loop.run_until_complete(processor.run_async(shared_storage))\n        \n        expected = [x * 2 for x in range(input_size)]\n        self.assertEqual(shared_storage['processed_numbers'], expected)\n    \n    def test_error_handling(self):\n        \"\"\"\n        Test error handling during parallel processing\n        \"\"\"\n        class ErrorProcessor(AsyncParallelNumberProcessor):\n            async def exec_async(self, item):\n                if item == 2:\n                    raise ValueError(f\"Error processing item {item}\")\n                return item\n        \n        shared_storage = {\n            'input_numbers': [1, 2, 3]\n        }\n        \n        processor = ErrorProcessor()\n        with self.assertRaises(ValueError):\n            self.loop.run_until_complete(processor.run_async(shared_storage))\n    \n    def test_concurrent_execution(self):\n        \"\"\"\n        Test that tasks are actually running concurrently by tracking execution order\n        \"\"\"\n        execution_order = []\n        \n        class OrderTrackingProcessor(AsyncParallelNumberProcessor):\n            async def exec_async(self, item):\n                delay = 0.1 if item % 2 == 0 else 0.05\n                await asyncio.sleep(delay)\n                execution_order.append(item)\n                return item\n        \n        shared_storage = {\n            'input_numbers': list(range(4))  # [0, 1, 2, 3]\n        }\n        \n        processor = OrderTrackingProcessor()\n        self.loop.run_until_complete(processor.run_async(shared_storage))\n        \n        # Odd numbers should finish before even numbers due to shorter delay\n        self.assertLess(execution_order.index(1), execution_order.index(0))\n        self.assertLess(execution_order.index(3), execution_order.index(2))\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_async_parallel_batch_node.py`:**\n\n1. **Primary Purpose:**  \n   This code unit defines a suite of unit tests for the `AsyncParallelBatchNode` (and related `AsyncParallelBatchFlow`) component, specifically focusing on parallel asynchronous batch processing of data. It ensures that the node correctly processes batches in parallel, handles different input conditions, manages errors, and preserves proper execution order in an asynchronous context.\n\n2. **Parameters:**  \n   - The main test class, `TestAsyncParallelBatchNode`, doesn't take parameters itself, but its test methods use a shared dictionary (`shared_storage`) that typically contains an `'input_numbers'` list as input to the processor.\n   - The processor classes (e.g., `AsyncParallelNumberProcessor`) can take a `delay` parameter (default `0.1` seconds), indicating simulated processing time per item.\n\n3. **Return Values:**  \n   - Most test methods do not directly return a value but use assertions to validate behavior.\n   - The asynchronous processor methods (`prep_async`, `exec_async`, `post_async`) return processed data for use by the batch node, with `post_async` storing the result in `shared_storage['processed_numbers']` and returning a status string (\"processed\").\n\n4. **Functions and Methods Called Internally:**  \n   - **From Python standard library:**\n     - `asyncio.new_event_loop`, `asyncio.set_event_loop`, `asyncio.sleep`, `asyncio.get_event_loop().time`\n     - Various `unittest.TestCase` methods (`setUp`, `tearDown`, `assertEqual`, `assertLess`, `assertRaises`)\n   - **From local codebase:**\n     - `AsyncParallelBatchNode.run_async` (the main method being tested)\n     - `AsyncParallelBatchNode.prep_async`\n     - `AsyncParallelBatchNode.exec_async`\n     - `AsyncParallelBatchNode.post_async`\n   - **User-defined subclasses and overrides:**  \n     - Custom processors like `AsyncParallelNumberProcessor`, `ErrorProcessor`, and `OrderTrackingProcessor`, each overriding `exec_async` for specific test behaviors.\n\n**In summary:**  \nThe file contains thorough asynchronous unit tests for verifying that an async parallel batch node correctly processes batches of numbers using custom processors, measuring execution time, concurrency, error handling, and different input cases. The test infrastructure closely interacts with Python's `asyncio` and `unittest` modules."
    },
    "208": {
      "unit_name": "test_flow_composition.py",
      "file_path": "/tmp/PocketFlow/tests/test_flow_composition.py",
      "code": "# tests/test_flow_composition.py\nimport unittest\nimport asyncio # Keep import, might be needed if other tests use it indirectly\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import Node, Flow\n\n# --- Existing Nodes ---\nclass NumberNode(Node):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n    def prep(self, shared_storage):\n        shared_storage['current'] = self.number\n    # post implicitly returns None\n\nclass AddNode(Node):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n    def prep(self, shared_storage):\n        shared_storage['current'] += self.number\n    # post implicitly returns None\n\nclass MultiplyNode(Node):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n    def prep(self, shared_storage):\n        shared_storage['current'] *= self.number\n    # post implicitly returns None\n\n# --- New Nodes for Action Propagation Test ---\nclass SignalNode(Node):\n    \"\"\"A node that returns a specific signal string from its post method.\"\"\"\n    def __init__(self, signal=\"default_signal\"):\n        super().__init__()\n        self.signal = signal\n    # No prep needed usually if just signaling\n    def post(self, shared_storage, prep_result, exec_result):\n        # Store the signal in shared storage for verification\n        shared_storage['last_signal_emitted'] = self.signal\n        return self.signal # Return the specific action string\n\nclass PathNode(Node):\n    \"\"\"A node to indicate which path was taken in the outer flow.\"\"\"\n    def __init__(self, path_id):\n        super().__init__()\n        self.path_id = path_id\n    def prep(self, shared_storage):\n        shared_storage['path_taken'] = self.path_id\n    # post implicitly returns None\n\n# --- Test Class ---\nclass TestFlowComposition(unittest.TestCase):\n\n    # --- Existing Tests (Unchanged) ---\n    def test_flow_as_node(self):\n        \"\"\"\n        1) Create a Flow (f1) starting with NumberNode(5), then AddNode(10), then MultiplyNode(2).\n        2) Create a second Flow (f2) whose start is f1.\n        3) Create a wrapper Flow (f3) that contains f2 to ensure proper execution.\n        Expected final result in shared_storage['current']: (5 + 10) * 2 = 30.\n        \"\"\"\n        shared_storage = {}\n        f1 = Flow(start=NumberNode(5))\n        f1 >> AddNode(10) >> MultiplyNode(2)\n        f2 = Flow(start=f1)\n        f3 = Flow(start=f2)\n        f3.run(shared_storage)\n        self.assertEqual(shared_storage['current'], 30)\n\n    def test_nested_flow(self):\n        \"\"\"\n        Demonstrates nested flows with proper wrapping:\n        inner_flow: NumberNode(5) -> AddNode(3)\n        middle_flow: starts with inner_flow -> MultiplyNode(4)\n        wrapper_flow: contains middle_flow to ensure proper execution\n        Expected final result: (5 + 3) * 4 = 32.\n        \"\"\"\n        shared_storage = {}\n        inner_flow = Flow(start=NumberNode(5))\n        inner_flow >> AddNode(3)\n        middle_flow = Flow(start=inner_flow)\n        middle_flow >> MultiplyNode(4)\n        wrapper_flow = Flow(start=middle_flow)\n        wrapper_flow.run(shared_storage)\n        self.assertEqual(shared_storage['current'], 32)\n\n    def test_flow_chaining_flows(self):\n        \"\"\"\n        Demonstrates chaining two flows with proper wrapping:\n        flow1: NumberNode(10) -> AddNode(10) # final = 20\n        flow2: MultiplyNode(2) # final = 40\n        wrapper_flow: contains both flow1 and flow2 to ensure proper execution\n        Expected final result: (10 + 10) * 2 = 40.\n        \"\"\"\n        shared_storage = {}\n        numbernode = NumberNode(10)\n        numbernode >> AddNode(10)\n        flow1 = Flow(start=numbernode)\n        flow2 = Flow(start=MultiplyNode(2))\n        flow1 >> flow2 # Default transition based on flow1 returning None\n        wrapper_flow = Flow(start=flow1)\n        wrapper_flow.run(shared_storage)\n        self.assertEqual(shared_storage['current'], 40)\n\n    def test_composition_with_action_propagation(self):\n        \"\"\"\n        Test that an outer flow can branch based on the action returned\n        by the last node's post() within an inner flow.\n        \"\"\"\n        shared_storage = {}\n\n        # 1. Define an inner flow that ends with a node returning a specific action\n        inner_start_node = NumberNode(100)       # current = 100, post -> None\n        inner_end_node = SignalNode(\"inner_done\") # post -> \"inner_done\"\n        inner_start_node >> inner_end_node\n        # Inner flow will execute start->end, and the Flow's execution will return \"inner_done\"\n        inner_flow = Flow(start=inner_start_node)\n\n        # 2. Define target nodes for the outer flow branches\n        path_a_node = PathNode(\"A\") # post -> None\n        path_b_node = PathNode(\"B\") # post -> None\n\n        # 3. Define the outer flow starting with the inner flow\n        outer_flow = Flow()\n        outer_flow.start(inner_flow) # Use the start() method\n\n        # 4. Define branches FROM the inner_flow object based on its returned action\n        inner_flow - \"inner_done\" >> path_b_node  # This path should be taken\n        inner_flow - \"other_action\" >> path_a_node # This path should NOT be taken\n\n        # 5. Run the outer flow and capture the last action\n        # Execution: inner_start -> inner_end -> path_b\n        last_action_outer = outer_flow.run(shared_storage)\n\n        # 6. Assert the results\n        # Check state after inner flow execution\n        self.assertEqual(shared_storage.get('current'), 100)\n        self.assertEqual(shared_storage.get('last_signal_emitted'), \"inner_done\")\n        # Check that the correct outer path was taken\n        self.assertEqual(shared_storage.get('path_taken'), \"B\")\n        # Check the action returned by the outer flow. The last node executed was\n        # path_b_node, which returns None from its post method.\n        self.assertIsNone(last_action_outer)\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary of `test_flow_composition.py`:**\n\n1. **Primary purpose**  \nThe code unit is a suite of unittests for verifying the composition, chaining, and branching behaviors of computational flows and nodes in the PocketFlow library. It ensures that different node combinations and nested flows behave as expected, including proper action propagation to enable branching in a flow graph.\n\n2. **Parameters**  \n- The test methods typically accept only `self` (from `unittest.TestCase`), as is standard for unittests.\n- The tests internally create and use `shared_storage` dictionaries to track state throughout node and flow execution.\n\n3. **Return value**  \n- The test methods do not return values (they rely on assertions).\n- Individual flow executions (e.g., `Flow.run()`) may return the result of the last node action (for branching), but in the test context these are consumed via assertions.\n\n4. **Functions/methods called internally**  \nThe following methods and classes are used within the test file:\n- **Node subclasses' methods:**\n  - `prep()` and `post()` methods of custom nodes (`NumberNode`, `AddNode`, `MultiplyNode`, `SignalNode`, `PathNode`)\n- **Flow framework methods:**\n  - `Flow.__init__()`\n  - `Flow.start()`\n  - `Flow.run()`\n  - Flow operator overloads: `>>` (chaining), `- <action> >>` (branching based on action)\n- **Test assertions:**\n  - `self.assertEqual()`\n  - `self.assertIsNone()`\n- **Python standard library:**\n  - `unittest.main()` for running the test suite\n\nOverall, the tests comprehensively exercise flow and node composition, the action dispatch/branch mechanism, and state-sharing via shared storage in the PocketFlow framework."
    },
    "209": {
      "unit_name": "test_async_parallel_batch_flow.py",
      "file_path": "/tmp/PocketFlow/tests/test_async_parallel_batch_flow.py",
      "code": "import unittest\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import AsyncNode, AsyncParallelBatchNode, AsyncParallelBatchFlow\n\nclass AsyncParallelNumberProcessor(AsyncParallelBatchNode):\n    def __init__(self, delay=0.1):\n        super().__init__()\n        self.delay = delay\n    \n    async def prep_async(self, shared_storage):\n        batch = shared_storage['batches'][self.params['batch_id']]\n        return batch\n    \n    async def exec_async(self, number):\n        await asyncio.sleep(self.delay)  # Simulate async processing\n        return number * 2\n        \n    async def post_async(self, shared_storage, prep_result, exec_result):\n        if 'processed_numbers' not in shared_storage:\n            shared_storage['processed_numbers'] = {}\n        shared_storage['processed_numbers'][self.params['batch_id']] = exec_result\n        return \"processed\"\n\nclass AsyncAggregatorNode(AsyncNode):\n    async def prep_async(self, shared_storage):\n        # Combine all batch results in order\n        all_results = []\n        processed = shared_storage.get('processed_numbers', {})\n        for i in range(len(processed)):\n            all_results.extend(processed[i])\n        return all_results\n    \n    async def exec_async(self, prep_result):\n        await asyncio.sleep(0.01)\n        return sum(prep_result)\n    \n    async def post_async(self, shared_storage, prep_result, exec_result):\n        shared_storage['total'] = exec_result\n        return \"aggregated\"\n\nclass TestAsyncParallelBatchFlow(unittest.TestCase):\n    def setUp(self):\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n    \n    def tearDown(self):\n        self.loop.close()\n\n    def test_parallel_batch_flow(self):\n        \"\"\"\n        Test basic parallel batch processing flow with batch IDs\n        \"\"\"\n        class TestParallelBatchFlow(AsyncParallelBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{'batch_id': i} for i in range(len(shared_storage['batches']))]\n\n        shared_storage = {\n            'batches': [\n                [1, 2, 3],  # batch_id: 0\n                [4, 5, 6],  # batch_id: 1\n                [7, 8, 9]   # batch_id: 2\n            ]\n        }\n\n        processor = AsyncParallelNumberProcessor(delay=0.1)\n        aggregator = AsyncAggregatorNode()\n        \n        processor - \"processed\" >> aggregator\n        flow = TestParallelBatchFlow(start=processor)\n        \n        start_time = self.loop.time()\n        self.loop.run_until_complete(flow.run_async(shared_storage))\n        execution_time = self.loop.time() - start_time\n\n        # Verify each batch was processed correctly\n        expected_batch_results = {\n            0: [2, 4, 6],    # [1,2,3] * 2\n            1: [8, 10, 12],  # [4,5,6] * 2\n            2: [14, 16, 18]  # [7,8,9] * 2\n        }\n        self.assertEqual(shared_storage['processed_numbers'], expected_batch_results)\n        \n        # Verify total\n        expected_total = sum(num * 2 for batch in shared_storage['batches'] for num in batch)\n        self.assertEqual(shared_storage['total'], expected_total)\n        \n        # Verify parallel execution\n        self.assertLess(execution_time, 0.2)\n\n    def test_error_handling(self):\n        \"\"\"\n        Test error handling in parallel batch flow\n        \"\"\"\n        class ErrorProcessor(AsyncParallelNumberProcessor):\n            async def exec_async(self, item):\n                if item == 2:\n                    raise ValueError(f\"Error processing item {item}\")\n                return item\n\n        class ErrorBatchFlow(AsyncParallelBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{'batch_id': i} for i in range(len(shared_storage['batches']))]\n\n        shared_storage = {\n            'batches': [\n                [1, 2, 3],  # Contains error-triggering value\n                [4, 5, 6]\n            ]\n        }\n\n        processor = ErrorProcessor()\n        flow = ErrorBatchFlow(start=processor)\n        \n        with self.assertRaises(ValueError):\n            self.loop.run_until_complete(flow.run_async(shared_storage))\n\n    def test_multiple_batch_sizes(self):\n        \"\"\"\n        Test parallel batch flow with varying batch sizes\n        \"\"\"\n        class VaryingBatchFlow(AsyncParallelBatchFlow):\n            async def prep_async(self, shared_storage):\n                return [{'batch_id': i} for i in range(len(shared_storage['batches']))]\n\n        shared_storage = {\n            'batches': [\n                [1],           # batch_id: 0\n                [2, 3, 4],    # batch_id: 1\n                [5, 6],       # batch_id: 2\n                [7, 8, 9, 10] # batch_id: 3\n            ]\n        }\n\n        processor = AsyncParallelNumberProcessor(delay=0.05)\n        aggregator = AsyncAggregatorNode()\n        \n        processor - \"processed\" >> aggregator\n        flow = VaryingBatchFlow(start=processor)\n        \n        self.loop.run_until_complete(flow.run_async(shared_storage))\n        \n        # Verify each batch was processed correctly\n        expected_batch_results = {\n            0: [2],                 # [1] * 2\n            1: [4, 6, 8],          # [2,3,4] * 2\n            2: [10, 12],           # [5,6] * 2\n            3: [14, 16, 18, 20]    # [7,8,9,10] * 2\n        }\n        self.assertEqual(shared_storage['processed_numbers'], expected_batch_results)\n        \n        # Verify total\n        expected_total = sum(num * 2 for batch in shared_storage['batches'] for num in batch)\n        self.assertEqual(shared_storage['total'], expected_total)\n\nif __name__ == '__main__':\n    unittest.main()",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_async_parallel_batch_flow.py`:**\n\n1. **Primary Purpose**  \n   This code unit defines a set of asynchronous unittests to verify the behavior of an asynchronous parallel batch processing framework provided by the PocketFlow package. It specifically tests the processing of number batches in parallel (mapping, aggregation), error handling in batch tasks, and operation with variable batch sizes.\n\n2. **Parameters**  \n   The main test methods (`test_parallel_batch_flow`, `test_error_handling`, `test_multiple_batch_sizes`) accept only `self` as a parameter because they're instance methods of the `unittest.TestCase` subclass.  \n   Internally, the custom processing nodes and test flows may accept configuration parameters, such as:\n   - `AsyncParallelNumberProcessor.__init__(delay=0.1)`: specifies artificial delay for processing each number asynchronously.\n   - `shared_storage`: a dict-like object passed between nodes, storing batch data and results.\n   - `params`: dictionary containing parameters (e.g., the batch ID for each work item).\n\n3. **Return Values**  \n   - The test methods themselves return nothing (their success is implicit in assertions passing).\n   - Node methods (`prep_async`, `exec_async`, `post_async`) return intermediate results:\n       - `prep_async` returns a batch slice or aggregated results.\n       - `exec_async` returns processed number(s) (e.g., multiplied by two).\n       - `post_async` returns simple status strings (\"processed\", \"aggregated\").\n   - Final results of interest (e.g., processed numbers and aggregate totals) are written into `shared_storage` for assertion by the tests.\n\n4. **Internally Called Functions/Methods**  \n   - **Async Framework & Utilities:**\n     - `asyncio.sleep`\n     - `asyncio.new_event_loop`, `asyncio.set_event_loop`, `run_until_complete`, `loop.time`, `loop.close`\n   - **PocketFlow Classes/Methods:**\n     - `AsyncNode`, `AsyncParallelBatchNode`, `AsyncParallelBatchFlow` and their respective `prep_async`, `exec_async`, `post_async`\n     - Overloaded operators (`- \"processed\" >> aggregator`) for wiring nodes\n     - `flow.run_async(shared_storage)`\n   - **Unittest Framework:**  \n     - `unittest.TestCase.setUp`, `tearDown`, `assertEqual`, `assertLess`, `assertRaises`\n   - **Standard Library:**  \n     - `sum`, `len`, `range`, `isinstance`, `extend`, basic dict/list operations\n   - **Custom/Subclassed Methods:**  \n     - Custom overrides of `prep_async`, `exec_async`, `post_async` in test-specific flow/node classes\n\n**In summary:**  \nThis unit tests that parallel batch processing via PocketFlow achieves correct results, proper parallelism, and reliable error handling in asynchronous scenarios, using carefully constructed test nodes and validation using Python\u2019s standard unittest framework."
    },
    "210": {
      "unit_name": "test_async_flow.py",
      "file_path": "/tmp/PocketFlow/tests/test_async_flow.py",
      "code": "import unittest\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom pocketflow import Node, AsyncNode, AsyncFlow\n\nclass AsyncNumberNode(AsyncNode):\n    \"\"\"\n    Simple async node that sets 'current' to a given number.\n    Demonstrates overriding .process() (sync) and using\n    post_async() for the async portion.\n    \"\"\"\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n\n    async def prep_async(self, shared_storage):\n        # Synchronous work is allowed inside an AsyncNode,\n        # but final 'condition' is determined by post_async().\n        shared_storage['current'] = self.number\n        return \"set_number\"\n\n    async def post_async(self, shared_storage, prep_result, proc_result):\n        # Possibly do asynchronous tasks here\n        await asyncio.sleep(0.01)\n        # Return a condition for the flow\n        return \"number_set\"\n\nclass AsyncIncrementNode(AsyncNode):\n    \"\"\"\n    Demonstrates incrementing the 'current' value asynchronously.\n    \"\"\"\n    async def prep_async(self, shared_storage):\n        shared_storage['current'] = shared_storage.get('current', 0) + 1\n        return \"incremented\"\n\n    async def post_async(self, shared_storage, prep_result, proc_result):\n        await asyncio.sleep(0.01)  # simulate async I/O\n        return \"done\"\n\nclass AsyncSignalNode(AsyncNode):\n    \"\"\" An async node that returns a specific signal string from post_async. \"\"\"\n    def __init__(self, signal=\"default_async_signal\"):\n        super().__init__()\n        self.signal = signal\n\n    # No prep needed usually if just signaling\n    async def prep_async(self, shared_storage):\n        await asyncio.sleep(0.01) # Simulate async work\n\n    async def post_async(self, shared_storage, prep_result, exec_result):\n        # Store the signal in shared storage for verification\n        shared_storage['last_async_signal_emitted'] = self.signal\n        await asyncio.sleep(0.01) # Simulate async work\n        print(self.signal)\n        return self.signal # Return the specific action string\n\nclass AsyncPathNode(AsyncNode):\n    \"\"\" An async node to indicate which path was taken in the outer flow. \"\"\"\n    def __init__(self, path_id):\n        super().__init__()\n        self.path_id = path_id\n\n    async def prep_async(self, shared_storage):\n        await asyncio.sleep(0.01) # Simulate async work\n        shared_storage['async_path_taken'] = self.path_id\n\n    # post_async implicitly returns None (for default transition out if needed)\n    async def post_async(self, shared_storage, prep_result, exec_result):\n         await asyncio.sleep(0.01)\n         # Return None by default\n\nclass TestAsyncNode(unittest.TestCase):\n    \"\"\"\n    Test the AsyncNode (and descendants) in isolation (not in a flow).\n    \"\"\"\n    def test_async_number_node_direct_call(self):\n        \"\"\"\n        Even though AsyncNumberNode is designed for an async flow,\n        we can still test it directly by calling run_async().\n        \"\"\"\n        async def run_node():\n            node = AsyncNumberNode(42)\n            shared_storage = {}\n            condition = await node.run_async(shared_storage)\n            return shared_storage, condition\n\n        shared_storage, condition = asyncio.run(run_node())\n        self.assertEqual(shared_storage['current'], 42)\n        self.assertEqual(condition, \"number_set\")\n\n    def test_async_increment_node_direct_call(self):\n        async def run_node():\n            node = AsyncIncrementNode()\n            shared_storage = {'current': 10}\n            condition = await node.run_async(shared_storage)\n            return shared_storage, condition\n\n        shared_storage, condition = asyncio.run(run_node())\n        self.assertEqual(shared_storage['current'], 11)\n        self.assertEqual(condition, \"done\")\n\n\nclass TestAsyncFlow(unittest.TestCase):\n    \"\"\"\n    Test how AsyncFlow orchestrates multiple async nodes.\n    \"\"\"\n    def test_simple_async_flow(self):\n        \"\"\"\n        Flow:\n          1) AsyncNumberNode(5) -> sets 'current' to 5\n          2) AsyncIncrementNode() -> increments 'current' to 6\n        \"\"\"\n\n        # Create our nodes\n        start = AsyncNumberNode(5)\n        inc_node = AsyncIncrementNode()\n\n        # Chain them: start >> inc_node\n        start - \"number_set\" >> inc_node\n\n        # Create an AsyncFlow with start\n        flow = AsyncFlow(start)\n\n        # We'll run the flow synchronously (which under the hood is asyncio.run())\n        shared_storage = {}\n        asyncio.run(flow.run_async(shared_storage))\n\n        self.assertEqual(shared_storage['current'], 6)\n\n    def test_async_flow_branching(self):\n        \"\"\"\n        Demonstrate a branching scenario where we return different\n        conditions. For example, you could have an async node that\n        returns \"go_left\" or \"go_right\" in post_async, but here\n        we'll keep it simpler for demonstration.\n        \"\"\"\n\n        class BranchingAsyncNode(AsyncNode):\n            def exec(self, data):\n                value = shared_storage.get(\"value\", 0)\n                shared_storage[\"value\"] = value\n                # We'll decide branch based on whether 'value' is positive\n                return None\n\n            async def post_async(self, shared_storage, prep_result, proc_result):\n                await asyncio.sleep(0.01)\n                if shared_storage[\"value\"] >= 0:\n                    return \"positive_branch\"\n                else:\n                    return \"negative_branch\"\n\n        class PositiveNode(Node):\n            def exec(self, data):\n                shared_storage[\"path\"] = \"positive\"\n                return None\n\n        class NegativeNode(Node):\n            def exec(self, data):\n                shared_storage[\"path\"] = \"negative\"\n                return None\n\n        shared_storage = {\"value\": 10}\n\n        start = BranchingAsyncNode()\n        positive_node = PositiveNode()\n        negative_node = NegativeNode()\n\n        # Condition-based chaining\n        start - \"positive_branch\" >> positive_node\n        start - \"negative_branch\" >> negative_node\n\n        flow = AsyncFlow(start)\n        asyncio.run(flow.run_async(shared_storage))\n\n        self.assertEqual(shared_storage[\"path\"], \"positive\", \n                         \"Should have taken the positive branch\")\n\n    def test_async_composition_with_action_propagation(self):\n        \"\"\"\n        Test AsyncFlow branches based on action from nested AsyncFlow's last node.\n        \"\"\"\n        async def run_test():\n            shared_storage = {}\n\n            # 1. Define an inner async flow ending with AsyncSignalNode\n            # Use existing AsyncNumberNode which should return None from post_async implicitly\n            inner_start_node = AsyncNumberNode(200)\n            inner_end_node = AsyncSignalNode(\"async_inner_done\") # post_async -> \"async_inner_done\"\n            inner_start_node - \"number_set\" >> inner_end_node\n            # Inner flow will execute start->end, Flow exec returns \"async_inner_done\"\n            inner_flow = AsyncFlow(start=inner_start_node)\n\n            # 2. Define target async nodes for the outer flow branches\n            path_a_node = AsyncPathNode(\"AsyncA\") # post_async -> None\n            path_b_node = AsyncPathNode(\"AsyncB\") # post_async -> None\n\n            # 3. Define the outer async flow starting with the inner async flow\n            outer_flow = AsyncFlow(start=inner_flow)\n\n            # 4. Define branches FROM the inner_flow object based on its returned action\n            inner_flow - \"async_inner_done\" >> path_b_node  # This path should be taken\n            inner_flow - \"other_action\" >> path_a_node      # This path should NOT be taken\n\n            # 5. Run the outer async flow and capture the last action\n            # Execution: inner_start -> inner_end -> path_b\n            last_action_outer = await outer_flow.run_async(shared_storage)\n\n            # 6. Return results for assertion\n            return shared_storage, last_action_outer\n\n        # Run the async test function\n        shared_storage, last_action_outer = asyncio.run(run_test())\n\n        # 7. Assert the results\n        # Check state after inner flow execution\n        self.assertEqual(shared_storage.get('current'), 200) # From AsyncNumberNode\n        self.assertEqual(shared_storage.get('last_async_signal_emitted'), \"async_inner_done\")\n        # Check that the correct outer path was taken\n        self.assertEqual(shared_storage.get('async_path_taken'), \"AsyncB\")\n        # Check the action returned by the outer flow. The last node executed was\n        # path_b_node, which returns None from its post_async method.\n        self.assertIsNone(last_action_outer)\n\nif __name__ == '__main__':\n    unittest.main()\n",
      "summary": "**Summary of `/tmp/PocketFlow/tests/test_async_flow.py`:**\n\n1. **Primary purpose of the code unit:**  \n   The code provides a set of unit tests for the asynchronous flow orchestration features in the `pocketflow` library, specifically testing the behavior of `AsyncNode` and `AsyncFlow` classes and their interaction, including branching logic, action propagation, and async node operations.\n\n2. **Brief description of its parameters (if any):**  \n   The file contains several test cases within `unittest.TestCase` classes. The methods themselves do not take parameters (besides `self`), as is standard with unittest. Any required inputs are provided via local function arguments (e.g., a `shared_storage` dictionary) or by constructing nodes with parameters (like the number in `AsyncNumberNode(number)` or `AsyncSignalNode(signal)`).\n\n3. **Brief description of its return value (if any):**  \n   - The unit test methods do not have explicit return values; they use assertions to validate state and transitions.\n   - Helper functions inside test methods (such as `run_node` or `run_test`) may return results (like `shared_storage` and condition/action values) for assertion purposes, but these are only used internally within the test methods.\n\n4. **List of other functions or methods it calls internally:**  \n   - Class methods of `AsyncNode` subclasses:\n     - `prep_async`\n     - `post_async`\n     - (sometimes) `exec`\n   - `AsyncNode.run_async(shared_storage)`\n   - `AsyncFlow.run_async(shared_storage)`\n   - Operator overloading for chaining nodes:  \n     - `-` and `>>` for chaining and branching nodes (`start - \"condition\" >> next_node`)\n   - `unittest.TestCase` methods:\n     - `self.assertEqual`\n     - `self.assertIsNone`\n   - `asyncio.run()` (to run asynchronous test routines)\n\n   The code also constructs flows using the `AsyncFlow` class, sets up node chaining, and uses Python's `unittest` framework for organizing and running the tests.\n\n**In summary:**  \nThis file tests asynchronous node and flow orchestration in the PocketFlow framework by simulating various async node behaviors, branching, and nested flow scenarios, checking that shared state is modified and routed as expected. It uses `unittest`'s assertion methods and Python's `asyncio` for running async code."
    },
    "211": {
      "unit_name": "update_pocketflow_mdc.py",
      "file_path": "/tmp/PocketFlow/utils/update_pocketflow_mdc.py",
      "code": "#!/usr/bin/env python3\n\"\"\"\nScript to generate MDC files from the PocketFlow docs folder, creating one MDC file per MD file.\n\nUsage:\n    python update_pocketflow_mdc.py [--docs-dir PATH] [--rules-dir PATH]\n\"\"\"\n\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\nimport sys\nimport html.parser\n\nclass HTMLTagStripper(html.parser.HTMLParser):\n    \"\"\"HTML Parser subclass to strip HTML tags from content\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.reset()\n        self.strict = False\n        self.convert_charrefs = True\n        self.text = []\n    \n    def handle_data(self, data):\n        self.text.append(data)\n    \n    def get_text(self):\n        return ''.join(self.text)\n\ndef strip_html_tags(html_content):\n    \"\"\"Remove HTML tags from content\"\"\"\n    stripper = HTMLTagStripper()\n    stripper.feed(html_content)\n    return stripper.get_text()\n\ndef extract_frontmatter(file_path):\n    \"\"\"Extract title, parent, and nav_order from markdown frontmatter\"\"\"\n    frontmatter = {}\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            \n            # Extract frontmatter between --- markers\n            fm_match = re.search(r'^---\\s*(.+?)\\s*---', content, re.DOTALL)\n            if fm_match:\n                frontmatter_text = fm_match.group(1)\n                \n                # Extract fields\n                title_match = re.search(r'title:\\s*\"?([^\"\\n]+)\"?', frontmatter_text)\n                parent_match = re.search(r'parent:\\s*\"?([^\"\\n]+)\"?', frontmatter_text)\n                nav_order_match = re.search(r'nav_order:\\s*(\\d+)', frontmatter_text)\n                \n                if title_match:\n                    frontmatter['title'] = title_match.group(1)\n                if parent_match:\n                    frontmatter['parent'] = parent_match.group(1)\n                if nav_order_match:\n                    frontmatter['nav_order'] = int(nav_order_match.group(1))\n    except Exception as e:\n        print(f\"Error reading frontmatter from {file_path}: {e}\")\n    \n    return frontmatter\n\ndef extract_first_heading(file_path):\n    \"\"\"Extract the first heading from markdown content\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            \n            # Remove frontmatter\n            content = re.sub(r'^---.*?---\\s*', '', content, flags=re.DOTALL)\n            \n            # Find first heading\n            heading_match = re.search(r'#\\s+(.+)', content)\n            if heading_match:\n                return heading_match.group(1).strip()\n    except Exception as e:\n        print(f\"Error extracting heading from {file_path}: {e}\")\n    \n    # Fallback to filename if no heading found\n    return Path(file_path).stem.replace('_', ' ').title()\n\ndef get_mdc_description(md_file, frontmatter, heading):\n    \"\"\"Generate a description for the MDC file based on file metadata\"\"\"\n    section = \"\"\n    subsection = \"\"\n    \n    # Determine section from path\n    path_parts = Path(md_file).parts\n    if 'core_abstraction' in path_parts:\n        section = \"Core Abstraction\"\n    elif 'design_pattern' in path_parts:\n        section = \"Design Pattern\"\n    elif 'utility_function' in path_parts:\n        section = \"Utility Function\"\n    \n    # Use frontmatter title or heading as subsection\n    if 'title' in frontmatter:\n        subsection = frontmatter['title']\n    else:\n        subsection = heading\n    \n    # For the combined guide and index\n    if Path(md_file).name == \"guide.md\":\n        return \"Guidelines for using PocketFlow, Agentic Coding\"\n    \n    # For index.md at root level, use a different format\n    if Path(md_file).name == \"index.md\" and section == \"\":\n        return \"Guidelines for using PocketFlow, a minimalist LLM framework\"\n    \n    # For other files, create a more specific description\n    if section:\n        return f\"Guidelines for using PocketFlow, {section}, {subsection}\"\n    else:\n        return f\"Guidelines for using PocketFlow, {subsection}\"\n\ndef process_markdown_content(content, remove_local_refs=False):\n    \"\"\"Process markdown content to make it suitable for MDC file\"\"\"\n    # Remove frontmatter\n    content = re.sub(r'^---.*?---\\s*', '', content, flags=re.DOTALL)\n    \n    # Replace HTML div tags and their content\n    content = re.sub(r'<div.*?>.*?</div>', '', content, flags=re.DOTALL)\n    \n    if remove_local_refs:\n        # Replace markdown links to local documentation with just the text in brackets\n        # This prevents automatically including all docs when the file is loaded\n        # Keep the brackets around the text for better discoverability\n        content = re.sub(r'\\[([^\\]]+)\\]\\(\\./[^)]+\\)', r'[\\1]', content)\n    else:\n        # Adjust relative links to maintain references within the docs structure\n        content = re.sub(r'\\]\\(\\./([^)]+)\\)', r'](mdc:./\\1)', content)\n        \n        # Ensure links to md files work correctly\n        content = re.sub(r'\\]\\(mdc:\\./(.+?)\\.md\\)', r'](mdc:./\\1.md)', content)\n        content = re.sub(r'\\]\\(mdc:\\./(.+?)\\.html\\)', r'](mdc:./\\1.md)', content)\n    \n    # Strip remaining HTML tags\n    content = strip_html_tags(content)\n    \n    return content\n\ndef get_documentation_first_policy():\n    \"\"\"Return the DOCUMENTATION FIRST POLICY text to be included in the guide\"\"\"\n    return \"\"\"# DOCUMENTATION FIRST POLICY\n\n**CRITICAL INSTRUCTION**: When implementing a Pocket Flow app:\n\n1. **ALWAYS REQUEST MDC FILES FIRST** - Before writing any code, request and review all relevant MDC documentation files. This doc provides an explaination of the documents.\n2. **UNDERSTAND THE FRAMEWORK** - Gain comprehensive understanding of the Pocket Flow framework from documentation\n3. **AVOID ASSUMPTION-DRIVEN DEVELOPMENT** - Do not base your implementation on assumptions or guesswork. Even if the human didn't explicitly mention pocket flow in their request, if the code you are editing is using pocket flow, you should request relevant docs to help you understand best practice as well before editing.\n\n**VERIFICATION**: Begin each implementation with a brief summary of the documentation you've reviewed to inform your approach.\n\n\"\"\"\n\ndef generate_mdc_header(md_file, description, always_apply=False):\n    \"\"\"Generate MDC file header with appropriate frontmatter\"\"\"\n    # Determine if we should include globs\n    # For index.md and guide.md, we include **/*.py to provide high-level context for Python files\n    # For other files, leave it empty to be less intrusive\n    globs = \"**/*.py\" if always_apply else \"\"\n    \n    return f\"\"\"---\ndescription: {description}\nglobs: {globs}\nalwaysApply: {\"true\" if always_apply else \"false\"}\n---\n\"\"\"\n\ndef has_substantive_content(content):\n    \"\"\"Check if the processed content has substantive content beyond the frontmatter\"\"\"\n    # Remove frontmatter\n    content_without_frontmatter = re.sub(r'^---.*?---\\s*', '', content, flags=re.DOTALL)\n    \n    # Remove whitespace and common HTML/markdown formatting\n    cleaned_content = re.sub(r'\\s+', '', content_without_frontmatter)\n    cleaned_content = re.sub(r'{:.*?}', '', cleaned_content)\n    \n    # If there's almost nothing left after cleaning, consider it empty\n    return len(cleaned_content) > 20  # Arbitrary threshold, adjust as needed\n\ndef create_combined_guide(docs_dir, rules_dir):\n    \"\"\"Create a combined guide that includes both the guide and index content\"\"\"\n    docs_path = Path(docs_dir)\n    rules_path = Path(rules_dir)\n    \n    guide_file = docs_path / \"guide.md\"\n    index_file = docs_path / \"index.md\"\n    \n    if not guide_file.exists() or not index_file.exists():\n        print(\"Warning: guide.md or index.md not found, skipping combined guide creation\")\n        return False\n    \n    # Get guide content and index content\n    with open(guide_file, 'r', encoding='utf-8') as f:\n        guide_content = f.read()\n    \n    with open(index_file, 'r', encoding='utf-8') as f:\n        index_content = f.read()\n    \n    # Process the content\n    processed_guide = process_markdown_content(guide_content, remove_local_refs=True)\n    processed_index = process_markdown_content(index_content, remove_local_refs=True)\n    \n    # Get the documentation first policy\n    doc_first_policy = get_documentation_first_policy()\n    \n    # Combine the content with the documentation first policy at the beginning\n    combined_content = doc_first_policy + processed_guide + \"\\n\\n\" + processed_index\n    \n    # Generate the MDC header\n    description = \"Guidelines for using PocketFlow, Agentic Coding\"\n    mdc_header = generate_mdc_header(guide_file, description, always_apply=True)\n    \n    # Combine header and processed content\n    mdc_content = mdc_header + combined_content\n    \n    # Create the output path with the new filename\n    output_path = rules_path / \"guide_for_pocketflow.mdc\"\n    \n    # Write the MDC file\n    with open(output_path, 'w', encoding='utf-8') as f:\n        f.write(mdc_content)\n    \n    print(f\"Created combined guide MDC file: {output_path}\")\n    return True\n\ndef convert_md_to_mdc(md_file, output_dir, docs_dir, special_treatment=False):\n    \"\"\"Convert a markdown file to MDC format and save to the output directory\"\"\"\n    try:\n        print(f\"Processing: {md_file}\")\n        \n        # Skip guide.md and index.md as they'll be handled separately\n        file_name = Path(md_file).name\n        if file_name in [\"guide.md\", \"index.md\"]:\n            print(f\"Skipping {file_name} for individual processing - it will be included in the combined guide\")\n            return True\n        \n        # Skip empty index.md files in subfolders\n        parent_dir = Path(md_file).parent.name\n        \n        # Check if this is an index.md in a subfolder (not the main index.md)\n        if (file_name == \"index.md\" and parent_dir != \"docs\" and \n            parent_dir in [\"core_abstraction\", \"design_pattern\", \"utility_function\"]):\n            \n            # Read the content\n            with open(md_file, 'r', encoding='utf-8') as f:\n                content = f.read()\n                \n            # Skip if it doesn't have substantive content\n            if not has_substantive_content(content):\n                print(f\"Skipping empty subfolder index: {md_file}\")\n                return True\n        \n        # Extract metadata from file\n        frontmatter = extract_frontmatter(md_file)\n        heading = extract_first_heading(md_file)\n        description = get_mdc_description(md_file, frontmatter, heading)\n        \n        # Read the content\n        with open(md_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        # Process the content\n        processed_content = process_markdown_content(content, remove_local_refs=special_treatment)\n        \n        # Generate the MDC header\n        mdc_header = generate_mdc_header(md_file, description, always_apply=special_treatment)\n        \n        # Combine header and processed content\n        mdc_content = mdc_header + processed_content\n        \n        # Perform a final check to ensure the processed content is substantive\n        if not has_substantive_content(processed_content):\n            print(f\"Skipping file with no substantive content after processing: {md_file}\")\n            return True\n        \n        # Get the path relative to the docs directory\n        rel_path = os.path.relpath(md_file, start=Path(docs_dir))\n        \n        # Extract just the filename and directory structure without the 'docs/' prefix\n        path_parts = Path(rel_path).parts\n        if len(path_parts) > 1 and path_parts[0] == 'docs':\n            # Remove the 'docs/' prefix from the path\n            rel_path = os.path.join(*path_parts[1:])\n        \n        # Create the output path\n        output_path = Path(output_dir) / rel_path\n        \n        # Create output directory if it doesn't exist\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Change extension from .md to .mdc\n        output_path = output_path.with_suffix('.mdc')\n        \n        # Write the MDC file\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(mdc_content)\n        \n        print(f\"Created MDC file: {output_path}\")\n        return True\n    \n    except Exception as e:\n        print(f\"Error converting {md_file} to MDC: {e}\")\n        return False\n\ndef generate_mdc_files(docs_dir, rules_dir):\n    \"\"\"Generate MDC files from all markdown files in the docs directory\"\"\"\n    docs_path = Path(docs_dir)\n    rules_path = Path(rules_dir)\n    \n    # Make sure the docs directory exists\n    if not docs_path.exists() or not docs_path.is_dir():\n        raise ValueError(f\"Directory not found: {docs_dir}\")\n    \n    print(f\"Generating MDC files from docs in: {docs_dir}\")\n    print(f\"Output will be written to: {rules_dir}\")\n    \n    # Create the rules directory if it doesn't exist\n    rules_path.mkdir(parents=True, exist_ok=True)\n    \n    # Create the combined guide file first (includes both guide.md and index.md)\n    create_combined_guide(docs_dir, rules_dir)\n    \n    # Process all other markdown files\n    success_count = 0\n    failure_count = 0\n    \n    # Find all markdown files\n    md_files = list(docs_path.glob(\"**/*.md\"))\n    \n    # Skip the main index.md and guide.md files as we've already processed them in create_combined_guide\n    md_files = [f for f in md_files if f.name != \"index.md\" and f.name != \"guide.md\"]\n    \n    # Process each markdown file\n    for md_file in md_files:\n        if convert_md_to_mdc(md_file, rules_path, docs_dir):\n            success_count += 1\n        else:\n            failure_count += 1\n    \n    print(f\"\\nProcessed {len(md_files) + 1} markdown files:\")  # +1 for the combined guide\n    print(f\"  - Successfully converted: {success_count + 1}\")  # +1 for the combined guide\n    print(f\"  - Failed conversions: {failure_count}\")\n    \n    return success_count > 0 and failure_count == 0\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Generate MDC files from PocketFlow docs\")\n    \n    # Get script directory\n    script_dir = Path(__file__).parent.absolute()\n    \n    # Default to PocketFlow/docs directory relative to script location\n    default_docs_dir = (script_dir.parent / \"docs\").as_posix()\n    \n    # Default rules directory - changed to .cursor/rules\n    default_rules_dir = (script_dir.parent / \".cursor\" / \"rules\").as_posix()\n    \n    parser.add_argument(\"--docs-dir\", \n                        default=default_docs_dir, \n                        help=\"Path to PocketFlow docs directory\")\n    parser.add_argument(\"--rules-dir\", \n                        default=default_rules_dir, \n                        help=\"Output directory for MDC files\")\n    \n    args = parser.parse_args()\n    \n    try:\n        success = generate_mdc_files(args.docs_dir, args.rules_dir)\n        sys.exit(0 if success else 1)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        sys.exit(1) ",
      "summary": "**Summary of `/tmp/PocketFlow/utils/update_pocketflow_mdc.py`:**\n\n1. **Primary Purpose**  \n   This script automates the conversion of all Markdown (`.md`) documentation files from the PocketFlow `docs` directory into MDC-formatted files (`.mdc`) suitable for use with code assistants or documentation-aware tools. It processes each Markdown file, extracts metadata, cleans up content, and writes a corresponding `.mdc` file with custom frontmatter. Special handling is provided for certain files like `index.md` and `guide.md`, which are combined into a single comprehensive MDC guide.\n\n2. **Parameters**  \n   When run as a script, it accepts two optional command-line parameters:\n   - `--docs-dir`: The path to the PocketFlow `docs` directory to read Markdown files from (default is `../docs` relative to the script).\n   - `--rules-dir`: The output directory where the generated MDC files will be written (default is `../.cursor/rules` relative to the script).\n\n3. **Return Value**  \n   The main processing function, `generate_mdc_files`, returns a boolean value:\n   - `True` if at least one MDC file was successfully generated and there were no processing failures.\n   - `False` otherwise.\n   \n   The script exits with code `0` on success, `1` on error.\n\n4. **Internally Called Functions / Methods**\n   - **HTMLTagStripper** class and its methods (`handle_data`, `get_text`) for stripping HTML tags.\n   - `strip_html_tags`: Removes HTML tags from content.\n   - `extract_frontmatter`: Pulls metadata (`title`, `parent`, `nav_order`) from Markdown frontmatter.\n   - `extract_first_heading`: Gets the first Markdown heading.\n   - `get_mdc_description`: Crafts the MDC file's `description` string from metadata.\n   - `process_markdown_content`: Cleans and standardizes Markdown content for MDC format, adjusts links, strips HTML, and optionally removes local refs.\n   - `get_documentation_first_policy`: Returns the \"Documentation First Policy\" instructional text.\n   - `generate_mdc_header`: Creates the YAML-like MDC header frontmatter.\n   - `has_substantive_content`: Checks whether meaningful content remains after processing.\n   - `create_combined_guide`: Specifically combines `guide.md` and `index.md` into a comprehensive guide MDC file.\n   - `convert_md_to_mdc`: Handles individual Markdown-to-MDC file conversion.\n   - `generate_mdc_files`: Oversees the orchestration of all conversions and summary reporting.\n\n**In summary:**  \nThis script batch-converts PocketFlow documentation from Markdown to enriched MDC format, cleaning and annotating the content, and depositing the results into a rules directory for enhanced integration with code tools. It provides command-line options for customizing input/output paths and contains several helper functions for parsing, cleaning, and assembling output."
    }
  },
  "index_dimension": 1024,
  "total_vectors": 212
}